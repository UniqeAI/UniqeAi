"""
Turkish Text Preprocessing for Synthetic Data + Morphological Test(GeliÅŸtirme sÃ¼recinde) 
============================================

Bu modÃ¼l, TÃ¼rkÃ§e metin verilerinin Ã¶n iÅŸlenmesi iÃ§in Zemberek kÃ¼tÃ¼phanesi
entegrasyonunu saÄŸlar, ayrÄ±ca yaygÄ±n konuÅŸma dilini resmi dile dÃ¶nÃ¼ÅŸtÃ¼rme
sistemini iÃ§erir. GeliÅŸtirmeye devam ediliyordur.

Requirements:
    pip install zemberek-python

Usage:
    from ai_model.scripts.turkish_preprocessing import TurkishPreprocessor

    preprocessor = TurkishPreprocessor()
    normalized = preprocessor.normalize_text("Ä°nternetimin hÄ±zÄ±nÄ± yÃ¼kseltmek istiyorum.")
"""

import re
from typing import List, Dict

# Zemberek import ve kullanÄ±labilirlik kontrolÃ¼
try:
    from zemberek import TurkishMorphology  # type: ignore
    ZEMBEREK_AVAILABLE = True
except ImportError:
    ZEMBEREK_AVAILABLE = False
    print("âš   Zemberek kÃ¼tÃ¼phanesi yÃ¼klÃ¼ deÄŸil. Kurulum iÃ§in: pip install zemberek-python")

# Emoji ve Ã¶zel karakter temizleme regex (Bu kÄ±sÄ±mlarÄ± Ã¶ÄŸretebilir miyim?)
_EMOJI_PATTERN = re.compile(
    "[\U0001F600-\U0001F64F"  # Emoticons
    "\U0001F300-\U0001F5FF"  # Symbols & Pictographs
    "\U0001F680-\U0001F6FF"  # Transport & Map Symbols
    "\u2600-\u26FF\u2700-\u27BF]+", flags=re.UNICODE)

# Argo -> formal Ã§eviri sÃ¶zlÃ¼ÄŸÃ¼(Bu kÄ±sÄ±mlarÄ± Ã¶ÄŸretebilir miyim?)
_INFORMAL_TO_FORMAL = {
    r"abi": "",
    r"ya+": "",
    r"ÅŸey ya": "",
    r"bi": "bir",
    r"niye": "neden",
    r"niÃ§in": "neden",
    r"gelmiyoo+r": "gelmiyor"
}

# Stop-words listesi(Bu kÄ±sÄ±mlarÄ± Ã¶ÄŸretebilir miyim?)
_STOP_WORDS = {
    've', 'ile', 'bu', 'ÅŸu', 'o', 'bir', 'iÃ§in', 'ki', 'da', 'de',
    'olan', 'olarak', 'gibi', 'kadar', 'daha', 'en', 'Ã§ok',
    'az', 'var', 'yok', 'mÄ±', 'mi', 'mu', 'mÃ¼'
}

class TurkishPreprocessor:
    """TÃ¼rkÃ§e metin Ã¶n iÅŸleme: normalize etme, kÃ¶k Ã§Ä±karma, formalizasyon."""

    def __init__(self):
        """Zemberek morphology analyzer'Ä± baÅŸlatÄ±lÄ±r; aÃ§Ä±lamazsa fallback mod."""
        if ZEMBEREK_AVAILABLE:
            try:
                # Zemberek nesnesi oluÅŸturulur
                self.morphology = TurkishMorphology.create_with_defaults()
                self.enabled = True
            except Exception as e:
                print(f"âš   Zemberek baÅŸlatÄ±lamadÄ±: {e}")
                self.enabled = False
        else:
            self.enabled = False

    def normalize_text(self, text: str) -> str:
        """
        Metni normalleÅŸtir:
        1. Temel temizlik (_basic_cleanup)
        2. Emoji temizleme
        3. Morfolojik kÃ¶k tabanlÄ± normalize (_zemberek_normalize) veya fallback
        4. Argo dÃ¼zeltme (_formalize_informal)
        5. KÃ¼Ã§Ã¼k harf ve kÄ±rpma
        """
        if not text:
            return ""
        cleaned = self._basic_cleanup(text)
        cleaned = _EMOJI_PATTERN.sub('', cleaned)
        if self.enabled:
            cleaned = self._zemberek_normalize(cleaned)
        else:
            cleaned = self._simple_normalize(cleaned)
        cleaned = self._formalize_informal(cleaned)
        return cleaned.strip().lower()

    def extract_keywords(self, text: str) -> List[str]:
        """
        Metinden anahtar kelimeleri Ã§Ä±kar:
        - Zemberek varsa morfolojik kÃ¶k
        - Yoksa stop-word filtresi
        """
        if not text:
            return []
        tokens = re.findall(r"\w+", text)
        keywords = []
        if self.enabled:
            for t in tokens:
                try:
                    analyses = self.morphology.analyze_sentence(t)
                    if analyses:
                        root = analyses[0].dictionaryItem.root
                        keywords.append(root)
                    else:
                        keywords.append(t.lower())
                except Exception:
                    keywords.append(t.lower())
        else:
            for t in tokens:
                if t.lower() not in _STOP_WORDS and len(t) > 2:
                    keywords.append(t.lower())
        return list(dict.fromkeys(keywords))

    def _basic_cleanup(self, text: str) -> str:
        """Fazla boÅŸluk ve Ã¶zel karakter temizliÄŸi."""
        text = re.sub(r"\s+", ' ', text)
        text = re.sub(r"[^\w\s\.,;!?'-]", '', text)
        return text

    def _zemberek_normalize(self, text: str) -> str:
        """
        Zemberek yoklamasÄ±yla her kelimenin kÃ¶kÃ¼nÃ¼ alarak normalize eder.
        Hata olursa fallback (_simple_normalize) kullanÄ±r.
        """
        tokens = re.findall(r"\w+", text)
        roots = []
        for t in tokens:
            try:
                analyses = self.morphology.analyze_sentence(t)
                if analyses:
                    roots.append(analyses[0].dictionaryItem.root)
                else:
                    roots.append(t)
            except Exception:
                roots.append(t)
        return ' '.join(roots)

    def _simple_normalize(self, text: str) -> str:
        """TÃ¼rkÃ§e karakterleri dÃ¼zelt ve metni iade et."""
        replacements = {'Ä°':'i','I':'Ä±','Åž':'ÅŸ','Äž':'ÄŸ','Ãœ':'Ã¼','Ã–':'Ã¶','Ã‡':'Ã§'}#bu iÅŸlemin kÄ±sayolu olmalÄ± bknz.-->zemberek_morph
        for old,new in replacements.items():
            text = text.replace(old,new)
        return text

    def _formalize_informal(self, text: str) -> str:
        """Argo ifadeleri resmi dile Ã§evirir."""
        for pat, repl in _INFORMAL_TO_FORMAL.items():
            text = re.sub(pat, repl, text, flags=re.IGNORECASE)
        return re.sub(r"\s+", ' ', text)


def preprocess_synthetic_data_inputs(data_points: List[Dict]) -> List[Dict]:
    """Sentetik input'larÄ± Ã¶n iÅŸler ve metadata ekler."""
    pre = TurkishPreprocessor()
    processed = []
    for point in data_points:
        if 'input' in point:
            orig = point['input']
            norm = pre.normalize_text(orig)
            keys = pre.extract_keywords(orig)
            new_point = point.copy()
            new_point['input'] = norm
            new_point['_metadata'] = {
                'original_input': orig,
                'keywords': keys,
                'preprocessed': True
            }
            processed.append(new_point)
        else:
            processed.append(point)
    return processed

# Basit testler
if __name__ == '__main__':
    sample_texts = [
        "Ä°nternetimin hÄ±zÄ±nÄ± yÃ¼kseltmek istiyorum.",
        "abi bu internet niye Ã§ekmiyooor ya? ðŸ˜Š",
        "Fatura Ã¶deme durumunu kontrol etmek istiyorum!"
    ]
    pre = TurkishPreprocessor()
    print("ðŸ§ª TÃ¼rkÃ§e Ã–n Ä°ÅŸleme Testi")
    print("="*40)
    for txt in sample_texts:
        print(f"Orijinal : {txt}")
        print(f"Normalize : {pre.normalize_text(txt)}")
        print(f"Keywords : {pre.extract_keywords(txt)}")
        print("-"*40)