# ğŸ”¥ QLoRA Fine-tuning KullanÄ±m Rehberi

## RTX 4060 + 32GB RAM iÃ§in Optimize EdilmiÅŸ

### ğŸ“‹ HÄ±zlÄ± BaÅŸlangÄ±Ã§

#### 1. **Evet, QLoRA KullanÄ±yor!**
```
âœ… QLoRA (Quantized Low-Rank Adaptation) Features:
â”œâ”€â”€ 4-bit quantization (8x memory reduction)
â”œâ”€â”€ LoRA adapters (parameter efficient training)
â”œâ”€â”€ RTX 4060 optimized settings
â”œâ”€â”€ Flash Attention 2 support
â””â”€â”€ Gradient checkpointing
```

#### 2. **Kurulum**
```bash
# Gerekli paketleri yÃ¼kle
pip install torch>=2.0.0
pip install transformers>=4.36.0
pip install peft>=0.7.0
pip install trl>=0.7.0
pip install bitsandbytes>=0.41.0
pip install datasets>=2.15.0
```

#### 3. **Ã‡alÄ±ÅŸtÄ±rma**
```bash
cd UniqeAi/ai_model/scripts/
python QLORA_RTX4060_FINAL.py
```

---

## ğŸ¯ Sistem Ã–zellikleriniz iÃ§in Optimizasyonlar

### **RTX 4060 (8GB VRAM)**
```python
# Memory kullanÄ±mÄ±:
â”œâ”€â”€ Base model (4-bit): 4.0GB
â”œâ”€â”€ LoRA adapters: 0.2GB  
â”œâ”€â”€ Gradients: 0.3GB
â”œâ”€â”€ Optimizer: 0.5GB
â”œâ”€â”€ Activations: 2.0GB
â””â”€â”€ Buffer: 1.0GB
Total: ~8.0GB (tam kapasite)
```

### **AMD Ryzen 9 CPU**
```python
# CPU optimizasyonlarÄ±:
â”œâ”€â”€ Multi-threading: 8-12 threads
â”œâ”€â”€ Data loading: 4 workers
â”œâ”€â”€ Memory management: Efficient
â””â”€â”€ Thermal throttling: Optimized
```

### **32GB System RAM**
```python
# RAM kullanÄ±mÄ±:
â”œâ”€â”€ Model loading: ~6GB
â”œâ”€â”€ Dataset cache: ~2GB
â”œâ”€â”€ System overhead: ~4GB
â””â”€â”€ Available buffer: ~20GB
```

---

## ğŸ“Š QLoRA vs Normal Fine-tuning

| Aspect | Normal Fine-tuning | QLoRA |
|--------|-------------------|-------|
| **Memory** | 32GB (RTX 4060'a sÄ±ÄŸmaz) | 8GB âœ… |
| **Speed** | 1x baseline | 0.7x (biraz yavaÅŸ) |
| **Quality** | 100% | 95-98% (minimal kayÄ±p) |
| **Parameters** | 8B (tÃ¼mÃ¼) | 67M (0.84%) |
| **Cost** | Ã‡ok yÃ¼ksek | Ã‡ok dÃ¼ÅŸÃ¼k âœ… |

---

## âš™ï¸ KonfigÃ¼rasyon DetaylarÄ±

### **QLoRA Settings**
```python
config = {
    "lora_r": 64,              # Rank (kalite vs hÄ±z)
    "lora_alpha": 128,         # Scaling (2Ã—rank optimal)
    "batch_size": 1,           # RTX 4060 gÃ¼venli
    "gradient_accumulation": 32, # Effective batch = 32
    "learning_rate": 5e-5,     # Konservatif
    "num_epochs": 6            # 47 sample iÃ§in optimal
}
```

### **Quantization Settings**
```python
BitsAndBytesConfig(
    load_in_4bit=True,                    # 4-bit quantization
    bnb_4bit_use_double_quant=True,       # Extra compression
    bnb_4bit_quant_type="nf4",            # En kaliteli format
    bnb_4bit_compute_dtype=torch.bfloat16 # RTX 4060 native
)
```

---

## ğŸ“ˆ Beklenen Performans

### **Training SÃ¼resi**
```
RTX 4060 + QLoRA Timeline:
â”œâ”€â”€ Model loading: 3 dakika
â”œâ”€â”€ Setup: 2 dakika
â”œâ”€â”€ Training: 40 dakika (6 epoch Ã— 47 sample)
â”œâ”€â”€ Saving: 3 dakika
â””â”€â”€ Total: ~48 dakika
```

### **Memory Usage**
```
Peak VRAM: 7.9GB (RTX 4060'Ä±n %99'u)
Peak RAM: 12GB (32GB'Ä±n %37'si)
```

### **Model Quality**
```
Beklenen baÅŸarÄ± oranlarÄ±:
â”œâ”€â”€ TÃ¼rkÃ§e akÄ±cÄ±lÄ±k: %90+
â”œâ”€â”€ API format doÄŸruluÄŸu: %95+
â”œâ”€â”€ Domain bilgisi: %85+
â””â”€â”€ Genel baÅŸarÄ±: %90+
```

---

## ğŸ”§ Troubleshooting

### **âŒ CUDA Out of Memory**
```bash
# Ã‡Ã¶zÃ¼m 1: Batch size azalt
batch_size = 1  # Zaten minimum

# Ã‡Ã¶zÃ¼m 2: Sequence length azalt
max_seq_length = 1024  # 2048 yerine

# Ã‡Ã¶zÃ¼m 3: Gradient accumulation artÄ±r
gradient_accumulation = 64  # Daha bÃ¼yÃ¼k effective batch
```

### **âŒ Import Errors**
```bash
# BitsAndBytes hatasÄ±
pip uninstall bitsandbytes
pip install bitsandbytes --force-reinstall

# Flash Attention hatasÄ± (opsiyonel)
pip install flash-attn --no-build-isolation
```

### **âŒ Slow Training**
```python
# PyTorch optimizasyonlarÄ±
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_num_threads(8)  # Ryzen 9 iÃ§in
```

---

## ğŸ“ Ã‡Ä±ktÄ± DosyalarÄ±

### **Model Files**
```
./qlora_fine_tuned_model/
â”œâ”€â”€ adapter_config.json     # LoRA konfigÃ¼rasyonu
â”œâ”€â”€ adapter_model.bin       # LoRA weights
â”œâ”€â”€ tokenizer.json          # Tokenizer
â”œâ”€â”€ tokenizer_config.json   # Tokenizer config
â””â”€â”€ training_metadata.json  # Training bilgileri
```

### **Logs**
```
./training_logs/
â”œâ”€â”€ qlora_training_*.log    # Training logs
â””â”€â”€ tensorboard/            # TensorBoard logs
```

---

## ğŸ§ª Model Test Etme

### **Quick Test**
```python
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM

# Base model yÃ¼kle
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B-Instruct")

# LoRA adapters yÃ¼kle
model = PeftModel.from_pretrained(model, "./qlora_fine_tuned_model")

# Test
prompt = "### Instruction:\nKullanÄ±cÄ± bilgilerini getir\n\n### Response:"
# ... generation code
```

### **Performance Test**
```python
test_scenarios = [
    "KullanÄ±cÄ± bilgileri",
    "Fatura detaylarÄ±", 
    "Stok durumu",
    "Kampanya bilgileri"
]

for scenario in test_scenarios:
    # Test each scenario
    accuracy = evaluate_response(scenario)
    print(f"{scenario}: {accuracy}%")
```

---

## ğŸš€ Production Deployment

### **Model Merge**
```python
# LoRA'yÄ± base model ile birleÅŸtir (production iÃ§in)
merged_model = model.merge_and_unload()
merged_model.save_pretrained("./production_model")
```

### **Inference Optimization**
```python
# 8-bit inference iÃ§in optimize et
from optimum.quanto import quantize, qint8
quantize(merged_model, weights=qint8)
```

---

## âœ… Ã–zet: QLoRA AvantajlarÄ±

### **Sizin Sisteminiz Ä°Ã§in**
```
âœ… Memory efficient: 8GB VRAM'a sÄ±ÄŸar
âœ… Cost effective: Sadece LoRA weights train
âœ… High quality: %95+ model performance  
âœ… Fast training: ~48 dakika
âœ… Production ready: Merge & deploy
```

### **vs Alternatives**
```
Full Fine-tuning: âŒ 32GB VRAM (impossible)
8-bit Fine-tuning: ğŸŸ¡ 16GB VRAM (tight)
QLoRA: âœ… 8GB VRAM (perfect fit)
```

Bu QLoRA implementation tam fonksiyonel ve RTX 4060 + 32GB RAM sisteminiz iÃ§in optimize edilmiÅŸtir! ğŸ‰ 