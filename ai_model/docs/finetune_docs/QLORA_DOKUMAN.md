# ğŸ”¥ QLoRA Fine-tuning DetaylÄ± DÃ¶kÃ¼man

## RTX 4060 + 32GB RAM iÃ§in Optimize EdilmiÅŸ QLoRA Rehberi

### ğŸ“‹ Ä°Ã§indekiler
1. [QLoRA Teknolojisi](#qlora-teknolojisi)
2. [Sistem Analizi](#sistem-analizi)  
3. [Kod DetaylarÄ±](#kod-detaylarÄ±)
4. [Optimizasyonlar](#optimizasyonlar)
5. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ QLoRA Teknolojisi

### QLoRA = Quantized LoRA
**QLoRA**, bÃ¼yÃ¼k dil modellerini memory-efficient ÅŸekilde fine-tune etme tekniÄŸidir.

#### ğŸ” Ana BileÅŸenler:

**1. 4-bit Quantization**
```python
# Normal model: 32-bit â†’ 4-bit (8x memory reduction)
BitsAndBytesConfig(
    load_in_4bit=True,                    # 4-bit quantization
    bnb_4bit_use_double_quant=True,       # Extra compression
    bnb_4bit_quant_type="nf4",            # Best quality format
    bnb_4bit_compute_dtype=torch.bfloat16 # Computation precision
)
```

**2. LoRA Adapters**
```python
# Sadece kÃ¼Ã§Ã¼k adaptÃ¶r katmanlarÄ± eÄŸitir
LoraConfig(
    r=64,              # Rank (adaptÃ¶r boyutu)
    lora_alpha=128,    # Scaling (genellikle 2Ã—r)
    lora_dropout=0.05  # Overfitting Ã¶nleme
)
```

---

## ğŸ–¥ï¸ Sistem Analizi

### Sizin Sistem: RTX 4060 + 32GB RAM

| BileÅŸen | Ã–zellik | QLoRA PerformansÄ± |
|---------|---------|-------------------|
| **GPU** | RTX 4060 (8GB VRAM) | âœ… Ä°deal (4-8GB arasÄ± optimal) |
| **CPU** | AMD Ryzen 9 | âœ… MÃ¼kemmel (Ã§ok Ã§ekirdekli) |
| **RAM** | 32GB | âœ… Fazla bile (16GB yeterdi) |

### Memory KullanÄ±m Analizi:
```
Llama-3.1-8B Model:
â”œâ”€â”€ Normal (FP32): ~32GB     âŒ RTX 4060'a sÄ±ÄŸmaz
â”œâ”€â”€ Half (FP16): ~16GB       âŒ RTX 4060'a sÄ±ÄŸmaz
â”œâ”€â”€ 8-bit: ~8GB              ğŸŸ¡ Tam sÄ±nÄ±rda
â””â”€â”€ 4-bit QLoRA: ~4GB        âœ… Rahat Ã§alÄ±ÅŸÄ±r

QLoRA VRAM Breakdown:
â”œâ”€â”€ Base model (4-bit): 4.0GB
â”œâ”€â”€ LoRA adapters: 0.2GB
â”œâ”€â”€ Gradients: 0.3GB
â”œâ”€â”€ Optimizer: 0.5GB
â”œâ”€â”€ Activations: 2.0GB
â””â”€â”€ Buffer: 1.0GB
Total: ~8.0GB (RTX 4060 tam kapasite)
```

---

## ğŸ“ Kod DetaylarÄ±

### ğŸ”§ Core Setup

```python
def setup_quantization():
    """4-bit quantization configuration"""
    return BitsAndBytesConfig(
        load_in_4bit=True,                    # Enable 4-bit
        bnb_4bit_use_double_quant=True,       # Extra compression
        bnb_4bit_quant_type="nf4",            # NormalFloat4 (best)
        bnb_4bit_compute_dtype=torch.bfloat16 # RTX 4060 native
    )

def setup_lora():
    """LoRA configuration for high quality"""
    return LoraConfig(
        r=64,                    # High rank for quality
        lora_alpha=128,          # 2Ã—rank scaling
        target_modules=[         # Which layers to adapt
            "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
            "gate_proj", "up_proj", "down_proj"       # MLP
        ],
        lora_dropout=0.05,       # Low dropout for small dataset
        bias="none",             # Don't adapt bias
        task_type=TaskType.CAUSAL_LM
    )
```

### ğŸ›ï¸ Training Configuration

```python
# RTX 4060 optimal settings
config = {
    "batch_size": 1,           # Safe for 8GB VRAM
    "gradient_accumulation": 32, # Effective batch = 32
    "learning_rate": 5e-5,     # Conservative for stability
    "num_epochs": 6,           # More epochs for 47 samples
    "max_length": 2048,        # Full context length
}

training_args = TrainingArguments(
    per_device_train_batch_size=config["batch_size"],
    gradient_accumulation_steps=config["gradient_accumulation"],
    learning_rate=config["learning_rate"],
    num_train_epochs=config["num_epochs"],
    
    # Memory optimization
    gradient_checkpointing=True,    # 50% memory save
    bf16=True,                      # RTX 4060 native precision
    tf32=True,                      # Faster matrix ops
    
    # RTX 4060 optimizations
    optim="paged_adamw_8bit",       # Memory-efficient optimizer
    dataloader_num_workers=4,       # Ryzen 9 parallelization
)
```

### ğŸ“Š Data Processing

```python
def format_data(item):
    """Convert to instruction format"""
    if item["input"].strip():
        return f"""### Instruction:
{item['instruction']}

### Input:
{item['input']}

### Response:
{item['output']}"""
    else:
        return f"""### Instruction:
{item['instruction']}

### Response:
{item['output']}"""
```

---

## ğŸš€ Optimizasyonlar

### 1. Memory Optimization

```python
# PyTorch optimizations for RTX 4060
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.set_num_threads(8)  # Ryzen 9 optimization

# Memory management
torch.cuda.empty_cache()
torch.cuda.set_per_process_memory_fraction(0.95)
```

### 2. Speed Optimization

```python
# Flash Attention 2 (RTX 4060 supports)
attn_implementation="flash_attention_2"

# Gradient checkpointing
gradient_checkpointing=True  # Trade compute for memory

# Efficient data loading
dataloader_num_workers=4     # Ryzen 9 parallelization
dataloader_pin_memory=False  # Prevent VRAM conflicts
```

### 3. Quality Optimization

```python
# High-rank LoRA for better quality
r=64              # vs 16 (4x more parameters)
lora_alpha=128    # Proper scaling

# Conservative learning
learning_rate=5e-5   # Stable convergence
warmup_steps=10      # Gradual start
weight_decay=0.01    # Regularization
```

---

## ğŸ”§ Troubleshooting

### âŒ CUDA Out of Memory
```bash
# Ã‡Ã¶zÃ¼mler:
1. Batch size azalt: batch_size=1
2. Sequence length azalt: max_length=1024
3. Gradient accumulation artÄ±r: gradient_accumulation=64
4. Memory cleanup: torch.cuda.empty_cache()
```

### âŒ Import Errors
```bash
# Flash Attention sorunu:
pip uninstall flash-attn
pip install flash-attn --no-build-isolation

# BitsAndBytes sorunu:
pip install bitsandbytes>=0.41.0

# PEFT version:
pip install peft>=0.7.0
```

### âŒ Training Instability
```python
# Loss exploding:
learning_rate=1e-5        # Daha konservatif
max_grad_norm=0.5         # SÄ±kÄ± gradient clipping
warmup_steps=20           # Uzun warmup

# NaN values:
bf16=True                 # fp16 yerine bf16 kullan
gradient_checkpointing=True
```

---

## ğŸ“Š Performance Expectations

### â±ï¸ Training Time
```
RTX 4060 + QLoRA estimates:
â”œâ”€â”€ Model loading: 3 minutes
â”œâ”€â”€ Setup: 2 minutes  
â”œâ”€â”€ Training: 40 minutes (6 epochs Ã— 47 samples)
â”œâ”€â”€ Saving: 3 minutes
â””â”€â”€ Total: ~48 minutes
```

### ğŸ’¾ Memory Usage
```
Peak VRAM usage: 7.9GB (99% of 8GB)
â”œâ”€â”€ Model: 4.0GB
â”œâ”€â”€ LoRA: 0.2GB
â”œâ”€â”€ Optimizer: 0.5GB
â”œâ”€â”€ Gradients: 0.3GB
â”œâ”€â”€ Activations: 2.0GB
â””â”€â”€ Buffer: 0.9GB
```

### ğŸ¯ Quality Metrics
```
Expected performance:
â”œâ”€â”€ Turkish fluency: 90%+
â”œâ”€â”€ API format: 95%+
â”œâ”€â”€ Domain knowledge: 85%+
â””â”€â”€ Overall: 90%+ success rate
```

---

## ğŸ¯ RTX 4060 Specific Tips

### 1. **VRAM Management**
- Batch size 1 is optimal
- Use gradient accumulation for larger effective batch
- Enable gradient checkpointing
- Avoid pinned memory

### 2. **Speed Optimization**
- Enable Flash Attention 2
- Use bfloat16 (native RTX 4060 support)
- Enable TF32 for matrix operations
- Optimize CPU threads for Ryzen 9

### 3. **Quality vs Speed**
- Rank 64: Best quality for your VRAM
- 6 epochs: Optimal for 47 samples
- Conservative LR: Stable training

---

## ğŸ“ˆ Expected Results

Sisteminizde QLoRA fine-tuning ile:

âœ… **Training sÃ¼re**: ~45-50 dakika
âœ… **VRAM kullanÄ±mÄ±**: ~7.9GB (gÃ¼venli)  
âœ… **Model kalitesi**: %90+ baÅŸarÄ± oranÄ±
âœ… **TÃ¼rkÃ§e support**: MÃ¼kemmel
âœ… **API calling**: %95+ doÄŸruluk

Bu konfigÃ¼rasyon RTX 4060 + 32GB RAM sisteminiz iÃ§in **optimal** ayarlardÄ±r!