"""
Llama-3.1-8B-Instruct Fine-tuning Script
========================================

Bu script, TÃ¼rkÃ§e Ã§aÄŸrÄ± merkezi senaryolarÄ± iÃ§in Llama-3.1-8B-Instruct modelini
LoRA (Low-Rank Adaptation) kullanarak fine-tune eder.

KullanÄ±m:
    python run_finetune.py

Gereksinimler:
    - CUDA destekli GPU (8GB+ VRAM Ã¶nerilir)
    - complete_training_dataset.json dosyasÄ±
    - requirements.txt'teki tÃ¼m paketler
"""

import os
import json
import torch
import logging
from datetime import datetime
from typing import Dict, List, Optional

from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    BitsAndBytesConfig,
    EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training
from trl import SFTTrainer

# Logging konfigÃ¼rasyonu
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class FineTuner:
    def __init__(self, model_name: str = "meta-llama/Llama-3.1-8B-Instruct"):
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None
        self.tokenizer = None
        
        # Dosya yollarÄ±
        self.data_path = "../data/complete_training_dataset.json"
        self.output_dir = "./fine_tuned_model"
        self.logs_dir = "./training_logs"
        
        # Dizinleri oluÅŸtur
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(self.logs_dir, exist_ok=True)
        
    def setup_logging(self):
        """Training loglarÄ±nÄ± ayarla"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        log_file = os.path.join(self.logs_dir, f"training_{timestamp}.log")
        
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        logger.info(f"Training baÅŸlatÄ±ldÄ±: {timestamp}")
        logger.info(f"Model: {self.model_name}")
        logger.info(f"Device: {self.device}")
        
    def check_gpu_memory(self):
        """GPU bellek durumunu kontrol et"""
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            logger.info(f"GPU Bellek: {gpu_memory:.1f} GB")
            
            if gpu_memory < 8:
                logger.warning("âš ï¸ 8GB'den az GPU belleÄŸi detected. 4-bit quantization kullanÄ±lacak.")
                return True  # Quantization gerekli
            return False  # Quantization opsiyonel
        else:
            logger.warning("âš ï¸ CUDA bulunamadÄ±. CPU kullanÄ±lacak (Ã§ok yavaÅŸ olacak).")
            return False

    def load_and_prepare_dataset(self) -> Dataset:
        """Veri setini yÃ¼kle ve SFTTrainer formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r"""
        logger.info("Veri seti yÃ¼kleniyor...")
        
        # JSON dosyasÄ±nÄ± yÃ¼kle
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"Veri dosyasÄ± bulunamadÄ±: {self.data_path}")
            
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"Toplam {len(data)} veri noktasÄ± yÃ¼klendi")
        
        # Instruction tuning formatÄ±nÄ± tek metin alanÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
        formatted_data = []
        for item in data:
            # Alpaca formatÄ±: Instruction, Input, Output'u birleÅŸtir
            if item["input"].strip():
                text = f"### Instruction:\n{item['instruction']}\n\n### Input:\n{item['input']}\n\n### Response:\n{item['output']}"
            else:
                text = f"### Instruction:\n{item['instruction']}\n\n### Response:\n{item['output']}"
                
            formatted_data.append({"text": text})
        
        # Dataset oluÅŸtur
        dataset = Dataset.from_list(formatted_data)
        
        # Ã–rnek veriyi gÃ¶ster
        logger.info("Ã–rnek veri noktasÄ±:")
        logger.info(f"Text: {formatted_data[0]['text'][:200]}...")
        
        return dataset

    def load_model_and_tokenizer(self, use_quantization: bool = True):
        """Model ve tokenizer'Ä± yÃ¼kle"""
        logger.info("Model ve tokenizer yÃ¼kleniyor...")
        
        # Quantization konfigÃ¼rasyonu
        bnb_config = None
        if use_quantization and self.device == "cuda":
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            logger.info("4-bit quantization aktif")

        # Tokenizer yÃ¼kle
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # Llama modellerinde pad token eksik
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
        logger.info("Tokenizer yÃ¼klendi")

        # Model yÃ¼kle
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if self.device == "cuda" else torch.float32,
            attn_implementation="flash_attention_2" if self.device == "cuda" else None
        )
        
        logger.info("Model yÃ¼klendi")
        
        # Quantized model iÃ§in ek hazÄ±rlÄ±k
        if use_quantization and self.device == "cuda":
            self.model = prepare_model_for_kbit_training(self.model)
            logger.info("Model quantization iÃ§in hazÄ±rlandÄ±")

    def setup_lora_config(self) -> LoraConfig:
        """LoRA konfigÃ¼rasyonunu ayarla"""
        logger.info("LoRA konfigÃ¼rasyonu ayarlanÄ±yor...")
        
        lora_config = LoraConfig(
            r=16,  # Rank - daha yÃ¼ksek = daha fazla parameter
            lora_alpha=32,  # Alpha - learning rate scaling
            target_modules=[
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        logger.info(f"LoRA rank: {lora_config.r}")
        logger.info(f"LoRA alpha: {lora_config.lora_alpha}")
        
        return lora_config

    def setup_training_arguments(self, dataset_size: int) -> TrainingArguments:
        """EÄŸitim argÃ¼manlarÄ±nÄ± ayarla"""
        logger.info("EÄŸitim parametreleri ayarlanÄ±yor...")
        
        # Veri boyutuna gÃ¶re epoch sayÄ±sÄ±nÄ± ayarla
        if dataset_size < 50:
            num_epochs = 3
        elif dataset_size < 100:
            num_epochs = 2
        else:
            num_epochs = 1
            
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=1,
            gradient_accumulation_steps=8,  # Effective batch size = 8
            learning_rate=1e-4,
            warmup_steps=5,
            logging_steps=1,
            save_steps=max(5, dataset_size // 10),  # Her %10'da bir kaydet
            eval_steps=max(5, dataset_size // 10),
            save_total_limit=2,
            remove_unused_columns=False,
            dataloader_pin_memory=False,
            bf16=True if self.device == "cuda" else False,
            fp16=False,
            gradient_checkpointing=True,
            optim="adamw_bnb_8bit" if self.device == "cuda" else "adamw_hf",
            lr_scheduler_type="cosine",
            report_to="none",  # Wandb vs. kullanmÄ±yoruz
            run_name=f"llama-finetune-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
        
        logger.info(f"Epoch sayÄ±sÄ±: {num_epochs}")
        logger.info(f"Batch size: {training_args.per_device_train_batch_size}")
        logger.info(f"Gradient accumulation: {training_args.gradient_accumulation_steps}")
        logger.info(f"Learning rate: {training_args.learning_rate}")
        
        return training_args

    def fine_tune(self):
        """Ana fine-tuning sÃ¼reci"""
        try:
            # Kurulum
            self.setup_logging()
            use_quantization = self.check_gpu_memory()
            
            # Veri setini hazÄ±rla
            dataset = self.load_and_prepare_dataset()
            
            # Model ve tokenizer'Ä± yÃ¼kle
            self.load_model_and_tokenizer(use_quantization)
            
            # LoRA konfigÃ¼rasyonu
            lora_config = self.setup_lora_config()
            
            # EÄŸitim argÃ¼manlarÄ±
            training_args = self.setup_training_arguments(len(dataset))
            
            # LoRA modelini hazÄ±rla
            self.model = get_peft_model(self.model, lora_config)
            self.model.print_trainable_parameters()
            
            logger.info("SFTTrainer oluÅŸturuluyor...")
            
            # Trainer oluÅŸtur
            trainer = SFTTrainer(
                model=self.model,
                train_dataset=dataset,
                tokenizer=self.tokenizer,
                args=training_args,
                dataset_text_field="text",
                max_seq_length=1024,
                packing=False,  # Veri seti kÃ¼Ã§Ã¼k olduÄŸu iÃ§in packing kullanma
                callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
            )
            
            logger.info("ðŸš€ EÄŸitim baÅŸlatÄ±lÄ±yor...")
            
            # EÄŸitimi baÅŸlat
            trainer.train()
            
            logger.info("âœ… EÄŸitim tamamlandÄ±!")
            
            # Modeli kaydet
            logger.info("Model kaydediliyor...")
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            
            # EÄŸitim loglarÄ±nÄ± kaydet
            with open(os.path.join(self.output_dir, "training_log.json"), "w") as f:
                json.dump(trainer.state.log_history, f, indent=2)
            
            logger.info(f"Model kaydedildi: {self.output_dir}")
            
            # GPU bellek temizliÄŸi
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            return True
            
        except Exception as e:
            logger.error(f"EÄŸitim sÄ±rasÄ±nda hata: {str(e)}")
            return False

    def test_model(self, test_prompts: Optional[List[str]] = None):
        """EÄŸitilmiÅŸ modeli test et"""
        if test_prompts is None:
            test_prompts = [
                "### Instruction:\nKullanÄ±cÄ± bilgilerini getir\n\n### Input:\nID'si 12345 olan kullanÄ±cÄ±nÄ±n bilgilerini al\n\n### Response:",
                "### Instruction:\nFatura detayÄ±nÄ± getir\n\n### Input:\nMÃ¼ÅŸteri 5551234567 numaralÄ± hattÄ±nÄ±n Ocak 2024 fatura detayÄ±nÄ± gÃ¶ster\n\n### Response:"
            ]
        
        logger.info("Model test ediliyor...")
        
        for i, prompt in enumerate(test_prompts):
            logger.info(f"\n--- Test {i+1} ---")
            logger.info(f"Prompt: {prompt[:100]}...")
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            logger.info(f"Response: {response[len(prompt):]}")

def main():
    """Ana fonksiyon"""
    print("ðŸ”¥ Llama-3.1-8B-Instruct Fine-tuning BaÅŸlatÄ±lÄ±yor...")
    print("=" * 60)
    
    # Fine-tuner oluÅŸtur
    fine_tuner = FineTuner()
    
    # Fine-tuning iÅŸlemini baÅŸlat
    success = fine_tuner.fine_tune()
    
    if success:
        print("\nðŸŽ‰ Fine-tuning baÅŸarÄ±yla tamamlandÄ±!")
        print(f"ðŸ“ Model lokasyonu: {fine_tuner.output_dir}")
        
        # Modeli test et (opsiyonel)
        response = input("\nModeli test etmek istiyor musunuz? (y/n): ")
        if response.lower() == 'y':
            fine_tuner.test_model()
    else:
        print("\nâŒ Fine-tuning baÅŸarÄ±sÄ±z oldu. LoglarÄ± kontrol edin.")

if __name__ == "__main__":
    main() 