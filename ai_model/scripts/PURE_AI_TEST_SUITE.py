# -*- coding: utf-8 -*-
"""
ğŸ§  PURE AI TEST SUITE v1.0 (GerÃ§ek AI Yetenek Testi)
=====================================================

Bu test, modelin EÄÄ°TÄ°MDEN Ã¶ÄŸrendiklerini kullanmasÄ±nÄ± test eder.
- âŒ AraÃ§ listesi verilmez
- âŒ Parametre Ã¶rnekleri verilmez  
- âœ… Model sadece eÄŸitiminden bildiÄŸi araÃ§larÄ± kullanÄ±r
- âœ… GerÃ§ek AI yeteneÄŸi Ã¶lÃ§Ã¼lÃ¼r

Bu, modelin gerÃ§ek zekasÄ±nÄ± ve Ã¶ÄŸrenme kapasitesini test eder!
"""

import os
import json
import re
import sys
import time
from pathlib import Path
from rich.console import Console
from rich.markdown import Markdown
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

# --- Proje KÃ¶k Dizini ve ModÃ¼l Yolu ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except (NameError, IndexError):
    PROJECT_ROOT = Path.cwd()

AI_MODEL_SCRIPTS_PATH = PROJECT_ROOT / "UniqeAi" / "ai_model" / "scripts"
if str(AI_MODEL_SCRIPTS_PATH) not in sys.path:
    sys.path.insert(0, str(AI_MODEL_SCRIPTS_PATH))

try:
    from tool_definitions import get_tool_definitions, get_tool_response
except ImportError:
    print(f"\n[HATA] 'tool_definitions' modÃ¼lÃ¼ bulunamadÄ±. Aranan yol: {AI_MODEL_SCRIPTS_PATH}")
    sys.exit(1)

# --- Model YapÄ±landÄ±rmasÄ± (terminal_app_gguf.py v8.0 ile senkronize) ---
GGUF_MODEL_DIR = PROJECT_ROOT / "UniqeAi" / "ai_model" / "results_2" / "gguf_model_v2"
CONTEXT_SIZE = 2048  # Performans iÃ§in optimize edildi
GPU_LAYERS = 35      # Optimum GPU katman sayÄ±sÄ±
TEMPERATURE = 0.2    # YaratÄ±cÄ±lÄ±k iÃ§in hafif artÄ±rÄ±ldÄ± (0.0 Ã§ok katÄ±ydÄ±)
DEFAULT_TEST_USER_ID = 12345

console = Console()
ALL_TOOL_DEFINITIONS = {tool['function']['name']: tool for tool in get_tool_definitions()}


class PureAITester:
    """Modelin GERÃ‡EK AI yeteneÄŸini test eden sÄ±nÄ±f - AraÃ§ listesi verilmez!"""
    
    def __init__(self, llm_model):
        self.llm = llm_model
        self.test_results = {}
        
    def create_system_prompt(self) -> str:
        """ğŸ§  GERÃ‡EK AI YETENEÄÄ° TESTÄ° - Minimal prompt, araÃ§ listesi YOK!"""
        # ARAÃ‡ LÄ°STESÄ° VERÄ°LMÄ°YOR - Model eÄŸitiminden Ã¶ÄŸrendiklerini kullanacak
        prompt = """Sen, UniqeAi tarafÄ±ndan geliÅŸtirilmiÅŸ, uzman bir Telekom MÃ¼ÅŸteri Hizmetleri AsistanÄ±sÄ±n.

**ANA GÃ–REVÄ°N:** KullanÄ±cÄ±nÄ±n talebini anÄ±nda yerine getirmek. Asla sohbet etme, soru sorma veya aÃ§Ä±klama yapma. **Sadece istenen eylemi gerÃ§ekleÅŸtir.**

**ARAÃ‡ KULLANIM KURALI:** Bir aracÄ± kullanman gerekiyorsa, baÅŸka hiÃ§bir ÅŸey yazmadan **SADECE araÃ§ Ã§aÄŸÄ±rma kodunu** ÅŸu formatta yaz: 
`<|begin_of_tool_code|>print(fonksiyon(parametre="deÄŸer"))<|end_of_tool_code|>`

**NOT:** Hangi araÃ§larÄ±n mevcut olduÄŸunu eÄŸitiminden biliyorsun. Uygun olanÄ± seÃ§ ve kullan."""
        return prompt.strip()

    def parse_tool_calls(self, text: str) -> Tuple[Optional[List[Dict[str, Any]]], str]:
        """HATA TOLERANSLI AYRIÅTIRICI: print() olsa da olmasa da araÃ§ Ã§aÄŸrÄ±sÄ±nÄ± yakalar."""
        pattern_with_print = r"<\|begin_of_tool_code\|>\s*print\((\w+)\((.*?)\)\)\s*<\|end_of_tool_code\|>"
        pattern_without_print = r"<\|begin_of_tool_code\|>\s*(\w+)\((.*?)\)\s*<\|end_of_tool_code\|>"
        
        match = re.search(pattern_with_print, text, re.DOTALL)
        pattern_used = pattern_with_print
        if not match:
            match = re.search(pattern_without_print, text, re.DOTALL)
            pattern_used = pattern_without_print

        if not match: return None, text

        function_name, args_str = match.group(1), match.group(2)
        params = {}
        if args_str:
            arg_pattern = re.compile(r"(\w+)\s*=\s*((?:\"(?:\\\"|[^\"])*\")|(?:'(?:\\'|[^'])*')|[\w.-]+)")
            for p_match in arg_pattern.finditer(args_str):
                key, raw_value = p_match.group(1), p_match.group(2)
                try: value = json.loads(raw_value)
                except (json.JSONDecodeError, TypeError): value = str(raw_value).strip("'\"")
                params[key] = value

        cleaned_text = re.sub(pattern_used, '', text).strip()
        tool_call = {"function": {"name": function_name, "arguments": params}}
        return [tool_call], cleaned_text

    def run_test_scenario(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Tek bir test senaryosunu Ã§alÄ±ÅŸtÄ±rÄ±r - PURE AI MOD!"""
        console.print(f"\nğŸ§  [bold blue]PURE AI Test:[/bold blue] {scenario['name']}")
        console.print(f"ğŸ“‹ [italic]{scenario['description']}[/italic]")
        
        system_prompt = self.create_system_prompt()
        dialogue = [{"role": "system", "content": system_prompt}, {"role": "user", "content": scenario['user_input']}]
        
        console.print(f"\nğŸ‘¤ [bold blue]KullanÄ±cÄ±:[/bold blue] {scenario['user_input']}")
        
        full_response = ""
        final_answer = ""
        tool_calls_detected = False

        with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), console=console) as progress:
            task = progress.add_task("ğŸ§  Model eÄŸitiminden hatÄ±rlÄ±yor...", total=None)
            try:
                response = self.llm.create_chat_completion(
                    messages=dialogue, temperature=TEMPERATURE, stop=["<|eot_id|>"]
                )
                assistant_response_text = response['choices'][0]['message']['content']
                
                tool_calls, cleaned_response = self.parse_tool_calls(assistant_response_text)
                progress.update(task, description="YanÄ±t analiz ediliyor...")

                if tool_calls:
                    tool_calls_detected = True
                    dialogue.append({"role": "assistant", "content": assistant_response_text})
                    
                    for call in tool_calls:
                        func_name, func_args = call["function"]["name"], call["function"]["arguments"]
                        
                        # GERÃ‡EK AI TESTÄ° - Parametre eksikse model baÅŸarÄ±sÄ±z!
                        if func_name in ALL_TOOL_DEFINITIONS:
                            func_def = ALL_TOOL_DEFINITIONS[func_name]["function"]
                            required_params = func_def.get("parameters", {}).get("required", [])
                            
                            # Sadece user_id iÃ§in yardÄ±m et (Ã§Ã¼nkÃ¼ test ortamÄ±nda sabit)
                            if "user_id" in required_params and "user_id" not in func_args:
                                func_args["user_id"] = DEFAULT_TEST_USER_ID
                                console.print(f"ğŸ§  [italic yellow]AI EksikliÄŸi: Model 'user_id' parametresini unuttu.[/italic yellow]")
                        
                        console.print(f"ğŸ› ï¸ [bold yellow]AI'nÄ±n SeÃ§tiÄŸi AraÃ§:[/bold yellow] {func_name}({json.dumps(func_args, ensure_ascii=False)})")
                        tool_response = get_tool_response(func_name, func_args)
                        console.print(f"âš™ï¸ [bold magenta]AraÃ§ YanÄ±tÄ±:[/bold magenta] {tool_response}")
                        dialogue.append({"role": "tool", "content": tool_response})

                    # Ã–zetleme prompt'u - GerÃ§ek veriyi kullanmasÄ± iÃ§in
                    chain_of_thought_prompt = f"""UYARI: YukarÄ±daki API yanÄ±tÄ± gerÃ§ek veridir. Bu veriyi AYNEN kullan.

AÅŸaÄŸÄ±daki JSON'u KELIME KELIME kopyala ve TÃ¼rkÃ§e aÃ§Ä±kla:

{tool_response}

KURALLARI:
1. Bu JSON'daki rakamlarÄ± AYNEN kullan
2. Kendi rakam EKLEME
3. Kendi JSON yaratma
4. Sadece bu gerÃ§ek veriyi Ã¶zetle

GerÃ§ek JSON'dan TÃ¼rkÃ§e Ã¶zet:"""
                    dialogue.append({"role": "user", "content": chain_of_thought_prompt})
                    
                    progress.update(task, description="Model sonuÃ§larÄ± Ã¶zetliyor...")
                    
                    summary_response = self.llm.create_chat_completion(
                        messages=dialogue, 
                        temperature=0.0,  # HalÃ¼sinasyonu Ã¶nlemek iÃ§in sÄ±fÄ±r
                        stop=["<|eot_id|>"],
                        max_tokens=256,
                        repeat_penalty=1.2,
                        top_k=1,  # En muhtemel kelimeyi seÃ§
                        top_p=0.1  # Ã‡ok dÃ¼ÅŸÃ¼k yaratÄ±cÄ±lÄ±k
                    )
                    final_answer = summary_response['choices'][0]['message']['content']
                    dialogue.pop() # GeÃ§ici CoT prompt'unu kaldÄ±r
                    dialogue.append({"role": "assistant", "content": final_answer})
                    
                    console.print(f"\nğŸ§  [bold green]Pure AI YanÄ±tÄ±:[/bold green]")
                    console.print(Markdown(final_answer))
                    full_response = assistant_response_text + "\n\n" + final_answer
                else:
                    final_answer = cleaned_response
                    dialogue.append({"role": "assistant", "content": final_answer})
                    console.print(f"\nğŸ§  [bold green]Pure AI YanÄ±tÄ±:[/bold green]")
                    console.print(Markdown(final_answer))
                    full_response = final_answer
                
                progress.remove_task(task)
                score = self.evaluate_response(scenario, final_answer, tool_calls_detected)
                
                return {
                    "scenario_name": scenario['name'], "user_input": scenario['user_input'],
                    "model_response": full_response, "used_tools": tool_calls_detected,
                    "evaluation": score, "dialogue": dialogue
                }
            except Exception as e:
                progress.remove_task(task)
                console.print(f"[bold red]HATA:[/bold red] {e}")
                return {"scenario_name": scenario['name'], "error": str(e), "evaluation": {"total_score": 0}}

    def evaluate_response(self, scenario: Dict[str, Any], response: str, used_tools: bool) -> Dict[str, Any]:
        """Model yanÄ±tÄ±nÄ± deÄŸerlendirir - PURE AI iÃ§in Ã¶zel kriterler"""
        scores = {}
        scores['tool_selection'] = 10 if used_tools and scenario.get('expects_tool_usage', False) else (8 if not used_tools and not scenario.get('expects_tool_usage', False) else 0)
        scores['accuracy'] = self.score_accuracy(response, scenario.get('expected_topics', []))
        scores['completeness'] = self.score_completeness(response)
        scores['natural_language'] = self.score_natural_language(response)
        
        total_score = sum(scores.values()) / len(scores) if scores else 0
        return {"individual_scores": scores, "total_score": round(total_score, 2), "grade": self.get_grade(total_score)}

    def score_accuracy(self, r: str, topics: List[str]) -> float:
        if not topics: return 8.0
        found = sum(1 for t in topics if t.lower() in r.lower())
        return min(10.0, (found / len(topics)) * 10)

    def score_completeness(self, r: str) -> float:
        # YanÄ±t uzunluÄŸu ve detay seviyesi
        return min(10.0, len(r.split()) / 20)

    def score_natural_language(self, r: str) -> float:
        # DoÄŸal dil kalitesi
        indicators = ['size', 'sizin', 'faturanÄ±z', 'paketiniz', 'hizmetiniz']
        found = sum(1 for i in indicators if i in r.lower())
        return min(10.0, found * 2 + 2)

    def get_grade(self, s: float) -> str:
        if s >= 9.0: return "A+ (OlaÄŸanÃ¼stÃ¼)"
        elif s >= 8.0: return "A (MÃ¼kemmel)"
        elif s >= 7.0: return "B+ (Ã‡ok Ä°yi)"
        elif s >= 6.0: return "B (Ä°yi)"
        elif s >= 5.0: return "C+ (Orta)"
        else: return "F (BaÅŸarÄ±sÄ±z)"

def find_latest_gguf_model(model_dir: Path) -> Optional[Path]:
    if not model_dir.exists(): return None
    gguf_files = list(model_dir.glob("*.gguf"))
    if not gguf_files: return None
    return max(gguf_files, key=lambda p: p.stat().st_mtime)

def load_gguf_model():
    try: from llama_cpp import Llama
    except ImportError: console.print("[bold red]HATA: `llama-cpp-python` kurulu deÄŸil.[/bold red]"); sys.exit(1)
    model_path = find_latest_gguf_model(GGUF_MODEL_DIR)
    if not model_path: console.print("[bold red]HATA: GGUF model bulunamadÄ±![/bold red]"); sys.exit(1)
    console.print(f"[yellow]ğŸ§  Pure AI Model yÃ¼kleniyor: [cyan]{model_path.name}[/cyan][/yellow]")
    try:
        llm = Llama(
            model_path=str(model_path), 
            n_ctx=CONTEXT_SIZE, 
            n_gpu_layers=GPU_LAYERS,
            n_threads=os.cpu_count() - 1 if os.cpu_count() and os.cpu_count() > 1 else 1,
            verbose=False, 
            chat_format="llama-3",
            # Performans optimizasyonlarÄ± (terminal_app_gguf.py ile aynÄ±)
            n_batch=512,
            use_mlock=True,
            use_mmap=True
        )
        console.print("[green]âœ… Pure AI Model baÅŸarÄ±yla yÃ¼klendi.[/green]")
        return llm
    except Exception as e:
        console.print(f"[bold red]HATA: Model yÃ¼klenemedi - {e}[/bold red]"); sys.exit(1)

def get_test_scenarios() -> List[Dict[str, Any]]:
    """GerÃ§ek AI yeteneÄŸini test eden senaryolar"""
    return [
        {"name": "ğŸ§  AI Fatura Testi", "description": "Model eÄŸitiminden hangi aracÄ± seÃ§eceÄŸini biliyor mu?", "user_input": "Bu ayki faturamÄ± gÃ¶rebilir miyim?", "expects_tool_usage": True, "expected_topics": ["fatura", "tutar"], "ai_challenge": "get_current_bill araÃ§Ä±nÄ± hatÄ±rlayabilecek mi?"},
        
        {"name": "ğŸ§  AI GeÃ§miÅŸ Testi", "description": "Model geÃ§miÅŸ faturalar iÃ§in doÄŸru aracÄ± seÃ§ebilir mi?", "user_input": "Son 3 ayÄ±n faturalarÄ±nÄ± gÃ¶ster lÃ¼tfen.", "expects_tool_usage": True, "expected_topics": ["geÃ§miÅŸ", "fatura", "liste"], "ai_challenge": "get_past_bills araÃ§Ä±nÄ± bulabilecek mi?"},
        
        {"name": "ğŸ§  AI Paket Testi", "description": "Model paket bilgisi iÃ§in uygun aracÄ± hatÄ±rlÄ±yor mu?", "user_input": "Hangi paketi kullanÄ±yorum ÅŸu anda?", "expects_tool_usage": True, "expected_topics": ["paket", "tarife"], "ai_challenge": "get_customer_package araÃ§Ä±nÄ± seÃ§ebilecek mi?"},
        
        {"name": "ğŸ§  AI AÄŸ Testi", "description": "Model aÄŸ durumu kontrolÃ¼ iÃ§in doÄŸru aracÄ± biliyor mu?", "user_input": "Ä°stanbul'da internet sorunu var mÄ±?", "expects_tool_usage": True, "expected_topics": ["aÄŸ", "durum", "sorun"], "ai_challenge": "check_network_status araÃ§Ä±nÄ± hatÄ±rlayabilecek mi?"},
        
        {"name": "ğŸ§  AI Roaming Testi", "description": "Model roaming aktivasyonu iÃ§in uygun aracÄ± seÃ§ebilir mi?", "user_input": "YarÄ±n Almanya'ya gidiyorum, roaming aÃ§ar mÄ±sÄ±nÄ±z?", "expects_tool_usage": True, "expected_topics": ["roaming", "aktivasyon"], "ai_challenge": "enable_roaming araÃ§Ä±nÄ± bulabilecek mi?"},
        
        {"name": "ğŸ§  AI Sohbet Testi", "description": "Model ne zaman araÃ§ kullanmayacaÄŸÄ±nÄ± biliyor mu?", "user_input": "Merhaba, nasÄ±lsÄ±nÄ±z?", "expects_tool_usage": False, "expected_topics": ["selamlaÅŸma"], "ai_challenge": "Gereksiz araÃ§ Ã§aÄŸrÄ±sÄ± yapmayacak mÄ±?"}
    ]

def display_test_results(results: List[Dict[str, Any]]):
    """Pure AI test sonuÃ§larÄ±nÄ± gÃ¶rsel olarak sunar"""
    table = Table(title="ğŸ§  PURE AI TEST RESULTS - GerÃ§ek Zeka Testi")
    table.add_column("AI Test Senaryosu", style="cyan")
    table.add_column("AI Skoru", justify="center", style="green")
    table.add_column("AI Notu", justify="center", style="yellow")
    table.add_column("AraÃ§ SeÃ§imi", justify="center", style="blue")
    table.add_column("AI Challenge", style="magenta")
    
    total_score, valid_results = 0, 0
    for result in results:
        if 'error' not in result:
            score, grade, used_tools = result['evaluation']['total_score'], result['evaluation']['grade'], "ğŸ§ âœ…" if result['used_tools'] else "ğŸ§ âŒ"
            # Test senaryosundan AI challenge bilgisini al
            challenge = next((s['ai_challenge'] for s in get_test_scenarios() if s['name'] == result['scenario_name']), "Bilinmiyor")
            table.add_row(result['scenario_name'], f"{score:.1f}/10", grade, used_tools, challenge)
            total_score += score; valid_results += 1
        else:
            table.add_row(result['scenario_name'], "ERROR", "F", "ğŸ§ âŒ", "Test baÅŸarÄ±sÄ±z")
    
    console.print(table)
    if valid_results > 0:
        avg_score = total_score / valid_results
        console.print(Panel(f"ğŸ§  **Pure AI PerformansÄ±:** {avg_score:.1f}/10 ({PureAITester(None).get_grade(avg_score)})\n\nğŸ’¡ **AI DeÄŸerlendirmesi:**\n{get_ai_assessment(avg_score)}", title="ğŸ§  PURE AI ASSESSMENT", expand=False))

def get_ai_assessment(s: float) -> str:
    if s >= 9.0: return "ğŸŒŸ Model olaÄŸanÃ¼stÃ¼ AI yeteneÄŸi sergiliyor! EÄŸitimden Ã¶ÄŸrendiklerini mÃ¼kemmel ÅŸekilde hatÄ±rlÄ±yor."
    elif s >= 8.0: return "ğŸ‰ Model mÃ¼kemmel AI performansÄ± gÃ¶steriyor! AraÃ§larÄ± doÄŸru seÃ§ip kullanabiliyor."
    elif s >= 7.0: return "ğŸ‘ Model iyi AI yeteneÄŸi sergiliyor. Ã‡oÄŸu durumda doÄŸru araÃ§larÄ± hatÄ±rlÄ±yor."
    elif s >= 5.0: return "âš ï¸ Model orta seviye AI yeteneÄŸi gÃ¶steriyor. BazÄ± araÃ§larÄ± hatÄ±rlÄ±yor ama geliÅŸime aÃ§Ä±k."
    else: return "ğŸ”´ Model AI eÄŸitimini tam olarak Ã¶ÄŸrenememiÅŸ. AraÃ§larÄ± hatÄ±rlamakta zorlanÄ±yor."

def main():
    """Pure AI Test Ana ProgramÄ±"""
    console.print("\n" + "="*80, style="bold blue")
    console.print("ğŸ§  [bold blue]PURE AI TEST SUITE v1.0 (GerÃ§ek Zeka Testi)[/bold blue]", justify="center")
    console.print("   âŒ AraÃ§ listesi verilmez")
    console.print("   âŒ Parametre Ã¶rnekleri verilmez")
    console.print("   âœ… Model sadece eÄŸitiminden hatÄ±rladÄ±klarÄ±nÄ± kullanÄ±r")
    console.print("   ğŸ§  GerÃ§ek AI yeteneÄŸi test edilir")
    console.print("="*80, style="bold blue")
    
    llm_model = load_gguf_model()
    tester = PureAITester(llm_model)
    test_scenarios = get_test_scenarios()
    
    console.print(f"\nğŸ§  [bold blue]{len(test_scenarios)} adet Pure AI test senaryosu yÃ¼klendi.[/bold blue]")
    start_test = console.input("ğŸ§  Pure AI testine baÅŸlamak iÃ§in ENTER'a basÄ±n (Ã§Ä±kmak iÃ§in 'q'): ")
    if start_test.lower() == 'q': return
    
    results = [tester.run_test_scenario(s) for s in test_scenarios]
    console.print(f"\n{'='*80}\nğŸ§  [bold blue]PURE AI TEST TAMAMLANDI - SONUÃ‡LAR[/bold blue]\n{'='*80}")
    display_test_results(results)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = PROJECT_ROOT / f"UniqeAi/ai_model/pure_ai_test_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    console.print(f"\nğŸ’¾ [green]Pure AI test sonuÃ§larÄ± kaydedildi: {results_file}[/green]")
    console.print("\nğŸ§  [bold blue]Pure AI Test Suite tamamlandÄ±![/bold blue]")

if __name__ == "__main__":
    main()