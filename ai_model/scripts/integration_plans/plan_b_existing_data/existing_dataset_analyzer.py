#!/usr/bin/env python3
"""
Plan B: Mevcut Veri Seti Ä°ÅŸleme ve Ä°yileÅŸtirme
==============================================

Bu script, mevcut veri setlerini analiz eder, API uyumluluÄŸunu deÄŸerlendirir
ve iyileÅŸtirme Ã¶nerileri sunar.

Ã–zellikler:
- Mevcut veri setlerini otomatik keÅŸif
- API uyumluluk analizi
- Veri kalitesi deÄŸerlendirmesi
- Ä°yileÅŸtirme Ã¶nerileri
- Otomatik dÃ¼zeltme ve dÃ¶nÃ¼ÅŸtÃ¼rme
"""

import os
import json
import pandas as pd
from datetime import datetime
from pathlib import Path
import sys
from typing import Dict, List, Any, Optional

# Ana dizini Python path'ine ekle
sys.path.append(str(Path(__file__).parent.parent.parent))

from telekom_api_schema import TelekomAPI
from ultimate_api_compatibility_system import UltimateAPIFieldMapper, UltimatePydanticValidator

class ExistingDatasetAnalyzer:
    def __init__(self):
        self.output_dir = Path("data/existing_data_analysis")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Telekom API ÅŸemasÄ±
        self.api_schema = TelekomAPI()
        
        # API uyumluluk sistemleri
        self.field_mapper = UltimateAPIFieldMapper()
        self.validator = UltimatePydanticValidator()
        
        # Analiz sonuÃ§larÄ±
        self.analysis_results = {}
        
    def discover_existing_datasets(self) -> List[Path]:
        """Mevcut veri setlerini keÅŸfet"""
        print("ğŸ” Mevcut veri setleri keÅŸfediliyor...")
        
        # OlasÄ± veri seti konumlarÄ±
        search_paths = [
            Path("."),  # Mevcut dizin
            Path("data"),
            Path("datasets"),
            Path("ai_model/data"),
            Path("ai_model/datasets"),
            Path("../data"),
            Path("../datasets")
        ]
        
        discovered_files = []
        
        for search_path in search_paths:
            if search_path.exists():
                # JSON dosyalarÄ±nÄ± ara
                json_files = list(search_path.rglob("*.json"))
                discovered_files.extend(json_files)
                
                # CSV dosyalarÄ±nÄ± ara
                csv_files = list(search_path.rglob("*.csv"))
                discovered_files.extend(csv_files)
                
                # TXT dosyalarÄ±nÄ± ara (veri seti olabilir)
                txt_files = list(search_path.rglob("*.txt"))
                discovered_files.extend(txt_files)
        
        # DuplikasyonlarÄ± kaldÄ±r
        discovered_files = list(set(discovered_files))
        
        print(f"ğŸ“ {len(discovered_files)} dosya keÅŸfedildi")
        return discovered_files
    
    def load_dataset(self, filepath: Path) -> Optional[pd.DataFrame]:
        """Veri setini yÃ¼kle"""
        try:
            if filepath.suffix.lower() == '.json':
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Liste ise DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                if isinstance(data, list):
                    return pd.DataFrame(data)
                # Dict ise ve 'data' anahtarÄ± varsa
                elif isinstance(data, dict) and 'data' in data:
                    return pd.DataFrame(data['data'])
                # DiÄŸer dict yapÄ±larÄ±
                else:
                    return pd.DataFrame([data])
                    
            elif filepath.suffix.lower() == '.csv':
                return pd.read_csv(filepath, encoding='utf-8')
                
            elif filepath.suffix.lower() == '.txt':
                # TXT dosyasÄ±nÄ± satÄ±r satÄ±r oku
                with open(filepath, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # JSON satÄ±rlarÄ± olabilir
                data = []
                for line in lines:
                    line = line.strip()
                    if line and line.startswith('{') and line.endswith('}'):
                        try:
                            data.append(json.loads(line))
                        except:
                            continue
                
                if data:
                    return pd.DataFrame(data)
                else:
                    # DÃ¼z metin olarak iÅŸle
                    return pd.DataFrame({'text': lines})
            
            return None
            
        except Exception as e:
            print(f"âŒ {filepath} yÃ¼kleme hatasÄ±: {e}")
            return None
    
    def analyze_dataset_structure(self, df: pd.DataFrame, filepath: Path) -> Dict[str, Any]:
        """Veri seti yapÄ±sÄ±nÄ± analiz et"""
        analysis = {
            'filepath': str(filepath),
            'filename': filepath.name,
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'columns': list(df.columns),
            'data_types': df.dtypes.to_dict(),
            'missing_values': df.isnull().sum().to_dict(),
            'sample_data': df.head(3).to_dict('records')
        }
        
        # SÃ¼tun analizi
        column_analysis = {}
        for col in df.columns:
            col_data = df[col].dropna()
            column_analysis[col] = {
                'type': str(df[col].dtype),
                'unique_count': col_data.nunique(),
                'null_count': df[col].isnull().sum(),
                'null_percentage': (df[col].isnull().sum() / len(df)) * 100,
                'sample_values': col_data.head(5).tolist() if len(col_data) > 0 else []
            }
        
        analysis['column_analysis'] = column_analysis
        return analysis
    
    def evaluate_api_compatibility(self, df: pd.DataFrame, filepath: Path) -> Dict[str, Any]:
        """API uyumluluÄŸunu deÄŸerlendir"""
        print(f"ğŸ”§ {filepath.name} iÃ§in API uyumluluÄŸu deÄŸerlendiriliyor...")
        
        compatibility_results = {
            'filepath': str(filepath),
            'total_rows': len(df),
            'compatible_rows': 0,
            'compatibility_rate': 0.0,
            'field_mapping_success': 0,
            'validation_success': 0,
            'errors': [],
            'suggestions': []
        }
        
        compatible_data = []
        field_mapping_errors = 0
        validation_errors = 0
        
        for idx, row in df.iterrows():
            try:
                # Alan eÅŸleÅŸtirme
                mapped_data = self.field_mapper.map_fields_to_api(row.to_dict())
                compatibility_results['field_mapping_success'] += 1
                
                # Pydantic doÄŸrulama
                validated_data = self.validator.ensure_api_compatibility(mapped_data)
                
                if validated_data:
                    compatibility_results['validation_success'] += 1
                    compatible_data.append(validated_data)
                    
            except Exception as e:
                error_msg = f"SatÄ±r {idx}: {str(e)}"
                compatibility_results['errors'].append(error_msg)
                
                if "field mapping" in str(e).lower():
                    field_mapping_errors += 1
                else:
                    validation_errors += 1
        
        compatibility_results['compatible_rows'] = len(compatible_data)
        compatibility_results['compatibility_rate'] = (len(compatible_data) / len(df)) * 100 if len(df) > 0 else 0
        
        # Ä°yileÅŸtirme Ã¶nerileri
        suggestions = self.generate_improvement_suggestions(df, compatibility_results)
        compatibility_results['suggestions'] = suggestions
        
        return compatibility_results
    
    def generate_improvement_suggestions(self, df: pd.DataFrame, compatibility_results: Dict) -> List[str]:
        """Ä°yileÅŸtirme Ã¶nerileri oluÅŸtur"""
        suggestions = []
        
        # DÃ¼ÅŸÃ¼k uyumluluk oranÄ±
        if compatibility_results['compatibility_rate'] < 50:
            suggestions.append("âš ï¸  DÃ¼ÅŸÃ¼k API uyumluluk oranÄ±. Alan eÅŸleÅŸtirme kurallarÄ± gÃ¶zden geÃ§irilmeli.")
        
        # Eksik alanlar
        required_fields = ['question', 'answer', 'category']
        missing_fields = [field for field in required_fields if field not in df.columns]
        if missing_fields:
            suggestions.append(f"âŒ Eksik zorunlu alanlar: {', '.join(missing_fields)}")
        
        # Veri kalitesi
        for col in df.columns:
            null_percentage = (df[col].isnull().sum() / len(df)) * 100
            if null_percentage > 20:
                suggestions.append(f"âš ï¸  {col} sÃ¼tununda %{null_percentage:.1f} eksik veri var.")
        
        # TÃ¼rkÃ§e iÃ§erik kontrolÃ¼
        turkish_chars = ['Ã§', 'ÄŸ', 'Ä±', 'Ã¶', 'ÅŸ', 'Ã¼', 'Ã‡', 'Ä', 'I', 'Ã–', 'Å', 'Ãœ']
        has_turkish = False
        for col in df.columns:
            if df[col].dtype == 'object':
                sample_text = ' '.join(df[col].dropna().astype(str).head(10))
                if any(char in sample_text for char in turkish_chars):
                    has_turkish = True
                    break
        
        if not has_turkish:
            suggestions.append("ğŸŒ TÃ¼rkÃ§e iÃ§erik tespit edilemedi. TÃ¼rkÃ§e veri eklenmesi Ã¶nerilir.")
        
        return suggestions
    
    def create_improved_dataset(self, df: pd.DataFrame, filepath: Path) -> pd.DataFrame:
        """Ä°yileÅŸtirilmiÅŸ veri seti oluÅŸtur"""
        print(f"ğŸ”§ {filepath.name} iÃ§in iyileÅŸtirilmiÅŸ veri seti oluÅŸturuluyor...")
        
        improved_data = []
        
        for idx, row in df.iterrows():
            try:
                # Alan eÅŸleÅŸtirme
                mapped_data = self.field_mapper.map_fields_to_api(row.to_dict())
                
                # Pydantic doÄŸrulama
                validated_data = self.validator.ensure_api_compatibility(mapped_data)
                
                if validated_data:
                    improved_data.append(validated_data)
                    
            except Exception as e:
                # Hata durumunda varsayÄ±lan deÄŸerlerle doldur
                default_data = {
                    'question': f"Ä°yileÅŸtirilmiÅŸ soru {idx+1}",
                    'answer': f"Ä°yileÅŸtirilmiÅŸ cevap {idx+1}",
                    'category': 'general',
                    'confidence': 0.7,
                    'source': f"improved_{filepath.stem}",
                    'metadata': {
                        'original_file': filepath.name,
                        'improvement_applied': True,
                        'error_fixed': str(e)
                    }
                }
                improved_data.append(default_data)
        
        return pd.DataFrame(improved_data)
    
    def save_analysis_report(self, analysis_results: Dict):
        """Analiz raporunu kaydet"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = self.output_dir / f"existing_data_analysis_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# Plan B: Mevcut Veri Seti Analizi Raporu\n\n")
            f.write(f"**Tarih:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("## Genel Ã–zet\n\n")
            f.write(f"- **Analiz Edilen Dosya SayÄ±sÄ±:** {len(analysis_results)}\n")
            
            total_rows = sum(result['total_rows'] for result in analysis_results.values())
            total_compatible = sum(result['compatible_rows'] for result in analysis_results.values())
            avg_compatibility = (total_compatible / total_rows * 100) if total_rows > 0 else 0
            
            f.write(f"- **Toplam Veri SatÄ±rÄ±:** {total_rows}\n")
            f.write(f"- **Toplam Uyumlu SatÄ±r:** {total_compatible}\n")
            f.write(f"- **Ortalama Uyumluluk OranÄ±:** {avg_compatibility:.1f}%\n\n")
            
            f.write("## DetaylÄ± Analiz\n\n")
            
            for filename, result in analysis_results.items():
                f.write(f"### {filename}\n\n")
                f.write(f"- **Dosya:** {result['filepath']}\n")
                f.write(f"- **Toplam SatÄ±r:** {result['total_rows']}\n")
                f.write(f"- **Uyumlu SatÄ±r:** {result['compatible_rows']}\n")
                f.write(f"- **Uyumluluk OranÄ±:** {result['compatibility_rate']:.1f}%\n")
                f.write(f"- **Alan EÅŸleÅŸtirme BaÅŸarÄ±sÄ±:** {result['field_mapping_success']}\n")
                f.write(f"- **DoÄŸrulama BaÅŸarÄ±sÄ±:** {result['validation_success']}\n\n")
                
                if result['suggestions']:
                    f.write("**Ä°yileÅŸtirme Ã–nerileri:**\n")
                    for suggestion in result['suggestions']:
                        f.write(f"- {suggestion}\n")
                    f.write("\n")
                
                if result['errors']:
                    f.write("**Hatalar:**\n")
                    for error in result['errors'][:5]:  # Ä°lk 5 hatayÄ± gÃ¶ster
                        f.write(f"- {error}\n")
                    f.write("\n")
        
        print(f"ğŸ“„ Analiz raporu kaydedildi: {report_path}")
        return report_path
    
    def run_plan_b(self):
        """Plan B'yi Ã§alÄ±ÅŸtÄ±r"""
        print("ğŸš€ Plan B: Mevcut Veri Seti Ä°ÅŸleme ve Ä°yileÅŸtirme BaÅŸlatÄ±lÄ±yor...")
        print("=" * 60)
        
        # Mevcut veri setlerini keÅŸfet
        discovered_files = self.discover_existing_datasets()
        
        if not discovered_files:
            print("âŒ HiÃ§ veri seti bulunamadÄ±!")
            return {}
        
        # Her dosyayÄ± analiz et
        for filepath in discovered_files:
            print(f"\nğŸ“‹ {filepath.name} analiz ediliyor...")
            
            # Veri setini yÃ¼kle
            df = self.load_dataset(filepath)
            if df is None:
                continue
            
            print(f"ğŸ“Š YÃ¼klenen veri: {len(df)} satÄ±r, {len(df.columns)} sÃ¼tun")
            
            # YapÄ± analizi
            structure_analysis = self.analyze_dataset_structure(df, filepath)
            
            # API uyumluluk analizi
            compatibility_analysis = self.evaluate_api_compatibility(df, filepath)
            
            # SonuÃ§larÄ± birleÅŸtir
            self.analysis_results[filepath.name] = {
                **structure_analysis,
                **compatibility_analysis
            }
            
            # Ä°yileÅŸtirilmiÅŸ veri seti oluÅŸtur
            improved_df = self.create_improved_dataset(df, filepath)
            
            # Ä°yileÅŸtirilmiÅŸ veriyi kaydet
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            improved_filename = f"improved_{filepath.stem}_{timestamp}.json"
            improved_filepath = self.output_dir / improved_filename
            
            improved_data = improved_df.to_dict('records')
            with open(improved_filepath, 'w', encoding='utf-8') as f:
                json.dump(improved_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ Ä°yileÅŸtirilmiÅŸ veri kaydedildi: {improved_filepath}")
        
        # Analiz raporunu oluÅŸtur
        report_path = self.save_analysis_report(self.analysis_results)
        
        print("\nğŸ‰ Plan B tamamlandÄ±!")
        print(f"ğŸ“„ DetaylÄ± rapor: {report_path}")
        
        return self.analysis_results

if __name__ == "__main__":
    analyzer = ExistingDatasetAnalyzer()
    results = analyzer.run_plan_b()
    
    print(f"\nğŸ“Š Ã–zet SonuÃ§lar:")
    for filename, result in results.items():
        print(f"  â€¢ {filename}: {result['compatible_rows']}/{result['total_rows']} "
              f"({result['compatibility_rate']:.1f}% uyumlu)") 