# -*- coding: utf-8 -*-
"""
ğŸ¤– UniqeAi Telekom Agent - FP16 Terminal UygulamasÄ± v7.0 (Orkestra Åefi Mimarisi)
=================================================================================

Bu, projenin son ve en geliÅŸmiÅŸ sÃ¼rÃ¼mÃ¼dÃ¼r ve yÃ¼ksek hassasiyetli FP16 
modelini kullanÄ±r. "Orkestra Åefi ve Ä°cracÄ±" mimarisini uygulayarak hem 
hafÄ±za (baÄŸlam) sorunlarÄ±nÄ± hem de modelin gÃ¼venilirlik (halÃ¼sinasyon) 
problemlerini kÃ¶kten Ã§Ã¶zer.

--- MÄ°MARÄ° AÃ‡IKLAMASI ---
- **ConversationManager (Orkestra Åefi):** KullanÄ±cÄ±yla olan tÃ¼m sohbeti
  yÃ¶netir, geÃ§miÅŸi tutar ve baÄŸlamÄ± korur. Bir sonraki adÄ±mÄ±n ne olacaÄŸÄ±na
  (araÃ§ Ã§aÄŸÄ±rma mÄ±, sohbet mi) karar verir.
- **Executor (Ä°cracÄ±):** "Orkestra Åefi"nden bir gÃ¶rev aldÄ±ÄŸÄ±nda, hafÄ±zasÄ±z
  ve hatasÄ±z bir ÅŸekilde o tekil gÃ¶revi yerine getirir (araÃ§ Ã§aÄŸÄ±rÄ±r ve
  API verisine sadÄ±k kalarak Ã¶zetler).
- **SonuÃ§:** Sistem, hem uzun sohbetleri hatÄ±rlayabilen (hafÄ±za) hem de
  araÃ§ kullanÄ±rken asla hata yapmayan (gÃ¼venilirlik) bir yapÄ±ya kavuÅŸur.
"""

import os
import json
import re
import sys
from pathlib import Path
from rich.console import Console
from rich.markdown import Markdown
from typing import Optional, List, Dict, Any, Tuple
import warnings
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- Proje KÃ¶k Dizini ve ModÃ¼l Yolu ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except (NameError, IndexError):
    PROJECT_ROOT = Path.cwd()

AI_MODEL_SCRIPTS_PATH = PROJECT_ROOT / "UniqeAi" / "ai_model" / "scripts"
if str(AI_MODEL_SCRIPTS_PATH) not in sys.path:
    sys.path.insert(0, str(AI_MODEL_SCRIPTS_PATH))

try:
    from tool_definitions import get_tool_definitions, get_tool_response
except ImportError:
    print(f"\n[HATA] Gerekli 'tool_definitions' modÃ¼lÃ¼ bulunamadÄ±. Aranan yol: {AI_MODEL_SCRIPTS_PATH}")
    sys.exit(1)

# --- FP16 Model YapÄ±landÄ±rmasÄ± ---
FP16_MODEL_DIR = PROJECT_ROOT / "UniqeAi" / "ai_model" / "merged_model_fp16_v2"
CONTEXT_SIZE = 4096
TEMPERATURE = 0.1 # YaratÄ±cÄ±lÄ±k iÃ§in hafif bir artÄ±ÅŸ
DEFAULT_TEST_USER_ID = 12345

console = Console()
ALL_TOOL_DEFINITIONS = {tool['function']['name']: tool for tool in get_tool_definitions()}
warnings.filterwarnings("ignore", category=UserWarning)


def find_fp16_model(model_dir: Path) -> Optional[Path]:
    console.print(f"[yellow]ğŸ¤– FP16 Model aranÄ±yor: [cyan]{model_dir}[/cyan][/yellow]")
    if not model_dir.exists():
        console.print(f"[bold red]HATA: Model klasÃ¶rÃ¼ bulunamadÄ±![/bold red]"); return None
    
    config_file = model_dir / "config.json"
    if not config_file.exists():
        console.print(f"[bold red]HATA: config.json dosyasÄ± bulunamadÄ±: {config_file}[/bold red]"); return None
    
    console.print(f"[green]âœ… FP16 model bulundu: [bold cyan]{model_dir.name}[/bold cyan][/green]")
    return model_dir

def load_fp16_model():
    model_path = find_fp16_model(FP16_MODEL_DIR)
    if not model_path: sys.exit(1)
    
    console.print(f"[yellow]ğŸš€ FP16 modeli yÃ¼kleniyor: [cyan]{model_path.name}[/cyan][/yellow]")
    
    try:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        console.print(f"[blue]ğŸ”§ Cihaz: {device}[/blue]")
        if device == "cpu":
            console.print("[yellow]âš ï¸  GPU bulunamadÄ±, CPU kullanÄ±lacak (yavaÅŸ olabilir)[/yellow]")
        
        console.print("[yellow]ğŸ“ Tokenizer yÃ¼kleniyor...[/yellow]")
        tokenizer = AutoTokenizer.from_pretrained(str(model_path), trust_remote_code=True, padding_side="left")
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        console.print("[yellow]ğŸ§  FP16 Model yÃ¼kleniyor...[/yellow]")
        model = AutoModelForCausalLM.from_pretrained(
            str(model_path),
            torch_dtype=torch.float16,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            attn_implementation="eager" if (device == "cuda" and hasattr(torch.nn.functional, 'scaled_dot_product_attention')) else "eager"
        )
        
        if device == "cpu":
            model = model.to(device)
        
        console.print("[green]âœ… FP16 Model baÅŸarÄ±yla yÃ¼klendi.[/green]")
        console.print(f"[blue]ğŸ“Š Model Precision: {model.dtype}[/blue]")
        
        return {"model": model, "tokenizer": tokenizer, "device": device}
        
    except Exception as e:
        console.print(f"\n[bold red]HATA: FP16 Model yÃ¼klenirken kritik bir hata oluÅŸtu: {e}[/bold red]"); sys.exit(1)

# --- Ortak YanÄ±t Ãœretme Fonksiyonu ---
def generate_response(model_data: Dict[str, Any], messages: List[Dict[str, str]], max_tokens: int, temperature: float) -> str:
    try:
        tokenizer = model_data["tokenizer"]
        model = model_data["model"]
        device = model_data["device"]

        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=CONTEXT_SIZE - max_tokens).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature if temperature > 0 else None,
                do_sample=temperature > 0,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True
            )
        
        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        return response.strip()
        
    except Exception as e:
        console.print(f"[bold red]HATA: YanÄ±t Ã¼retilirken hata oluÅŸtu: {e}[/bold red]")
        return "ÃœzgÃ¼nÃ¼m, bir hata oluÅŸtu."

# --- Katman 2: Ä°cracÄ± (Executor) ---
class Executor:
    """Tekil gÃ¶revleri hafÄ±zasÄ±z ve hatasÄ±z bir ÅŸekilde yerine getiren katman."""

    def __init__(self, model_data: Dict[str, Any]):
        self.model_data = model_data
        self.system_prompt = self._create_system_prompt()

    def _create_system_prompt(self) -> str:
        tools_string = "\n".join([f"  - `{name}`: {d['function']['description']}" for name, d in ALL_TOOL_DEFINITIONS.items()])
        return f"""Sen, TÃ¼rkÃ§e konuÅŸan bir AI motorusun. GÃ¶revin, sana verilen kullanÄ±cÄ± komutunu analiz edip ilgili aracÄ± Ã§aÄŸÄ±rmaktÄ±r. 
**KRÄ°TÄ°K KURAL: ASLA SOHBET ETME, SADECE ARAÃ‡ Ã‡AÄIR!**
**Ã–RNEKLER:**
- "faturamÄ± Ã¶ÄŸrenebilir miyim" â†’ HEMEN `get_current_bill` Ã§aÄŸÄ±r
- "hesabÄ±mÄ±n faturasÄ±" â†’ HEMEN `get_current_bill` Ã§aÄŸÄ±r  
- "internet hÄ±zÄ±m" â†’ HEMEN `test_internet_speed` Ã§aÄŸÄ±r
- "faturamÄ± Ã¶de" â†’ HEMEN `pay_bill` Ã§aÄŸÄ±r
**YAPMA:**
- Hesap numarasÄ± sorma (zaten user_id=12345 var)
- "Tabii ki yardÄ±mcÄ± olabilirim" gibi sohbet
- AÃ§Ä±klama yapma
**ARAÃ‡ KULLANIM KURALI:** `<|begin_of_tool_code|>print(fonksiyon(parametre="deÄŸer"))<|end_of_tool_code|>`
**KULLANABÄ°LECEÄÄ°N ARAÃ‡LAR:**
{tools_string}"""

    def parse_tool_calls(self, text: str) -> Optional[List[Dict[str, Any]]]:
        pattern = r"<\|begin_of_tool_code\|>\s*print\((.*?)\)\s*<\|end_of_tool_code\|>"
        match = re.search(pattern, text, re.DOTALL)
        if not match: return None
        
        call_str = match.group(1)
        func_name_match = re.match(r"(\w+)\(", call_str)
        if not func_name_match: return None
        function_name = func_name_match.group(1)

        params = {}
        arg_pattern = re.compile(r"(\w+)\s*=\s*((?:\"(?:\\\"|[^\"])*\")|(?:'(?:\\'|[^'])*')|[\w.-]+)")
        for p_match in arg_pattern.finditer(call_str):
            key, raw_value = p_match.group(1), p_match.group(2)
            try: value = json.loads(raw_value)
            except (json.JSONDecodeError, TypeError): value = str(raw_value).strip("'\"")
            params[key] = value
        return [{"name": function_name, "arguments": params}]

    def execute_tool(self, tool_call: Dict[str, Any], context_data: Dict[str, Any] = None) -> str:
        func_name, func_args = tool_call["name"], tool_call["arguments"]
        
        if func_name == "pay_bill":
            return self._handle_smart_payment(func_args, context_data)
        
        if "user_id" not in func_args:
            func_args["user_id"] = DEFAULT_TEST_USER_ID
            console.print(f"ğŸ§  [italic yellow]Ä°cracÄ±: 'user_id' parametresi varsayÄ±lan ID ({DEFAULT_TEST_USER_ID}) ile tamamlandÄ±.[/italic yellow]")

        console.print(f"ğŸ› ï¸  [bold yellow]Ä°cracÄ± AraÃ§ Ã‡aÄŸrÄ±sÄ±:[/bold yellow] [green]{func_name}({func_args})[/green]")
        response = get_tool_response(func_name, func_args)
        console.print(f"âš™ï¸  [bold magenta]Ä°cracÄ± AraÃ§ YanÄ±tÄ±:[/bold magenta] {response}")
        return response
        
    def _handle_smart_payment(self, func_args: Dict[str, Any], context_data: Dict[str, Any] = None) -> str:
        console.print("ğŸ§  [bold cyan]Ä°cracÄ±: AkÄ±llÄ± Ã¶deme modu - Ã¶nce fatura kontrol ediliyor...[/bold cyan]")
        user_id = func_args.get("user_id", DEFAULT_TEST_USER_ID)
        bill_check_response = get_tool_response("get_current_bill", {"user_id": user_id})
        console.print(f"ğŸ” [bold blue]Fatura Kontrol Sonucu:[/bold blue] {bill_check_response}")
        
        try:
            bill_data = json.loads(bill_check_response)
            if not bill_data.get("success", False):
                return "Hata: Ã–denmemiÅŸ faturanÄ±z bulunmamaktadÄ±r."
            
            current_bill_id = bill_data["data"]["bill_id"]
            amount = bill_data["data"]["amount"]
            
            requested_bill_id = func_args.get("bill_id")
            if requested_bill_id and requested_bill_id != current_bill_id:
                return f"Hata: {requested_bill_id} numaralÄ± fatura bulunamadÄ±. Mevcut faturanÄ±z: {current_bill_id}"
            
            payment_args = {
                "bill_id": current_bill_id,
                "method": func_args.get("method", "credit_card"),
                "user_id": user_id
            }
            
            console.print(f"ğŸ’³ [bold green]Ä°cracÄ±: {current_bill_id} numaralÄ± fatura ({amount} TL) Ã¶deme iÅŸlemi baÅŸlatÄ±lÄ±yor...[/bold green]")
            payment_response = get_tool_response("pay_bill", payment_args)
            console.print(f"âš™ï¸  [bold magenta]Ã–deme Sonucu:[/bold magenta] {payment_response}")
            return payment_response
            
        except (json.JSONDecodeError, KeyError) as e:
            console.print(f"âš ï¸ [italic red]Ä°cracÄ±: Fatura kontrol edilirken hata oluÅŸtu: {e}[/italic red]")
            return "Hata: Fatura bilgileri alÄ±nÄ±rken bir sorun oluÅŸtu."
        
    def summarize_tool_result(self, tool_response_content: str) -> str:
        summarizer_prompt = """Sen, API yanÄ±tlarÄ±nÄ± TÃ¼rkÃ§e Ã¶zetleyen bir asistansÄ±n. KURALLAR: Sadece verilen JSON verisini kullan, kullanÄ±cÄ±ya direkt hitap et ("Sizin iÃ§in..."), ASLA Ã¼Ã§Ã¼ncÃ¼ ÅŸahÄ±s konuÅŸmasÄ± yapma ("kullanÄ±cÄ±nÄ±n"), HÄ°Ã‡BÄ°R ZAMAN araÃ§ Ã§aÄŸÄ±rma kodu kullanma, samimi ve kiÅŸisel bir ton kullan."""
        
        iron_cage_prompt = f"""Bir API'den aÅŸaÄŸÄ±daki JSON yanÄ±tÄ± alÄ±ndÄ±:
```json
{tool_response_content}
```
GÃ¶revin: Bu JSON verisinin dÄ±ÅŸÄ±na **ASLA** Ã§Ä±kma. Kendi kendine bilgi veya rakam **EKLEME**. **SADECE** bu JSON'daki bilgileri kullanarak, sonucu kullanÄ±cÄ±ya **tek bir akÄ±cÄ± TÃ¼rkÃ§e paragrafta** Ã¶zetle. **HÄ°Ã‡BÄ°R ÅEKÄ°LDE** araÃ§ Ã§aÄŸÄ±rma kodu (<|begin_of_tool_code|>) kullanma. YanÄ±tÄ±n sadece ve sadece bu paragrafÄ± iÃ§ersin."""
        
        dialogue = [{"role": "system", "content": summarizer_prompt}, {"role": "user", "content": iron_cage_prompt}]
        
        console.print("[yellow]... Ä°cracÄ± veriye sadÄ±k kalarak Ã¶zetliyor ...[/yellow]")
        raw_summary = generate_response(self.model_data, dialogue, max_tokens=512, temperature=0.05)
        
        clean_summary = re.sub(r'<\|begin_of_tool_code\|>.*?<\|end_of_tool_code\|>', '', raw_summary, flags=re.DOTALL).strip()
        clean_summary = re.sub(r'print\(.*?\)', '', clean_summary).strip()
        
        return clean_summary if clean_summary else raw_summary

    def run_task(self, user_input: str, context_data: Dict[str, Any] = None) -> Tuple[Optional[str], bool]:
        dialogue = [{"role": "system", "content": self.system_prompt}, {"role": "user", "content": user_input}]
        assistant_response_text = generate_response(self.model_data, dialogue, max_tokens=256, temperature=TEMPERATURE)
        
        tool_calls = self.parse_tool_calls(assistant_response_text)
        if tool_calls:
            tool_response = self.execute_tool(tool_calls[0], context_data)
            final_summary = self.summarize_tool_result(tool_response)
            return final_summary, True
        
        return assistant_response_text, False

# --- Katman 1: Orkestra Åefi (ConversationManager) ---
class ConversationManager:
    """Sohbeti yÃ¶neten, hafÄ±zayÄ± tutan ve Executor'Ä± tetikleyen katman."""

    def __init__(self, model_data: Dict[str, Any]):
        self.model_data = model_data
        self.executor = Executor(model_data)
        self.dialogue = [{"role": "system", "content": self._create_system_prompt()}]
        self.context_data = {}

    def _create_system_prompt(self) -> str:
        return """Sen, UniqeAi tarafÄ±ndan geliÅŸtirilmiÅŸ, nazik, yardÄ±msever ve TÃ¼rkÃ§e konuÅŸan bir Telekom MÃ¼ÅŸteri Hizmetleri AsistanÄ±sÄ±n. 
Ã–NEMLI KONUÅMA KURALLARI:
- Her zaman kullanÄ±cÄ±ya direkt hitap et ("Sizin faturanÄ±z...", "Size yardÄ±mcÄ± olabilirim...")
- ASLA Ã¼Ã§Ã¼ncÃ¼ ÅŸahÄ±s konuÅŸmasÄ± yapma ("mÃ¼ÅŸterimize", "kullanÄ±cÄ±nÄ±n" gibi)
- SÄ±cak, samimi ve kiÅŸisel bir ton kullan
- KullanÄ±cÄ±nÄ±n sorularÄ±nÄ± anla ve uygun eylemi gerÃ§ekleÅŸtir
GÃ¶revin, kullanÄ±cÄ±yla sohbet etmek, konuÅŸmanÄ±n geÃ§miÅŸini hatÄ±rlamak ve bir eylem gerÃ§ekleÅŸtirilmesi gerektiÄŸinde ilgili aracÄ± Ã§aÄŸÄ±rmaktÄ±r."""

    def handle_user_input(self, user_input: str):
        self.dialogue.append({"role": "user", "content": user_input})
        
        console.print("[yellow]... Orkestra Åefi niyeti analiz ediyor ...[/yellow]")
        tool_keywords = ['fatura', 'paket', 'tarife', 'internet', 'roaming', 'Ã¶de', 'iptal', 'sorgula', 'listele']
        bill_inquiry_patterns = ['faturasÄ±nÄ± Ã¶ÄŸren', 'faturamÄ± gÃ¶ster', 'fatura bilgi', 'bu ayki fatura', 'gÃ¼ncel fatura']
        
        has_tool_keyword = any(keyword in user_input.lower() for keyword in tool_keywords)
        is_bill_inquiry = any(pattern in user_input.lower() for pattern in bill_inquiry_patterns)
        
        if has_tool_keyword or is_bill_inquiry:
            console.print("[cyan]Orkestra Åefi: Eylem gerektiren bir komut algÄ±landÄ±. GÃ¶rev Ä°cracÄ±'ya devrediliyor...[/cyan]")
            summary, executed = self.executor.run_task(user_input, self.context_data)
            
            if executed:
                self.dialogue.append({"role": "assistant", "content": summary})
                console.print(f"ğŸ¤– [bold green]Asistan (Orkestra Åefi):[/bold green] ", end="")
                console.print(Markdown(summary))
            else:
                self.handle_chat(summary)
        else:
            console.print("[cyan]Orkestra Åefi: Normal sohbet olarak devam ediliyor...[/cyan]")
            self.handle_chat(user_input)
    
    def handle_chat(self, last_input: str):
        chat_response = generate_response(self.model_data, self.dialogue, max_tokens=512, temperature=0.5)
        self.dialogue.append({"role": "assistant", "content": chat_response})
        console.print(f"ğŸ¤– [bold green]Asistan (Orkestra Åefi):[/bold green] ", end="")
        console.print(Markdown(chat_response))

def main_loop(model_data: Dict[str, Any]):
    console.print("\n" + "="*60, style="bold green")
    console.print("ğŸ¤– [bold green]UniqeAi Telekom Agent v7.0 (FP16 - Orkestra Åefi Mimarisi)[/bold green]")
    console.print("   ğŸš€ Native FP16 modeli ile maksimum performans ve hassasiyet!")
    console.print("   ğŸ§  AkÄ±llÄ±: Ã–nce fatura kontrolÃ¼, sonra Ã¶deme!")
    console.print("   âš¡ GeliÅŸmiÅŸ: 'Orkestra Åefi' mimarisi ile gÃ¼venilir araÃ§ kullanÄ±mÄ±!")
    console.print("   Ã‡Ä±kmak iÃ§in 'quit' veya 'exit' yazabilirsiniz.")
    console.print("="*60, style="bold green")
    
    manager = ConversationManager(model_data)

    while True:
        try:
            user_input = console.input("\n[bold blue]ğŸ‘¤ Siz:[/bold blue] ")
            if user_input.lower() in ["quit", "exit"]: break
            if user_input.lower() == "new":
                console.print("[bold yellow]Yeni bir sohbet oturumu baÅŸlatÄ±lÄ±yor...[/bold yellow]")
                manager = ConversationManager(model_data)
                continue
        except (KeyboardInterrupt, EOFError): break
        
        manager.handle_user_input(user_input)

    console.print("\n[bold red]GÃ¶rÃ¼ÅŸmek Ã¼zere![/bold red]")

if __name__ == "__main__":
    model_data = load_fp16_model()
    if model_data:
        main_loop(model_data)
