# -*- coding: utf-8 -*-
"""
ğŸš€ LoRA AdaptÃ¶r BirleÅŸtirme Script'i (v2.2 - Nihai Offload DÃ¼zeltmesi)
======================================================================

Bu script, PEFT ile eÄŸitilmiÅŸ bir LoRA adaptÃ¶rÃ¼nÃ¼, orijinal base model
ile birleÅŸtirir.

YENÄ° GÃœNCELLEME: Bu versiyon, hem base modelin yÃ¼klenmesi hem de
LoRA adaptÃ¶rÃ¼nÃ¼n onun Ã¼zerine uygulanmasÄ± sÄ±rasÄ±nda `offload_folder`
belirterek, inatÃ§Ä± `ValueError`'Ä± kesin olarak Ã§Ã¶zer.
"""

import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
from pathlib import Path
import logging
import shutil

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- YapÄ±landÄ±rma ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except NameError:
    PROJECT_ROOT = Path.cwd()

# Model ve adaptÃ¶r yollarÄ±
BASE_MODEL_ID = "meta-llama/Meta-Llama-3-8B-Instruct"
ADAPTER_MODEL_PATH = PROJECT_ROOT / "UniqeAi/ai_model/results/final_model_bf16"
MERGED_MODEL_OUTPUT_PATH = PROJECT_ROOT / "UniqeAi/ai_model/merged_model_fp16_v5"
OFFLOAD_DIR = PROJECT_ROOT / "temp_offload" # GeÃ§ici dosyalar iÃ§in klasÃ¶r

def main():
    """Ana birleÅŸtirme fonksiyonu."""
    
    if OFFLOAD_DIR.exists():
        logger.warning(f"Eski geÃ§ici klasÃ¶r '{OFFLOAD_DIR}' bulunup siliniyor...")
        shutil.rmtree(OFFLOAD_DIR)
    os.makedirs(OFFLOAD_DIR, exist_ok=True)
    
    logger.info(f"ğŸš€ GÃ¼venli birleÅŸtirme iÅŸlemi baÅŸlÄ±yor (Ã‡Ä±ktÄ±: FP16)...")
    logger.info(f"ğŸ”¹ Ã‡Ä±ktÄ± KlasÃ¶rÃ¼: {MERGED_MODEL_OUTPUT_PATH}")
    logger.info(f"ğŸ”¹ GeÃ§ici Offload KlasÃ¶rÃ¼: {OFFLOAD_DIR}")

    # 1. Base modeli yÃ¼kle
    logger.info("1/4 - Orijinal base model (FP16 formatÄ±nda) yÃ¼kleniyor...")
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        torch_dtype=torch.float16,
        device_map="auto",
        offload_folder=str(OFFLOAD_DIR), 
        low_cpu_mem_usage=True
    )
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)

    # 2. PeftModel'i (adaptÃ¶r ile birlikte) yÃ¼kle
    logger.info("2/4 - LoRA adaptÃ¶rÃ¼ base model Ã¼zerine yÃ¼kleniyor...")
    
    # NÄ°HAÄ° DÃœZELTME: `offload_folder` parametresini buraya da ekliyoruz.
    merged_model = PeftModel.from_pretrained(
        base_model, 
        str(ADAPTER_MODEL_PATH),
        offload_folder=str(OFFLOAD_DIR) # HatanÄ±n asÄ±l kaynaÄŸÄ±nÄ± Ã§Ã¶zen satÄ±r.
    )

    # 3. Modelleri birleÅŸtir
    logger.info("3/4 - AÄŸÄ±rlÄ±klar birleÅŸtiriliyor (merge)...")
    merged_model = merged_model.merge_and_unload()
    logger.info("âœ… BirleÅŸtirme tamamlandÄ±.")

    # 4. BirleÅŸtirilmiÅŸ tam modeli ve tokenizer'Ä± kaydet
    logger.info(f"4/4 - BirleÅŸtirilmiÅŸ model '{MERGED_MODEL_OUTPUT_PATH}' klasÃ¶rÃ¼ne kaydediliyor...")
    merged_model.save_pretrained(str(MERGED_MODEL_OUTPUT_PATH))
    tokenizer.save_pretrained(str(MERGED_MODEL_OUTPUT_PATH))
    logger.info("âœ… Model ve tokenizer baÅŸarÄ±yla kaydedildi.")

    # Son olarak geÃ§ici klasÃ¶rÃ¼ tekrar temizle
    logger.info(f"GeÃ§ici offload klasÃ¶rÃ¼ '{OFFLOAD_DIR}' temizleniyor...")
    shutil.rmtree(OFFLOAD_DIR)
    logger.info("âœ… Temizlik tamamlandÄ±.")


if __name__ == "__main__":
    main()
