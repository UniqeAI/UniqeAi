# -*- coding: utf-8 -*-
"""
ğŸ¤– ChoyrensAI Telekom Agent - Terminal UygulamasÄ± v1.0
======================================================

Bu uygulama, Hugging Face Hub'a yÃ¼klenmiÅŸ olan "ChoyrensAI-Telekom-Agent"
modeli ile interaktif bir sohbet oturumu baÅŸlatÄ±r.

KullanÄ±cÄ±lar, Python veya herhangi bir kÃ¼tÃ¼phane bilgisine ihtiyaÃ§ duymadan,
doÄŸrudan bu uygulama Ã¼zerinden modelin araÃ§ kullanma ve akÄ±l yÃ¼rÃ¼tme
yeteneklerini test edebilirler.

NasÄ±l Ã‡alÄ±ÅŸÄ±r?
1. BaÅŸlangÄ±Ã§ta modeli Hugging Face Hub'dan indirir (veya Ã¶nbellekten yÃ¼kler).
2. Renkli ve temiz bir sohbet arayÃ¼zÃ¼ sunar.
3. Arka planda tÃ¼m araÃ§ Ã§aÄŸÄ±rma ve yanÄ±t iÅŸleme mantÄ±ÄŸÄ±nÄ± yÃ¶netir.
"""

import os
import torch
import json
import re
import sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextStreamer
from rich.console import Console
from rich.markdown import Markdown
from dotenv import load_dotenv
# rich.markup.escape artÄ±k kullanÄ±lmayacaÄŸÄ± iÃ§in kaldÄ±rÄ±labilir veya kalabilir.
# Temizlik aÃ§Ä±sÄ±ndan kaldÄ±rmak daha iyidir ama bÄ±rakmak da sorun yaratmaz.

# --- Proje KÃ¶k Dizini ve ModÃ¼l Yolu ---
try:
    # Bu script 'scripts' klasÃ¶rÃ¼nde olduÄŸu iÃ§in 3 seviye yukarÄ± Ã§Ä±kÄ±yoruz.
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except NameError:
    # Alternatif olarak, mevcut Ã§alÄ±ÅŸma dizinini kÃ¶k olarak kabul et.
    PROJECT_ROOT = Path.cwd()

# tool_definitions.py'yi import edebilmek iÃ§in yolu sisteme ekliyoruz.
sys.path.append(str(PROJECT_ROOT))
from UniqeAi.ai_model.scripts.tool_definitions_v5 import get_tool_definitions, get_tool_response

console = Console()

# --- YENÄ°: Model YapÄ±landÄ±rmasÄ± ---
# Yerel modeli mi (True) yoksa Hugging Face Hub'daki modeli mi (False) kullanacaÄŸÄ±nÄ±zÄ± seÃ§in.
# ÅÄ°MDÄ°LÄ°K YEREL TEST Ä°Ã‡Ä°N AYARLANDI.
USE_LOCAL_MODEL = True

if USE_LOCAL_MODEL:
    # merge_lora.py'nin oluÅŸturduÄŸu yerel birleÅŸtirilmiÅŸ modelin yolu
    MODEL_PATH_OR_REPO_ID = PROJECT_ROOT / "UniqeAi/ai_model/merged_model_v5"
else:
    # ArkadaÅŸlarÄ±nÄ±zla paylaÅŸmak iÃ§in Hugging Face Hub'daki repo adÄ±
    MODEL_PATH_OR_REPO_ID = "Choyrens/ChoyrensAI-Telekom-Agent-v1-merged"

# --- YENÄ°: KonuÅŸma GeÃ§miÅŸi Limiti ---
# Modelin "odaklanacaÄŸÄ±" en son kaÃ§ mesajÄ±n (kullanÄ±cÄ± + asistan) seÃ§ileceÄŸini belirler.
# Bu, "context" sorununu (kafa karÄ±ÅŸÄ±klÄ±ÄŸÄ±nÄ±) Ã§Ã¶zerken uzun hafÄ±zayÄ± korur.
FOCUSED_HISTORY_TURNS = 4

def load_huggingface_token():
    """Token'Ä± .env dosyasÄ±ndan okur ve doÄŸrular."""
    # --- YENÄ°: PaketlenmiÅŸ uygulama iÃ§in .env yolu dÃ¼zeltmesi ---
    if getattr(sys, 'frozen', False):
        # EÄŸer uygulama paketlenmiÅŸse (.exe ise), .env dosyasÄ±nÄ± .exe'nin yanÄ±ndan arar.
        base_path = Path.cwd()
    else:
        # Normal script olarak Ã§alÄ±ÅŸÄ±yorsa, proje kÃ¶k dizininden arar.
        base_path = PROJECT_ROOT

    dotenv_path = base_path / ".env"
    
    if dotenv_path.exists():
        load_dotenv(dotenv_path=dotenv_path)
    
    token = os.getenv('HUGGINGFACE_HUB_TOKEN') or os.getenv('HF_TOKEN')
    if not token:
        console.print("[bold red]HATA: Hugging Face token'Ä± bulunamadÄ±.[/bold red]")
        console.print("UygulamanÄ±n modeli indirebilmesi iÃ§in .exe'nin yanÄ±nda bir `.env` dosyasÄ± ve iÃ§inde `HUGGINGFACE_HUB_TOKEN` bulunmalÄ±dÄ±r.")
        return None
    return token

def load_model_and_tokenizer(model_path_or_repo_id, token: str):
    """Modeli ve tokenizer'Ä± yerel yoldan veya Hugging Face Hub'dan yÃ¼kler."""
    console.print(f"[yellow]ğŸš€ Model yÃ¼kleniyor: [cyan]{model_path_or_repo_id}[/cyan][/yellow]")
    
    # Sadece Hub'dan indiriliyorsa ek bilgilendirme mesajÄ± gÃ¶ster
    if not isinstance(model_path_or_repo_id, Path):
        console.print("[italic]Bu iÅŸlem ilk Ã§alÄ±ÅŸtÄ±rmada internet hÄ±zÄ±nÄ±za baÄŸlÄ± olarak birkaÃ§ dakika sÃ¼rebilir.[/italic]")

    # Daha verimli yÃ¼kleme iÃ§in 4-bit kuantizasyon ayarlarÄ±
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path_or_repo_id,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            token=token,
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path_or_repo_id, token=token)
    except Exception as e:
        # --- KESÄ°N Ã‡Ã–ZÃœM: Hata mesajÄ±nÄ± rich yerine standart print ile yazdÄ±r ---
        # Bu, rich kÃ¼tÃ¼phanesinden kaynaklanan tÃ¼m Ã§Ã¶kmeleri engeller ve asÄ±l hatayÄ± gÃ¶sterir.
        print("\n\033[91mHATA: Model veya Tokenizer yÃ¼klenirken kritik bir hata oluÅŸtu.\033[0m")
        print("\033[93m================== ASIL HATA MESAJI ==================\033[0m")
        import traceback
        traceback.print_exc() # En detaylÄ± hata dÃ¶kÃ¼mÃ¼nÃ¼ verir
        print("\033[93m=====================================================\033[0m\n")
        print("\033[93mOlasÄ± Ã‡Ã¶zÃ¼mler:\033[0m")
        print("1. Ä°nternet baÄŸlantÄ±nÄ±zÄ± kontrol edin.")
        if isinstance(model_path_or_repo_id, Path):
             print(f"2. '{model_path_or_repo_id}' yolunun doÄŸru olduÄŸundan emin olun.")
        else:
             print(f"2. Hugging Face repo adÄ±nÄ±n ('{model_path_or_repo_id}') doÄŸru olduÄŸundan ve eriÅŸim izniniz olduÄŸundan emin olun.")
        sys.exit(1)

    console.print("[green]âœ… Model ve Tokenizer baÅŸarÄ±yla yÃ¼klendi.[/green]")
    return model, tokenizer

def parse_tool_calls(text: str):
    """Modelin Ã§Ä±ktÄ±sÄ±ndaki tool-call formatÄ±nÄ± yakalar."""
    pattern = r"<\|begin_of_tool_code\|>([\s\S]*?)<\|end_of_tool_code\|>"
    match = re.search(pattern, text)
    if not match:
        return None
    
    tool_code_str = match.group(1).strip()
    call_pattern = re.compile(r"print\((\w+)\((.*)\)\)")
    call_match = call_pattern.search(tool_code_str)
    
    if not call_match:
        return None
        
    function_name = call_match.group(1)
    args_str = call_match.group(2)
    
    try:
        params = {}
        # Parametreleri ayrÄ±ÅŸtÄ±rmak iÃ§in daha gÃ¼venilir bir regex
        arg_pattern = re.compile(r"(\w+)=((?:\"(?:\\\"|[^\"])*\")|(?:'(?:\\'|[^'])*')|[^,)]+)")
        for p_match in arg_pattern.finditer(args_str):
            key = p_match.group(1)
            raw_value = p_match.group(2)
            try:
                # json.loads kullanarak string, sayÄ±, boolean gibi deÄŸerleri doÄŸru ÅŸekilde iÅŸleyelim
                params[key] = json.loads(raw_value.lower())
            except json.JSONDecodeError:
                # EÄŸer json.loads baÅŸarÄ±sÄ±z olursa (Ã¶rneÄŸin tÄ±rnaksÄ±z bir metinse), ham string olarak al
                params[key] = raw_value.strip("\"'")
        
        return [{
            "id": f"tool_call_{os.urandom(4).hex()}",
            "type": "function",
            "function": { "name": function_name, "arguments": json.dumps(params, ensure_ascii=False) }
        }]
    except Exception as e:
        console.print(f"[red]HATA: AraÃ§ parametreleri ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {args_str}. Detay: {e}[/red]")
        return None

def main():
    """Ana sohbet dÃ¶ngÃ¼sÃ¼nÃ¼ Ã§alÄ±ÅŸtÄ±rÄ±r."""
    token = None
    # Sadece Hub'dan model indirirken token gerekli.
    if not USE_LOCAL_MODEL:
        token = load_huggingface_token()
        if not token:
            sys.exit(1)

    model, tokenizer = load_model_and_tokenizer(MODEL_PATH_OR_REPO_ID, token)
    
    # KonuÅŸma geÃ§miÅŸini tutacak olan liste
    dialogue = []
    
    # Llama-3'Ã¼n konuÅŸmayÄ± bitirdiÄŸini anladÄ±ÄŸÄ± Ã¶zel tokenlar
    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]

    # --- Uygulama BaÅŸlangÄ±Ã§ MesajÄ± ---
    console.print("\n" + "="*60, style="bold green")
    console.print("ğŸ¤– [bold green]ChoyrensAI Telekom Agent'a HoÅŸ Geldiniz[/bold green]")
    console.print("   Fatura, tarife ve teknik destek konularÄ±nda size yardÄ±mcÄ± olabilirim.")
    console.print("   `!simple <sorunuz>` yazarak modelin temel konuÅŸma yeteneÄŸini test edebilirsiniz.")
    console.print("   Ã‡Ä±kmak iÃ§in 'quit' veya 'exit' yazabilirsiniz.")
    console.print("="*60, style="bold green")

    while True:
        try:
            user_input = console.input("\n[bold blue]ğŸ‘¤ Siz:[/bold blue] ")
            if user_input.lower() in ["quit", "exit"]:
                break
        except (KeyboardInterrupt, EOFError):
            break

        # --- YENÄ°: Diagnostik MantÄ±ÄŸÄ± (advanced_playground'dan taÅŸÄ±ndÄ±) ---
        if user_input.startswith("!simple "):
            simple_question = user_input[len("!simple "):]
            simple_dialogue = [{"role": "user", "content": simple_question}]
            
            console.print("[yellow]... model basit modda dÃ¼ÅŸÃ¼nÃ¼yor ...[/yellow]")
            
            token_ids = tokenizer.apply_chat_template(simple_dialogue, add_generation_prompt=True, tokenize=True, return_tensors="pt").to(model.device)
            attention_mask = torch.ones_like(token_ids)

            # Basit mod iÃ§in yanÄ±tta akÄ±tma (streaming) kullanmÄ±yoruz, tek seferde alÄ±yoruz.
            console.print(f"ğŸ¤– [bold green]Asistan (Basit YanÄ±t):[/bold green] ", end="")
            outputs = model.generate(
                input_ids=token_ids,
                attention_mask=attention_mask,
                max_new_tokens=256,
                eos_token_id=terminators,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
            )
            response_text = tokenizer.decode(outputs[0][token_ids.shape[-1]:], skip_special_tokens=True)
            console.print(Markdown(response_text))
            
            # Bu konuÅŸma ana diyalog geÃ§miÅŸine eklenmez, dÃ¶ngÃ¼ye yeniden baÅŸla.
            continue
            
        # --- Normal KonuÅŸma DÃ¶ngÃ¼sÃ¼ ---

        # KonuÅŸma geÃ§miÅŸini budama ve yÃ¶netme
        if len(dialogue) > FOCUSED_HISTORY_TURNS:
            # En son N mesajÄ± alarak odaklanmÄ±ÅŸ bir baÄŸlam oluÅŸtur
            focused_dialogue = dialogue[-FOCUSED_HISTORY_TURNS:]
        else:
            # EÄŸer konuÅŸma yeterince kÄ±saysa, tamamÄ±nÄ± kullan
            focused_dialogue = dialogue

        # KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± diyalog geÃ§miÅŸine ekle
        dialogue.append({"role": "user", "content": user_input})
        
        console.print("[yellow]... model dÃ¼ÅŸÃ¼nÃ¼yor ...[/yellow]")

        # Modelin kafasÄ±nÄ± karÄ±ÅŸtÄ±rmamak iÃ§in ona tÃ¼m geÃ§miÅŸi deÄŸil, sadece
        # en ilgili kÄ±sÄ±mlarÄ± gÃ¶nderiyoruz.
        if len(dialogue) > FOCUSED_HISTORY_TURNS:
            focused_dialogue = dialogue[-FOCUSED_HISTORY_TURNS:]
            console.print(f"[italic grey50](Sadece son {FOCUSED_HISTORY_TURNS} mesaja odaklanÄ±lÄ±yor...)[/italic grey50]")
        else:
            # EÄŸer konuÅŸma yeterince kÄ±saysa, tamamÄ±nÄ± kullan
            focused_dialogue = dialogue

        # KonuÅŸma geÃ§miÅŸini modele uygun formata Ã§evir
        token_ids = tokenizer.apply_chat_template(focused_dialogue, add_generation_prompt=True, tokenize=True, return_tensors="pt").to(model.device)

        # Modelden bir yanÄ±t Ã¼retmesini iste
        run_generation_loop(model, tokenizer, dialogue, token_ids, terminators, do_sample=True)

def run_generation_loop(model, tokenizer, dialogue, token_ids, terminators, do_sample=True):
    """
    Modelden yanÄ±t Ã¼retme, araÃ§ Ã§aÄŸÄ±rma ve Ã¶zetleme dÃ¶ngÃ¼sÃ¼nÃ¼ yÃ¶netir.
    Bu, kod tekrarÄ±nÄ± Ã¶nler ve mantÄ±ÄŸÄ± merkezileÅŸtirir.
    """
    # --- YENÄ°: TextStreamer Entegrasyonu ---
    # YanÄ±tlarÄ± kelime kelime ekrana yazdÄ±rmak iÃ§in bir streamer oluÅŸturuyoruz.
    streamer = TextStreamer(tokenizer, skip_prompt=True)
    
    # Modelden bir yanÄ±t Ã¼retmesini iste
    # `do_sample=True` olduÄŸunda kullanÄ±lacak yaratÄ±cÄ±lÄ±k ayarlarÄ±.
    generation_params = {
        "max_new_tokens": 1024,
        "eos_token_id": terminators,
        "pad_token_id": tokenizer.eos_token_id,
        "streamer": streamer  # Streamer'Ä± generate fonksiyonuna ekliyoruz
    }
    if do_sample:
        generation_params["temperature"] = 0.6
        generation_params["top_p"] = 0.9

    # --- YENÄ°: Attention Mask'Ä± manuel olarak ekliyoruz ---
    # Bu, kÃ¼tÃ¼phanenin uyarÄ±sÄ±nÄ± giderir ve daha gÃ¼venilir sonuÃ§lar saÄŸlar.
    attention_mask = torch.ones_like(token_ids)

    # --- YENÄ°: AkÄ±ÅŸ iÃ§in Thread KullanÄ±mÄ± ---
    # Streamer'Ä±n dÃ¼zgÃ¼n Ã§alÄ±ÅŸmasÄ± iÃ§in `generate` fonksiyonunu ayrÄ± bir thread'de
    # Ã§alÄ±ÅŸtÄ±rmak en iyi pratiktir. Bu, ana programÄ±n takÄ±lmasÄ±nÄ± Ã¶nler.
    # Ancak basitlik adÄ±na ÅŸimdilik doÄŸrudan Ã§aÄŸÄ±rÄ±yoruz. `generate` zaten
    # akÄ±ÅŸÄ± destekleyecek ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.

    console.print(f"ğŸ¤– [bold green]Asistan:[/bold green] ", end="")
    outputs = model.generate(
        input_ids=token_ids,
        attention_mask=attention_mask,
        do_sample=do_sample,
        **generation_params
    )
    
    # Streamer Ã§Ä±ktÄ±yÄ± zaten ekrana yazdÄ±rdÄ±ÄŸÄ± iÃ§in burada tekrar yazdÄ±rmÄ±yoruz.
    # Sadece tam metni alÄ±p tool-call var mÄ± diye kontrol edeceÄŸiz.
    response_text = tokenizer.decode(outputs[0][token_ids.shape[-1]:], skip_special_tokens=False)

    # Modelin bir araÃ§ Ã§aÄŸÄ±rÄ±p Ã§aÄŸÄ±rmadÄ±ÄŸÄ±nÄ± kontrol et
    tool_calls = parse_tool_calls(response_text)

    if tool_calls:
        # AraÃ§ Ã§aÄŸrÄ±ldÄ±ysa...
        console.print(f"ğŸ› ï¸  [bold yellow]AraÃ§ Ã‡aÄŸrÄ±sÄ± AlgÄ±landÄ±:[/bold yellow] [green]{response_text.strip()}[/green]")
        dialogue.append({"role": "assistant", "content": response_text})

        # TÃ¼m araÃ§larÄ± Ã§alÄ±ÅŸtÄ±r ve sonuÃ§larÄ± topla
        for call in tool_calls:
            func_name = call["function"]["name"]
            func_args = json.loads(call["function"]["arguments"])

            # Sahte API'den (tool_definitions.py) yanÄ±tÄ± al
            tool_response_content = get_tool_response(func_name, func_args)

            console.print(f"âš™ï¸  [bold magenta]AraÃ§ YanÄ±tÄ± ({func_name}):[/bold magenta] {tool_response_content}")

            # AracÄ±n yanÄ±tÄ±nÄ± diyalog geÃ§miÅŸine ekle
            dialogue.append({
                "role": "tool",
                "content": tool_response_content,
            })

        console.print("[yellow]... model araÃ§ sonuÃ§larÄ±nÄ± deÄŸerlendirip Ã¶zetliyor ...[/yellow]")

        # --- YENÄ°: Ã–zetleme iÃ§in de OdaklanmÄ±ÅŸ BaÄŸlam KullanÄ±mÄ± ---
        # GÃ¼ncellenmiÅŸ diyalog geÃ§miÅŸiyle tekrar modele git ve nihai yanÄ±tÄ± al
        if len(dialogue) > FOCUSED_HISTORY_TURNS:
            focused_dialogue_for_summary = dialogue[-FOCUSED_HISTORY_TURNS:]
        else:
            focused_dialogue_for_summary = dialogue
            
        final_token_ids = tokenizer.apply_chat_template(focused_dialogue_for_summary, add_generation_prompt=True, tokenize=True, return_tensors="pt").to(model.device)
        
        # Ã–zetleme adÄ±mÄ±nÄ± tekrar Ã§aÄŸÄ±r, bu sefer `do_sample=False` ile
        # Bu, iÃ§ iÃ§e geÃ§miÅŸ bir dÃ¶ngÃ¼ gibi davranÄ±r ve Ã¶zetlemenin de araÃ§ Ã§aÄŸÄ±rabilmesine olanak tanÄ±r (ileride).
        # Dikkat: `dialogue` listesinin tamamÄ±nÄ± paslamaya devam ediyoruz ki tam geÃ§miÅŸ korunabilsin.
        run_generation_loop(model, tokenizer, dialogue, final_token_ids, terminators, do_sample=False)
        
    else:
        # Streamer zaten yazdÄ±ÄŸÄ± iÃ§in, burada sadece konuÅŸma geÃ§miÅŸini gÃ¼ncelliyoruz.
        # TemizlenmiÅŸ yanÄ±tÄ± geÃ§miÅŸe eklemek daha iyi olabilir.
        cleaned_response = re.sub(r'<\|.*?\|>', '', response_text).strip()
        dialogue.append({"role": "assistant", "content": cleaned_response})

    console.print("\n[bold red]GÃ¶rÃ¼ÅŸmek Ã¼zere![/bold red]")

if __name__ == "__main__":
    main()
