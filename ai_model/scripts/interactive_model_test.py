#!/usr/bin/env python3
"""
ğŸ¤– Interaktif Model Test - QLoRA Fine-tuned Llama 3
==================================================

Bu script fine-tuned modelinizle direkt sohbet etmenizi saÄŸlar.
Telekom senaryolarÄ±nÄ± test edebilir ve model yanÄ±tlarÄ±nÄ± gÃ¶rebilirsiniz.

KullanÄ±m:
    python interactive_model_test.py
    
Ã‡Ä±kmak iÃ§in: 'exit' veya 'quit' yazÄ±n
"""

import torch
import gc
import warnings
import os
import logging
from dotenv import load_dotenv
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

warnings.filterwarnings("ignore")

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')
logger = logging.getLogger(__name__)

# Load environment variables from .env file at the project root
load_dotenv()

def setup_huggingface_token():
    """Setup Hugging Face token from environment variable."""
    token = os.getenv('HUGGINGFACE_HUB_TOKEN')
    if token:
        logger.info("âœ… Hugging Face token found in environment")
        # Set environment variable for transformers library
        os.environ['HF_TOKEN'] = token
        return True
    else:
        logger.warning("âš ï¸ HUGGINGFACE_HUB_TOKEN environment variable not found. Relying on global login.")
        # transformers library will try to use cached token from `huggingface-cli login`
        return False

class InteractiveModelTester:
    def __init__(self):
        setup_huggingface_token()
        self.model_path = "./qlora_fine_tuned_model"
        self.base_model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
        self.model = None
        self.tokenizer = None
        
    def load_model(self):
        """Load the fine-tuned model with optimizations"""
        print("ğŸ¤– Model yÃ¼kleniyor...")
        print("â³ Bu birkaÃ§ dakika sÃ¼rebilir...")
        
        try:
            # Quantization config for memory efficiency
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
            
            # Load tokenizer
            print("ğŸ“ Tokenizer yÃ¼kleniyor...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.base_model_name,
                trust_remote_code=True
            )
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Load base model
            print("ğŸ§  Base model yÃ¼kleniyor...")
            base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                quantization_config=bnb_config,
                device_map="auto",
                torch_dtype=torch.float16,
                trust_remote_code=True,
                offload_buffers=True  # Memory optimization
            )
            
            # Load fine-tuned adapters
            print("ğŸ¯ Fine-tuned adapters yÃ¼kleniyor...")
            self.model = PeftModel.from_pretrained(
                base_model,
                self.model_path,
                device_map="auto"
            )
            
            print("âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
            print(f"ğŸ’¾ GPU Memory: {torch.cuda.memory_allocated()/1024**3:.1f}GB")
            
        except Exception as e:
            print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            print("\nğŸ”§ Sorun giderme Ã¶nerileri:")
            print("1. QLoRA eÄŸitimi tamamlandÄ± mÄ±?")
            print("2. Model dosyalarÄ± ./qlora_fine_tuned_model/ klasÃ¶rÃ¼nde mi?")
            print("3. GPU memory yeterli mi? (nvidia-smi ile kontrol edin)")
            return False
            
        return True
    
    def generate_response(self, user_input):
        """Generate response from the model"""
        if not self.model or not self.tokenizer:
            return "âŒ Model yÃ¼klÃ¼ deÄŸil!"
        
        # Clear cache
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # Format prompt like training data
        prompt = f"### Instruction:\n{user_input[:20].title()}\n\n### Input:\n{user_input}\n\n### Response:\n"
        
        try:
            # Tokenize with longer context
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=1024,  # Increased for better context
                truncation=True,
                padding=False
            )
            
            # Move to GPU if available
            if torch.cuda.is_available():
                inputs = {k: v.to("cuda:0") for k, v in inputs.items()}
            
            # Generate with optimized parameters
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=80,  # Reduced for cleaner output
                    temperature=0.3,    # Slightly higher for variety
                    top_p=0.9,         # Top-p sampling
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=False,
                    repetition_penalty=1.3,  # Higher to prevent repetition
                    no_repeat_ngram_size=3   # Prevent 3-gram repetition
                )
            
            # Decode response
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the model's response part
            if "### Response:\n" in full_response:
                response = full_response.split("### Response:\n")[-1].strip()
            else:
                response = full_response.strip()
            
            # Clean up common artifacts
            response = response.replace("<|endoftext|>", "")
            response = response.split("### Instruction:")[0]  # Stop at next instruction
            response = response.split("### Input:")[0]        # Stop at next input
            response = response.strip()
            
            return response
            
        except Exception as e:
            return f"âŒ YanÄ±t oluÅŸturma hatasÄ±: {e}"
    
    def chat_loop(self):
        """Main interactive chat loop"""
        print("\n" + "="*60)
        print("ğŸ¤– TelekomAI Fine-tuned Model Test")
        print("="*60)
        print("ğŸ’¬ Telekom ile ilgili sorularÄ±nÄ±zÄ± sorabilirsiniz!")
        print("ğŸ“‹ Ã–rnek sorular:")
        print("   â€¢ Paketimi deÄŸiÅŸtirmek istiyorum")
        print("   â€¢ FaturamÄ± gÃ¶rmek istiyorum") 
        print("   â€¢ Ä°nternetim Ã§ok yavaÅŸ")
        print("   â€¢ Roaming'i aÃ§mak istiyorum")
        print("   â€¢ ArÄ±za kaydÄ± oluÅŸturmak istiyorum")
        print("\nğŸ’¡ Ã‡Ä±kmak iÃ§in: 'exit' veya 'quit' yazÄ±n")
        print("="*60)
        
        while True:
            try:
                # Get user input
                user_input = input("\nğŸ‘¤ Siz: ").strip()
                
                # Check for exit commands
                if user_input.lower() in ['exit', 'quit', 'Ã§Ä±kÄ±ÅŸ', 'bye']:
                    print("ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
                    break
                
                if not user_input:
                    print("âš ï¸ LÃ¼tfen bir soru yazÄ±n!")
                    continue
                
                # Generate response
                print("ğŸ¤– TelekomAI: ", end="", flush=True)
                response = self.generate_response(user_input)
                print(response)
                
                # Check if it contains tool calls and validate APIs
                if "<tool_code>" in response:
                    print("âœ… Model tool Ã§aÄŸrÄ±sÄ± yaptÄ±!")
                    self.validate_api_calls(response)
                elif "backend_api." in response:
                    print("ğŸŸ¡ API Ã§aÄŸrÄ±sÄ± var ama format eksik")
                    self.validate_api_calls(response)
                else:
                    print("â„¹ï¸ Normal yanÄ±t (tool Ã§aÄŸrÄ±sÄ± yok)")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z!")
                break
            except Exception as e:
                print(f"âŒ Hata: {e}")
    
    def validate_api_calls(self, response):
        """Validate if API calls are correct"""
        # Correct APIs from schema
        correct_apis = [
            "get_customer_package", "get_available_packages", "change_package",
            "get_remaining_quotas", "get_package_details", "enable_roaming",
            "get_current_bill", "get_past_bills", "pay_bill", 
            "get_payment_history", "setup_autopay",
            "check_network_status", "create_fault_ticket", 
            "get_fault_ticket_status", "test_internet_speed",
            "get_customer_profile", "update_customer_contact",
            "suspend_line", "reactivate_line", "check_number_portability"
        ]
        
        found_apis = []
        for api in correct_apis:
            if api in response:
                found_apis.append(api)
        
        if found_apis:
            print(f"ğŸ“‹ DoÄŸru API'ler: {', '.join(found_apis)}")
        else:
            print("âš ï¸ TanÄ±nmayan API kullanÄ±mÄ± tespit edildi")
    
    def cleanup(self):
        """Clean up resources"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()

def main():
    """Main function"""
    tester = InteractiveModelTester()
    
    # Load model
    if not tester.load_model():
        return
    
    try:
        # Start chat
        tester.chat_loop()
    finally:
        # Cleanup
        print("ğŸ§¹ Temizlik yapÄ±lÄ±yor...")
        tester.cleanup()

if __name__ == "__main__":
    main() 