# -*- coding: utf-8 -*-
"""
ğŸš€ Ä°nteraktif Ã‡Ä±karÄ±m (Sohbet) Script'i (Interactive Inference Script)
=====================================================================

Bu script, birleÅŸtirilmiÅŸ (merged) model ile interaktif bir sohbet oturumu
baÅŸlatmak iÃ§in kullanÄ±lÄ±r. Bu, modelin niteliksel (qualitative) olarak
deÄŸerlendirilmesi, davranÄ±ÅŸlarÄ±nÄ±n anlaÅŸÄ±lmasÄ± ve zayÄ±f noktalarÄ±nÄ±n
tespit edilmesi iÃ§in kritik bir araÃ§tÄ±r.

"eval_loss" gibi metrikler matematiksel performansÄ± gÃ¶sterirken, bu script
modelin "ruhunu" ve gerÃ§ek dÃ¼nyadaki muhakeme yeteneÄŸini anlamamÄ±zÄ± saÄŸlar.

NasÄ±l Ã‡alÄ±ÅŸÄ±r?
- BirleÅŸtirilmiÅŸ modeli ve tokenizer'Ä± yÃ¼kler.
- Terminalde bir sohbet dÃ¶ngÃ¼sÃ¼ baÅŸlatÄ±r.
- Llama-3'Ã¼n sohbet formatÄ±na uygun ÅŸekilde geÃ§miÅŸi yÃ¶netir.
- Modelin metin ve araÃ§ Ã§aÄŸÄ±rma (tool call) yanÄ±tlarÄ±nÄ± ayrÄ±ÅŸtÄ±rÄ±p
  anlaÅŸÄ±lÄ±r bir ÅŸekilde gÃ¶sterir.
"""

import os
import torch
import logging
import json
from pathlib import Path
from dataclasses import dataclass, field
from transformers import AutoModelForCausalLM, AutoTokenizer, HfArgumentParser, BitsAndBytesConfig

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Proje KÃ¶k Dizini TanÄ±mlamasÄ± ---
PROJECT_ROOT = Path(__file__).resolve().parents[3]

# --- YapÄ±landÄ±rma ---
@dataclass
class ChatConfig:
    """
    Sohbet arayÃ¼zÃ¼ iÃ§in yapÄ±landÄ±rma parametreleri.
    """
    model_path: str = field(
        default="UniqeAi/ai_model/merged_model_v3",
        metadata={"help": "KullanÄ±lacak birleÅŸtirilmiÅŸ modelin proje kÃ¶kÃ¼nden gÃ¶reli yolu."}
    )
    # Ã‡Ä±karÄ±m sÄ±rasÄ±nda daha iyi performans iÃ§in quantize edebiliriz
    load_in_4bit: bool = field(default=True, metadata={"help": "Modeli 4-bit olarak yÃ¼kle (VRAM'i azaltÄ±r)."})
    max_new_tokens: int = field(default=1024, metadata={"help": "Modelin tek seferde Ã¼reteceÄŸi maksimum token sayÄ±sÄ±."})
    temperature: float = field(default=0.6, metadata={"help": "Ãœretimdeki rastgelelik. Daha dÃ¼ÅŸÃ¼k deÄŸerler daha deterministik sonuÃ§lar verir."})
    top_p: float = field(default=0.9, metadata={"help": "Nucleus sampling. OlasÄ±lÄ±klarÄ± kÃ¼mÃ¼latif olarak %p'yi aÅŸan token'larÄ± filtreler."})


class Chatbot:
    """
    Modeli yÃ¼kleyen ve interaktif sohbet oturumunu yÃ¶neten ana sÄ±nÄ±f.
    """
    def __init__(self, config: ChatConfig):
        self.config = config
        self.model_full_path = str(PROJECT_ROOT / self.config.model_path)
        logger.info(f"Sohbet botu baÅŸlatÄ±lÄ±yor. Model yolu: {self.model_full_path}")

        quantization_config = None
        if self.config.load_in_4bit:
            logger.info("4-bit quantization etkinleÅŸtirildi.")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
            )
        
        # 1. Tokenizer ve BirleÅŸtirilmiÅŸ Modeli YÃ¼kle
        logger.info("Tokenizer yÃ¼kleniyor...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_full_path)
        
        logger.info("Model yÃ¼kleniyor... Bu iÅŸlem VRAM'inize baÄŸlÄ± olarak zaman alabilir.")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_full_path,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            quantization_config=quantization_config,
        )
        self.model.eval() # Modeli deÄŸerlendirme moduna al

        self.history = []
        logger.info("âœ… Model ve tokenizer baÅŸarÄ±yla yÃ¼klendi. Sohbet baÅŸlayabilir.")

    def _parse_assistant_response(self, response_text: str) -> dict:
        """
        Modelin ham metin Ã§Ä±ktÄ±sÄ±nÄ±, iÃ§erik (content) ve araÃ§ Ã§aÄŸrÄ±larÄ± (tool_calls)
        iÃ§eren yapÄ±sal bir sÃ¶zlÃ¼ÄŸe dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        """
        # Llama 3'Ã¼n Ã¶zel token'larÄ±nÄ± kullanarak yanÄ±tÄ± parÃ§alara ayÄ±r
        content_part_raw = response_text
        tool_calls_part_raw = None

        if "<|start_header_id|>tool_calls<|end_header_id|>" in response_text:
            parts = response_text.split("<|start_header_id|>tool_calls<|end_header_id|>", 1)
            content_part_raw = parts[0]
            tool_calls_part_raw = parts[1]

        # Ä°Ã§erik kÄ±smÄ±nÄ± temizle
        content = content_part_raw.strip()
        if content.endswith("<|eot_id|>"):
            content = content[:-len("<|eot_id|>")].strip()
        
        # AraÃ§ Ã§aÄŸÄ±rma kÄ±smÄ±nÄ± temizle ve JSON olarak ayrÄ±ÅŸtÄ±r
        tool_calls = None
        if tool_calls_part_raw:
            tool_calls_str = tool_calls_part_raw.strip()
            if tool_calls_str.endswith("<|eot_id|>"):
                tool_calls_str = tool_calls_str[:-len("<|eot_id|>")].strip()
            
            try:
                # Modelin Ã¼rettiÄŸi string bir JSON listesi olmalÄ±
                tool_calls = json.loads(tool_calls_str)
            except json.JSONDecodeError:
                logger.error("Modelin araÃ§ Ã§aÄŸÄ±rma Ã§Ä±ktÄ±sÄ± JSON formatÄ±nda deÄŸil: %s", tool_calls_str)
                content += f"\n\n[HATA: GeÃ§ersiz araÃ§ Ã§aÄŸrÄ±sÄ± formatÄ±: {tool_calls_str}]"
        
        return {
            "role": "assistant",
            "content": content if content else None,
            "tool_calls": tool_calls
        }

    def run(self):
        """
        Ana sohbet dÃ¶ngÃ¼sÃ¼nÃ¼ baÅŸlatÄ±r.
        """
        print("\n--- Uzman Model Sohbet ArayÃ¼zÃ¼ ---")
        print("Model ile sohbete baÅŸlayÄ±n.")
        print("GeÃ§miÅŸi temizlemek iÃ§in 'temizle', Ã§Ä±kmak iÃ§in 'Ã§Ä±kÄ±ÅŸ' yazÄ±n.")
        print("-" * 35)

        while True:
            try:
                user_input = input("ğŸ‘¤ Siz: ")
                if user_input.lower() in ["Ã§Ä±kÄ±ÅŸ", "cikis", "exit", "quit"]:
                    print("GÃ¶rÃ¼ÅŸmek Ã¼zere!")
                    break
                if user_input.lower() in ["temizle", "clear"]:
                    self.history = []
                    print("\nâœ¨ Sohbet geÃ§miÅŸi temizlendi.\n")
                    continue

                # 1. KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle
                self.history.append({"role": "user", "content": user_input})

                # 2. Sohbet geÃ§miÅŸini modele uygun formata Ã§evir ve token'laÅŸtÄ±r
                input_ids = self.tokenizer.apply_chat_template(
                    self.history,
                    add_generation_prompt=True,
                    return_tensors="pt"
                ).to(self.model.device)

                # 3. Modelden yanÄ±t Ã¼ret
                print("ğŸ¤– Model dÃ¼ÅŸÃ¼nÃ¼yor...", end="\r")
                with torch.no_grad():
                    outputs = self.model.generate(
                        input_ids,
                        max_new_tokens=self.config.max_new_tokens,
                        eos_token_id=self.tokenizer.eos_token_id,
                        do_sample=True,
                        temperature=self.config.temperature,
                        top_p=self.config.top_p,
                    )
                
                # Sadece Ã¼retilen yeni token'larÄ± al
                response_ids = outputs[0][input_ids.shape[-1]:]
                assistant_response_raw = self.tokenizer.decode(response_ids, skip_special_tokens=False)
                
                # 4. Model yanÄ±tÄ±nÄ± ayrÄ±ÅŸtÄ±r ve geÃ§miÅŸe ekle
                assistant_message = self._parse_assistant_response(assistant_response_raw)
                self.history.append(assistant_message)
                
                # 5. YanÄ±tÄ± kullanÄ±cÄ±ya gÃ¶ster
                print("ğŸ¤– Model: ", end="", flush=True)
                if assistant_message["content"]:
                    print(assistant_message["content"])

                if assistant_message["tool_calls"]:
                    print("\n--- ğŸ› ï¸ AraÃ§ Ã‡aÄŸrÄ±sÄ± AlgÄ±landÄ± ---")
                    for tool_call in assistant_message["tool_calls"]:
                        func_name = tool_call.get("function", {}).get("name")
                        func_args = tool_call.get("function", {}).get("arguments")
                        print(f"  ğŸ“ Fonksiyon: {func_name}")
                        # ArgÃ¼manlarÄ± daha okunaklÄ± gÃ¶stermek iÃ§in JSON olarak formatla
                        try:
                            args_dict = json.loads(func_args)
                            pretty_args = json.dumps(args_dict, indent=2, ensure_ascii=False)
                            print(f"  ğŸ“‹ Parametreler:\n{pretty_args}")
                        except:
                             print(f"  ğŸ“‹ Parametreler (raw): {func_args}")
                    print("---------------------------------")
                print()


            except KeyboardInterrupt:
                print("\nÃ‡Ä±kÄ±ÅŸ yapÄ±lÄ±yor...")
                break
            except Exception as e:
                logger.error(f"Sohbet sÄ±rasÄ±nda bir hata oluÅŸtu: {e}", exc_info=True)
                break

def main():
    parser = HfArgumentParser((ChatConfig,))
    config, = parser.parse_args_into_dataclasses()

    try:
        chatbot = Chatbot(config)
        chatbot.run()
    except Exception as e:
        logger.error(f"Sohbet botu baÅŸlatÄ±lÄ±rken bir hata oluÅŸtu: {e}", exc_info=True)

if __name__ == "__main__":
    main() 