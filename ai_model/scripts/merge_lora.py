# -*- coding: utf-8 -*-
"""
âœ… GÃ¼venilir Model BirleÅŸtirme Script'i (Reliable Model Merging Script)
======================================================================

QLoRA adaptÃ¶r aÄŸÄ±rlÄ±klarÄ±nÄ± temel model ile birleÅŸtirir,
tek parÃ§a, daÄŸÄ±tÄ±ma hazÄ±r model Ã¼retir.
"""

import torch
import os
import sys
from pathlib import Path
import tempfile
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from dotenv import load_dotenv

# Rich kullanÄ±lamazsa fallback olarak logging'e geÃ§
try:
    from rich.console import Console
    console = Console()
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO)
    class ConsoleFallback:
        def print(self, msg, **kwargs):
            logging.info(msg)
    console = ConsoleFallback()

# --- Proje kÃ¶k dizini tanÄ±mlama ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except NameError:
    PROJECT_ROOT = Path.cwd()

# --- YapÄ±landÄ±rmalar ---
BASE_MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"
ADAPTER_MODEL_PATH = PROJECT_ROOT / "UniqeAi/ai_model/results/final_model_v6"
MERGED_MODEL_SAVE_PATH = PROJECT_ROOT / "UniqeAi/ai_model/merged_model_v6"
PUSH_TO_HUB = False
HF_REPO_NAME = "Choyrens/ChoyrensAI-Telekom-Agent-v5"

def main():
    # --- Python sÃ¼rÃ¼m kontrolÃ¼ ---
    if sys.version_info < (3, 9):
        console.print("[bold red]âš ï¸ Python 3.9+ sÃ¼rÃ¼mÃ¼ Ã¶nerilir.[/bold red]")

    console.print("\n" + "="*50, style="bold green")
    console.print("ðŸš€ [bold green]GÃ¼venilir Model BirleÅŸtirme SÃ¼reci BaÅŸlatÄ±lÄ±yor...[/bold green]")
    console.print("="*50, style="bold green")

    console.print(f"ðŸ“– [cyan]Temel Model:[/cyan] {BASE_MODEL_NAME}")
    console.print(f"ðŸ”© [cyan]AdaptÃ¶r Modeli:[/cyan] {ADAPTER_MODEL_PATH}")
    console.print(f"ðŸ’¾ [cyan]Hedef KlasÃ¶r:[/cyan] {MERGED_MODEL_SAVE_PATH}")
    if PUSH_TO_HUB:
        console.print(f"â˜ï¸  [cyan]Hub Repo:[/cyan] {HF_REPO_NAME}")

    # .env dosyasÄ± yÃ¼kleniyor
    dotenv_path = PROJECT_ROOT / ".env"
    if dotenv_path.exists():
        load_dotenv(dotenv_path=dotenv_path)

    hf_token = os.getenv('HUGGINGFACE_HUB_TOKEN') or os.getenv('HF_TOKEN')
    if not hf_token:
        console.print("[bold red]HATA: Hugging Face token bulunamadÄ±![/bold red]")
        sys.exit(1)

    # --- Temel model yÃ¼kleniyor ---
    console.print("\n[yellow]1. Temel model (Llama-3) yÃ¼kleniyor...[/yellow]")
    try:
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_NAME,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            token=hf_token
        )
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, token=hf_token)

        # Tokenizer pad_token kontrolÃ¼
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            console.print("[blue]â„¹ï¸ Tokenizer pad_token ayarlandÄ±.[/blue]")

        device_info = next(base_model.parameters()).device
        console.print(f"âœ… Model yÃ¼klendi. [cyan]Cihaz:[/cyan] {device_info}")
    except Exception as e:
        console.print("[bold red]HATA: Temel model yÃ¼klenemedi.[/bold red]")
        console.print(e)
        sys.exit(1)

    # --- LoRA adaptÃ¶rÃ¼ birleÅŸtiriliyor ---
    console.print("\n[yellow]2. LoRA adaptÃ¶rÃ¼ yÃ¼kleniyor ve birleÅŸtiriliyor...[/yellow]")
    try:
        # GeÃ§ici offload klasÃ¶rÃ¼ oluÅŸturuluyor
        offload_dir = tempfile.mkdtemp(prefix="offload_")
        merged_model = PeftModel.from_pretrained(
            base_model,
            str(ADAPTER_MODEL_PATH),
            device_map="auto",
            offload_folder=offload_dir
        )
        merged_model = merged_model.merge_and_unload()
        console.print("[green]âœ… AdaptÃ¶r baÅŸarÄ±yla birleÅŸtirildi.[/green]")
    except Exception as e:
        console.print("[bold red]HATA: AdaptÃ¶r yÃ¼klenemedi veya birleÅŸtirilemedi.[/bold red]")
        console.print(e)
        sys.exit(1)

    # --- Hedef klasÃ¶r kontrolÃ¼ ---
    if MERGED_MODEL_SAVE_PATH.exists():
        console.print(f"[bold yellow]âš ï¸ UyarÄ±: '{MERGED_MODEL_SAVE_PATH}' klasÃ¶rÃ¼ mevcut, Ã¼zerine yazÄ±lacak.[/bold yellow]")

    # --- Model kaydediliyor ---
    console.print(f"\n[yellow]3. Model '{MERGED_MODEL_SAVE_PATH}' klasÃ¶rÃ¼ne kaydediliyor...[/yellow]")
    try:
        os.makedirs(MERGED_MODEL_SAVE_PATH, exist_ok=True)
        merged_model.save_pretrained(str(MERGED_MODEL_SAVE_PATH), max_shard_size="2GB")
        tokenizer.save_pretrained(str(MERGED_MODEL_SAVE_PATH))

        # config.json kontrolÃ¼
        if not (MERGED_MODEL_SAVE_PATH / "config.json").exists():
            console.print("[red]âš ï¸ UyarÄ±: config.json eksik olabilir![/red]")

        console.print("[bold green]ðŸŽ‰ Model baÅŸarÄ±yla kaydedildi.[/bold green]")
    except Exception as e:
        console.print("[bold red]HATA: Model kaydedilemedi.[/bold red]")
        console.print(e)
        sys.exit(1)

    # --- Hugging Face Hub'a yÃ¼kleme ---
    if PUSH_TO_HUB:
        console.print(f"\n[yellow]4. Hugging Face Hub'a yÃ¼kleniyor: '{HF_REPO_NAME}'...[/yellow]")
        try:
            merged_model.push_to_hub(HF_REPO_NAME, token=hf_token, safe_serialization=True)
            tokenizer.push_to_hub(HF_REPO_NAME, token=hf_token)
            console.print(f"[bold green]ðŸš€ Model Hugging Face Hub'a yÃ¼klendi: https://huggingface.co/{HF_REPO_NAME}[/bold green]")
        except Exception as e:
            console.print("[bold red]HATA: Hugging Face Hub'a yÃ¼kleme baÅŸarÄ±sÄ±z oldu.[/bold red]")
            console.print(e)
            sys.exit(1)

if __name__ == "__main__":
    main()
