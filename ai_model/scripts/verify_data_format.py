# -*- coding: utf-8 -*-
"""
ğŸ•µï¸ Veri FormatÄ± DoÄŸrulama Script'i (Data Format Verification Script)
======================================================================

Bu script'in tek amacÄ±, eÄŸitim verimizdeki bir Ã¶rneÄŸin, `expert_trainer.py`
iÃ§indeki formatlama mantÄ±ÄŸÄ± ve tokenizer tarafÄ±ndan iÅŸlendiÄŸinde, Llama-3'Ã¼n
beklediÄŸi nihai string formatÄ±na doÄŸru bir ÅŸekilde dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼p
dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmediÄŸini kontrol etmektir.

Bu, yeniden eÄŸitim yapmadan Ã¶nce, sorunun veri formatlamasÄ±nda olup
olmadÄ±ÄŸÄ±nÄ± anlamak iÃ§in kritik bir adÄ±mdÄ±r.
"""
import torch
import json
import os
import sys
from pathlib import Path
from transformers import AutoTokenizer
from rich.console import Console
from rich.syntax import Syntax
from dotenv import load_dotenv
import uuid  # expert_trainer'daki mantÄ±kla aynÄ± olmasÄ± iÃ§in

# --- Proje KÃ¶kÃ¼ ve Sistem Yolu ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except NameError:
    PROJECT_ROOT = Path.cwd()

sys.path.append(str(PROJECT_ROOT))
console = Console()

# --- YapÄ±landÄ±rma ---
BASE_MODEL_NAME = "meta-llama/Meta-Llama-3-8B-Instruct"
DATA_FILE_PATH = PROJECT_ROOT / "UniqeAi/ai_model/data/expert_tool_chaining_data_v2_validated.json"

# --- Token YÃ¼kleme ---
def setup_huggingface_token():
    dotenv_path = PROJECT_ROOT / ".env"
    if dotenv_path.exists():
        load_dotenv(dotenv_path=dotenv_path)
    
    token = os.getenv('HUGGINGFACE_HUB_TOKEN') or os.getenv('HF_TOKEN')
    if not token:
        console.print("[bold red]HATA: Hugging Face token bulunamadÄ±![/bold red]")
        return None
    return token

# --- `expert_trainer.py`'den Kopyalanan Formatlama Fonksiyonu ---
# Bu fonksiyonun, ana eÄŸitim script'indeki ile %100 aynÄ± olmasÄ± Ã¶nemlidir.
def _format_dialogue(item):
    dialogue = []
    for turn in item["donguler"]:
        role = turn["rol"]
        content = turn.get("icerik")
        if role == "kullanici":
            dialogue.append({"role": "user", "content": content})
        elif role == "asistan":
            if content:
                dialogue.append({"role": "assistant", "content": content})
            if "arac_cagrilari" in turn and turn["arac_cagrilari"]:
                tool_calls = []
                for call in turn["arac_cagrilari"]:
                    tool_call_id = str(uuid.uuid4())
                    tool_calls.append({
                        "id": tool_call_id,
                        "type": "function",
                        "function": {"name": call["fonksiyon"], "arguments": json.dumps(call["parametreler"])}
                    })
                dialogue.append({"role": "assistant", "content": None, "tool_calls": tool_calls})
        elif role == "arac":
            try:
                last_tool_call_id = dialogue[-1]["tool_calls"][0]["id"]
                dialogue.append({"role": "tool", "content": content, "tool_call_id": last_tool_call_id})
            except (KeyError, IndexError):
                pass
    return dialogue

# --- Ana DoÄŸrulama Fonksiyonu ---
def main():
    console.print("\n" + "="*60, style="bold yellow")
    console.print("ğŸ•µï¸  [bold yellow]EÄŸitim Veri FormatÄ± DoÄŸrulama AracÄ±[/bold yellow]")
    console.print("="*60)

    token = setup_huggingface_token()
    if not token:
        return

    # --- Tokenizer'Ä± YÃ¼kle ---
    try:
        console.print(f"\n[cyan]1. Tokenizer yÃ¼kleniyor:[/cyan] {BASE_MODEL_NAME}")
        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, token=token)
        console.print("[green]âœ… Tokenizer baÅŸarÄ±yla yÃ¼klendi.[/green]")
    except Exception as e:
        console.print(f"[bold red]HATA: Tokenizer yÃ¼klenemedi: {e}[/bold red]")
        return
        
    # --- Veri Ã–rneÄŸini YÃ¼kle ---
    try:
        console.print(f"\n[cyan]2. Veri dosyasÄ± okunuyor:[/cyan] {DATA_FILE_PATH}")
        with open(DATA_FILE_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # Sadece ilk diyalog Ã¶rneÄŸini alÄ±yoruz
        sample_item = data[0]
        console.print("[green]âœ… Ä°lk veri Ã¶rneÄŸi baÅŸarÄ±yla okundu.[/green]")
        console.print(f"   [dim]Senaryo: {sample_item['senaryo']}[/dim]")
    except Exception as e:
        console.print(f"[bold red]HATA: Veri dosyasÄ± okunamadÄ±: {e}[/bold red]")
        return

    # --- Formatlama ve Ã‡evirme Ä°ÅŸlemi ---
    console.print("\n[cyan]3. Ã–rnek veri, Llama-3 formatÄ±na Ã§evriliyor...[/cyan]")
    
    # AdÄ±m a: Diyalogu sÃ¶zlÃ¼k listesine Ã§evir
    dialogue_dict_list = _format_dialogue(sample_item)
    
    # AdÄ±m b: Tokenizer'Ä±n `apply_chat_template` fonksiyonunu kullan
    # Bu, tÃ¼m Ã¶zel Llama-3 token'larÄ±nÄ± ekleyerek nihai string'i oluÅŸturur.
    final_formatted_string = tokenizer.apply_chat_template(
        dialogue_dict_list, 
        tokenize=False, 
        add_generation_prompt=False # EÄŸitimde de False kullanÄ±yoruz
    )
    
    console.print("[green]âœ… Ã‡evirme iÅŸlemi tamamlandÄ±.[/green]")
    
    # --- Sonucu GÃ¶ster ---
    console.print("\n" + "-"*60, style="bold blue")
    console.print("âœ¨ [bold blue]Ä°ÅLENMÄ°Å NÄ°HAÄ° METÄ°N (Modelin eÄŸitimde gÃ¶rdÃ¼ÄŸÃ¼ hali):[/bold blue]")
    console.print("-"*60)
    
    # Ã‡Ä±ktÄ±yÄ± daha okunaklÄ± hale getirmek iÃ§in rich.Syntax kullanÄ±yoruz
    syntax = Syntax(final_formatted_string, "html", theme="monokai", line_numbers=False)
    console.print(syntax)
    console.print("\n" + "-"*60, style="bold blue")
    console.print("â“ [bold]KONTROL EDÄ°LECEKLER:[/bold]")
    console.print("   - Ã‡Ä±ktÄ±da `<|start_header_id|>tool_calls<|end_header_id|>` gibi Ã¶zel token'lar gÃ¶rÃ¼nÃ¼yor mu?")
    console.print("   - AraÃ§ Ã§aÄŸÄ±rma parametreleri dÃ¼zgÃ¼n bir JSON string'i olarak gÃ¶rÃ¼nÃ¼yor mu?")
    console.print("   - Genel yapÄ±, Llama-3'Ã¼n resmi dÃ¶kÃ¼mantasyonundaki sohbet formatÄ±na benziyor mu?")
    console.print("-"*60)

if __name__ == "__main__":
    main() 