# -*- coding: utf-8 -*-
"""
ğŸš€ Hugging Face Modelini Evrensel GGUF FormatÄ±na DÃ¶nÃ¼ÅŸtÃ¼rme Script'i (v2 - BasitleÅŸtirilmiÅŸ)
==========================================================================================

Bu script, Hugging Face Hub'daki bir transformatÃ¶r modelini, evrensel
GGUF formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

Ã–NEMLÄ° GEREKSÄ°NÄ°M:
-------------------
Bu script'i Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce, terminalde aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rarak
Hugging Face hesabÄ±nÄ±za giriÅŸ yapmÄ±ÅŸ olmalÄ±sÄ±nÄ±z:
```bash
huggingface-cli login
```

KULLANIM:
----------
Script'i proje ana dizininden Ã§alÄ±ÅŸtÄ±rÄ±n:
```bash
python UniqeAi/ai_model/scripts/convert_to_gguf.py
```

"""
import os
import sys
import subprocess
from pathlib import Path
import shutil

# --- YapÄ±landÄ±rma ---
# Proje ana dizinini belirle (bu script'in bulunduÄŸu yerin 4 Ã¼st dizini)
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except NameError:
    # EtkileÅŸimli bir ortamda Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yorsa (Ã¶rn. Jupyter)
    PROJECT_ROOT = Path.cwd()

# DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lecek modelin Hugging Face Hub adresi
MODEL_ID = "Choyrens/ChoyrensAI-Telekom-Agent-v1-merged"

# llama.cpp'nin klonlanacaÄŸÄ± dizin
LLAMA_CPP_DIR = PROJECT_ROOT / "UniqeAi" / "llama.cpp"

# GGUF modelinin kaydedileceÄŸi dizin
OUTPUT_DIR = PROJECT_ROOT / "UniqeAi" / "ai_model" / "gguf_model"

# GGUF niceleme (quantization) formatÄ±.
# Q8_0, kalite ve dosya boyutu arasÄ±nda harika bir denge sunar.
QUANTIZATION_TYPE = "Q8_0"

# --- Script MantÄ±ÄŸÄ± ---

def run_command(command, cwd, check=True):
    """Belirtilen dizinde bir komut Ã§alÄ±ÅŸtÄ±rÄ±r ve Ã§Ä±ktÄ±sÄ±nÄ± canlÄ± olarak yazdÄ±rÄ±r."""
    print(f"ğŸ”© Komut Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor: `{' '.join(command)}` in `{cwd}`")
    try:
        process = subprocess.Popen(
            command,
            cwd=cwd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            encoding='utf-8',
            errors='replace'
        )
        while True:
            line = process.stdout.readline()
            if not line:
                break
            print(line.strip())
        process.wait()
        if check and process.returncode != 0:
            raise subprocess.CalledProcessError(process.returncode, command)
        print(f"âœ… Komut baÅŸarÄ±yla tamamlandÄ±.")
        return True
    except FileNotFoundError:
        print(f"âŒ HATA: Komut bulunamadÄ±: `{command[0]}`. PATH ortam deÄŸiÅŸkeninizi kontrol edin.")
        return False
    except subprocess.CalledProcessError as e:
        print(f"âŒ HATA: Komut `{e.cmd}` {e.returncode} hata koduyla baÅŸarÄ±sÄ±z oldu.")
        return False
    except Exception as e:
        print(f"Beklenmedik bir hata oluÅŸtu: {e}")
        return False

def setup_llama_cpp():
    """llama.cpp reposunu klonlar ve baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± kurar."""
    if not (LLAMA_CPP_DIR / "convert_hf_to_gguf.py").exists():
        if LLAMA_CPP_DIR.exists():
            print(f"ğŸ§¹ Mevcut ama eksik `llama.cpp` klasÃ¶rÃ¼ temizleniyor...")
            shutil.rmtree(LLAMA_CPP_DIR)
        print(f"ğŸ“‚ `llama.cpp` klonlanÄ±yor...")
        if not run_command(["git", "clone", "https://github.com/ggerganov/llama.cpp.git", str(LLAMA_CPP_DIR)], cwd=PROJECT_ROOT):
            print("HATA: `llama.cpp` klonlanamadÄ±. Git'in yÃ¼klÃ¼ ve eriÅŸilebilir olduÄŸundan emin olun.")
            sys.exit(1)
    else:
         print(f"âœ… `llama.cpp` zaten doÄŸru bir ÅŸekilde mevcut: {LLAMA_CPP_DIR}")

    print("ğŸ `llama.cpp` Python baÄŸÄ±mlÄ±lÄ±klarÄ± kuruluyor...")
    requirements_path = LLAMA_CPP_DIR / "requirements.txt"
    if not run_command([sys.executable, "-m", "pip", "install", "-r", str(requirements_path)], cwd=LLAMA_CPP_DIR):
        print("HATA: `llama.cpp` baÄŸÄ±mlÄ±lÄ±klarÄ± kurulamadÄ±.")
        sys.exit(1)
    if not run_command([sys.executable, "-m", "pip", "install", "sentencepiece"], cwd=LLAMA_CPP_DIR):
        print("HATA: 'sentencepiece' kurulamadÄ±.")
        sys.exit(1)

def convert_model():
    """Modeli GGUF formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
    print("\n" + "="*50)
    print("ğŸš€ Model GGUF formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
    print(f"Model: {MODEL_ID}")
    print(f"Niceleme: {QUANTIZATION_TYPE}")
    print("="*50 + "\n")

    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    output_filename = f"{MODEL_ID.split('/')[-1].lower()}-{QUANTIZATION_TYPE.lower()}.gguf"
    output_filepath = OUTPUT_DIR / output_filename
    
    model_download_path = LLAMA_CPP_DIR / "models" / MODEL_ID.split("/")[-1]

    convert_script = LLAMA_CPP_DIR / "convert_hf_to_gguf.py"

    command = [
        sys.executable,
        str(convert_script),
        MODEL_ID,
        "--remote",
        "--outfile",
        str(output_filepath),
        "--outtype",
        QUANTIZATION_TYPE.lower(),
    ]

    if not run_command(command, cwd=LLAMA_CPP_DIR):
        print("âŒ HATA: Model dÃ¶nÃ¼ÅŸtÃ¼rme iÅŸlemi baÅŸarÄ±sÄ±z oldu.")
        if model_download_path.exists():
            print(f"ğŸ§¹ Temizlik: Ä°ndirilen model kalÄ±ntÄ±larÄ± siliniyor: {model_download_path}")
            shutil.rmtree(model_download_path, ignore_errors=True)
        sys.exit(1)
        
    print("\n" + "*"*50)
    print("ğŸ‰ DÃ–NÃœÅTÃœRME BAÅARILI! ğŸ‰")
    print(f"Modeliniz ÅŸuraya kaydedildi: {output_filepath}")
    if output_filepath.exists():
        print(f"Dosya Boyutu: {output_filepath.stat().st_size / (1024**3):.2f} GB")
    print("*"*50)
    
    if model_download_path.exists():
        print(f"ğŸ§¹ Temizlik: Ä°ndirilen orijinal model siliniyor: {model_download_path}")
        shutil.rmtree(model_download_path, ignore_errors=True)


def main():
    """Ana Ã§alÄ±ÅŸma fonksiyonu."""
    print("=== GGUF DÃ–NÃœÅTÃœRME SÄ°HÄ°RBAZI (v2) ===")
    os.chdir(PROJECT_ROOT)
    setup_llama_cpp()
    convert_model()
    print("\nâœ… TÃ¼m iÅŸlemler tamamlandÄ±.")

if __name__ == "__main__":
    main() 