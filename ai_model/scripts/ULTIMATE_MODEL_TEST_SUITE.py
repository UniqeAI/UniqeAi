# -*- coding: utf-8 -*-
"""
ğŸš€ ULTIMATE MODEL TEST SUITE v3.0 (Proje Final SÃ¼rÃ¼mÃ¼)
==========================================================

Bu sÃ¼rÃ¼m, `terminal_app.py v4.0` ile tam senkronize hale getirilmiÅŸtir.
Modelin en son yeteneklerini, en gÃ¼ncel ve doÄŸru yÃ¶ntemlerle Ã¶lÃ§er.

--- YENÄ°LÄ°KLER (v3.0) ---
- âœ… **Tam Senkronizasyon:** Sistem mesajÄ±, araÃ§ ayrÄ±ÅŸtÄ±rÄ±cÄ± ve ana mantÄ±k,
  projenin son sÃ¼rÃ¼m `terminal_app.py` ile %100 aynÄ±dÄ±r.
- âŒ **Gereksiz Kodlar KaldÄ±rÄ±ldÄ±:** ArtÄ±k terk edilmiÅŸ olan `should_use_tool`
  ve "ikinci ÅŸans verme" mekanizmalarÄ± tamamen kaldÄ±rÄ±ldÄ±.
- ğŸ§  **En Son Zeka Entegre Edildi:** "AkÄ±llÄ± Parametre Tamamlama" ve
  "AdÄ±m AdÄ±m DÃ¼ÅŸÃ¼nme" gibi en son prompt mÃ¼hendisliÄŸi teknikleri
  test sÃ¼recine dahil edildi.
"""

import os
import json
import re
import sys
import time
from pathlib import Path
from rich.console import Console
from rich.markdown import Markdown
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime

# --- Proje KÃ¶k Dizini ve ModÃ¼l Yolu ---
try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except (NameError, IndexError):
    PROJECT_ROOT = Path.cwd()

AI_MODEL_SCRIPTS_PATH = PROJECT_ROOT / "UniqeAi" / "ai_model" / "scripts"
if str(AI_MODEL_SCRIPTS_PATH) not in sys.path:
    sys.path.insert(0, str(AI_MODEL_SCRIPTS_PATH))

try:
    from tool_definitions import get_tool_definitions, get_tool_response
except ImportError:
    print(f"\n[HATA] 'tool_definitions' modÃ¼lÃ¼ bulunamadÄ±. Aranan yol: {AI_MODEL_SCRIPTS_PATH}")
    sys.exit(1)

# --- Model YapÄ±landÄ±rmasÄ± ---
GGUF_MODEL_DIR = PROJECT_ROOT / "UniqeAi" / "ai_model" / "gguf_model_v2"
CONTEXT_SIZE = 4096
GPU_LAYERS = -1
TEMPERATURE = 0.0
DEFAULT_TEST_USER_ID = 12345

console = Console()
ALL_TOOL_DEFINITIONS = {tool['function']['name']: tool for tool in get_tool_definitions()}


class ModelCapabilityTester:
    """Modelin yeteneklerini sistematik olarak test eden ana sÄ±nÄ±f"""
    
    def __init__(self, llm_model):
        self.llm = llm_model
        self.test_results = {}
        
    def create_system_prompt(self) -> str:
        """ terminal_app.py ile %100 aynÄ± sistem mesajÄ±nÄ± oluÅŸturur."""
        tools_string = "\n".join([
            f"  - `{name}`: {tool['function']['description']}"
            for name, tool in ALL_TOOL_DEFINITIONS.items()
        ])
        prompt = f"""Sen, UniqeAi tarafÄ±ndan geliÅŸtirilmiÅŸ, uzman bir Telekom MÃ¼ÅŸteri Hizmetleri AsistanÄ±sÄ±n.
**ANA GÃ–REVÄ°N:** KullanÄ±cÄ±nÄ±n talebini anÄ±nda yerine getirmek. Asla sohbet etme, soru sorma veya aÃ§Ä±klama yapma. **Sadece istenen eylemi gerÃ§ekleÅŸtir.**
**ARAÃ‡ KULLANIM KURALI:** Bir aracÄ± kullanman gerekiyorsa, baÅŸka hiÃ§bir ÅŸey yazmadan **SADECE araÃ§ Ã§aÄŸÄ±rma kodunu** ÅŸu formatta yaz: `<|begin_of_tool_code|>print(fonksiyon(parametre="deÄŸer"))<|end_of_tool_code|>`.
**KULLANABÄ°LECEÄÄ°N ARAÃ‡LAR:**
{tools_string}
"""
        return prompt.strip()

    def parse_tool_calls(self, text: str) -> Tuple[Optional[List[Dict[str, Any]]], str]:
        """HATA TOLERANSLI AYRIÅTIRICI: print() olsa da olmasa da araÃ§ Ã§aÄŸrÄ±sÄ±nÄ± yakalar."""
        pattern_with_print = r"<\|begin_of_tool_code\|>\s*print\((\w+)\((.*?)\)\)\s*<\|end_of_tool_code\|>"
        pattern_without_print = r"<\|begin_of_tool_code\|>\s*(\w+)\((.*?)\)\s*<\|end_of_tool_code\|>"
        
        match = re.search(pattern_with_print, text, re.DOTALL)
        pattern_used = pattern_with_print
        if not match:
            match = re.search(pattern_without_print, text, re.DOTALL)
            pattern_used = pattern_without_print

        if not match: return None, text

        function_name, args_str = match.group(1), match.group(2)
        params = {}
        if args_str:
            arg_pattern = re.compile(r"(\w+)\s*=\s*((?:\"(?:\\\"|[^\"])*\")|(?:'(?:\\'|[^'])*')|[\w.-]+)")
            for p_match in arg_pattern.finditer(args_str):
                key, raw_value = p_match.group(1), p_match.group(2)
                try: value = json.loads(raw_value)
                except (json.JSONDecodeError, TypeError): value = str(raw_value).strip("'\"")
                params[key] = value

        cleaned_text = re.sub(pattern_used, '', text).strip()
        tool_call = {"function": {"name": function_name, "arguments": params}}
        return [tool_call], cleaned_text

    def run_test_scenario(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """Tek bir test senaryosunu, terminal_app.py v4.0 mantÄ±ÄŸÄ±yla Ã§alÄ±ÅŸtÄ±rÄ±r."""
        console.print(f"\nğŸ¯ [bold blue]Test Senaryosu:[/bold blue] {scenario['name']}")
        console.print(f"ğŸ“‹ [italic]{scenario['description']}[/italic]")
        
        system_prompt = self.create_system_prompt()
        dialogue = [{"role": "system", "content": system_prompt}, {"role": "user", "content": scenario['user_input']}]
        
        console.print(f"\nğŸ‘¤ [bold blue]KullanÄ±cÄ±:[/bold blue] {scenario['user_input']}")
        
        full_response = ""
        final_answer = ""
        tool_calls_detected = False

        with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), console=console) as progress:
            task = progress.add_task("Model dÃ¼ÅŸÃ¼nÃ¼yor...", total=None)
            try:
                response = self.llm.create_chat_completion(
                    messages=dialogue, temperature=TEMPERATURE, stop=["<|eot_id|>"]
                )
                assistant_response_text = response['choices'][0]['message']['content']
                
                tool_calls, cleaned_response = self.parse_tool_calls(assistant_response_text)
                progress.update(task, description="YanÄ±t analiz ediliyor...")

                if tool_calls:
                    tool_calls_detected = True
                    dialogue.append({"role": "assistant", "content": assistant_response_text})
                    
                    for call in tool_calls:
                        func_name, func_args = call["function"]["name"], call["function"]["arguments"]
                        
                        if func_name in ALL_TOOL_DEFINITIONS:
                            func_def = ALL_TOOL_DEFINITIONS[func_name]["function"]
                            required_params = func_def.get("parameters", {}).get("required", [])
                            if "user_id" in required_params and "user_id" not in func_args:
                                func_args["user_id"] = DEFAULT_TEST_USER_ID
                                console.print(f"ğŸ§  [italic yellow]UyarÄ±: Model 'user_id' parametresini unuttu. VarsayÄ±lan ID ({DEFAULT_TEST_USER_ID}) ile tamamlandÄ±.[/italic yellow]")
                        
                        console.print(f"ğŸ› ï¸ [bold yellow]AraÃ§ Ã‡aÄŸrÄ±sÄ±:[/bold yellow] {func_name}({json.dumps(func_args, ensure_ascii=False)})")
                        tool_response = get_tool_response(func_name, func_args)
                        console.print(f"âš™ï¸ [bold magenta]AraÃ§ YanÄ±tÄ±:[/bold magenta] {tool_response}")
                        dialogue.append({"role": "tool", "content": tool_response})

                    chain_of_thought_prompt = """API yanÄ±tÄ±nÄ± aldÄ±m. Åimdi adÄ±m adÄ±m TÃ¼rkÃ§e bir Ã¶zet oluÅŸturacaÄŸÄ±m.
1. **Ana Fikri Bul:** JSON iÃ§indeki en Ã¶nemli sonuÃ§ nedir? (Ã–rn: 'roaming aktif edildi' veya 'geÃ§miÅŸ faturalar baÅŸarÄ±yla listelendi').
2. **Ã–nemli DetaylarÄ± Ã‡Ä±kar:** Bu ana fikirle ilgili kilit rakamlar veya bilgiler nelerdir? (Ã–rn: 'gÃ¼nlÃ¼k Ã¼cret 25.0 TL' veya 'toplam 2 fatura bulundu, toplam tutar 225.95 TL').
3. **CÃ¼mleyi Kur:** Bu bilgileri birleÅŸtirerek, kullanÄ±cÄ±ya hitap eden, akÄ±cÄ± ve eksiksiz **tek bir TÃ¼rkÃ§e paragraf** oluÅŸtur.
LÃ¼tfen bana sadece 3. adÄ±mdaki nihai paragrafÄ± yanÄ±t olarak ver."""
                    dialogue.append({"role": "user", "content": chain_of_thought_prompt})
                    
                    progress.update(task, description="Model sonuÃ§larÄ± Ã¶zetliyor...")
                    
                    summary_response = self.llm.create_chat_completion(
                        messages=dialogue, temperature=TEMPERATURE, stop=["<|eot_id|>"]
                    )
                    final_answer = summary_response['choices'][0]['message']['content']
                    dialogue.pop() # GeÃ§ici CoT prompt'unu kaldÄ±r
                    dialogue.append({"role": "assistant", "content": final_answer})
                    
                    console.print(f"\nğŸ¤– [bold green]Model (Final):[/bold green]")
                    console.print(Markdown(final_answer))
                    full_response = assistant_response_text + "\n\n" + final_answer
                else:
                    final_answer = cleaned_response
                    dialogue.append({"role": "assistant", "content": final_answer})
                    console.print(f"\nğŸ¤– [bold green]Model:[/bold green]")
                    console.print(Markdown(final_answer))
                    full_response = final_answer
                
                progress.remove_task(task)
                score = self.evaluate_response(scenario, final_answer, tool_calls_detected)
                
                return {
                    "scenario_name": scenario['name'], "user_input": scenario['user_input'],
                    "model_response": full_response, "used_tools": tool_calls_detected,
                    "evaluation": score, "dialogue": dialogue
                }
            except Exception as e:
                progress.remove_task(task)
                console.print(f"[bold red]HATA:[/bold red] {e}")
                return {"scenario_name": scenario['name'], "error": str(e), "evaluation": {"total_score": 0}}

    def evaluate_response(self, scenario: Dict[str, Any], response: str, used_tools: bool) -> Dict[str, Any]:
        """Model yanÄ±tÄ±nÄ± deÄŸerlendirir"""
        scores = {}
        scores['relevance'] = self.score_relevance(response, scenario.get('expected_topics', []))
        scores['empathy'] = self.score_empathy(response, scenario.get('emotional_context', 'neutral'))
        scores['depth'] = self.score_depth(response)
        scores['tool_usage'] = 10 if used_tools and scenario.get('expects_tool_usage', False) else (5 if not used_tools and not scenario.get('expects_tool_usage', False) else 0)
        scores['creativity'] = self.score_creativity(response, scenario.get('complexity_level', 'medium'))
        
        for criterion in scenario.get('evaluation_criteria', {}):
            if criterion == 'strategic_thinking': scores[criterion] = self.score_strategic_thinking(response)
            elif criterion == 'negotiation_skills': scores[criterion] = self.score_negotiation_skills(response)
            elif criterion == 'cultural_sensitivity': scores[criterion] = self.score_cultural_sensitivity(response)
        
        total_score = sum(scores.values()) / len(scores) if scores else 0
        return {"individual_scores": scores, "total_score": round(total_score, 2), "grade": self.get_grade(total_score)}

    def score_relevance(self, r: str, topics: List[str]) -> float:
        if not topics: return 8.0
        found = sum(1 for t in topics if t.lower() in r.lower())
        return min(10.0, (found / len(topics)) * 10)

    def score_empathy(self, r: str, context: str) -> float:
        indicators = ['anlÄ±yorum', 'Ã¼zgÃ¼nÃ¼m', 'hissediyorum', 'destek', 'yanÄ±nÄ±zda', 'yardÄ±m', 'hassas']
        found = sum(1 for i in indicators if i in r.lower())
        return min(10.0, found * (2 if context != 'neutral' else 1.5) + (0 if context != 'neutral' else 5))

    def score_depth(self, r: str) -> float:
        indicators = ['analiz', 'strateji', 'Ã§Ã¶zÃ¼m', 'plan', 'yaklaÅŸÄ±m', 'deÄŸerlendirme', 'Ã¶neri', 'alternatif']
        return min(5.0, len(r.split()) / 50) + min(5.0, sum(1 for i in indicators if i in r.lower()) * 1.5)

    def score_creativity(self, r: str, complexity: str) -> float:
        indicators = ['inovatif', 'yaratÄ±cÄ±', 'Ã¶zgÃ¼n', 'farklÄ±', 'yeni', 'benzersiz', 'alternatif']
        found = sum(1 for i in indicators if i in r.lower())
        return min(10.0, found * (2.5 if complexity == 'high' else 2) + (0 if complexity == 'high' else 3))

    def score_strategic_thinking(self, r: str) -> float:
        indicators = ['uzun vadeli', 'kÄ±sa vadeli', 'plan', 'strateji', 'roadmap', 'aÅŸama', 'gelecek', 'Ã¶ngÃ¶rÃ¼']
        return min(10.0, sum(1 for i in indicators if i in r.lower()) * 2)

    def score_negotiation_skills(self, r: str) -> float:
        indicators = ['anlaÅŸma', 'uzlaÅŸma', 'karÅŸÄ±lÄ±klÄ±', 'win-win', 'denge', 'esneklik', 'alternatif', 'teklif']
        return min(10.0, sum(1 for i in indicators if i in r.lower()) * 2.5)

    def score_cultural_sensitivity(self, r: str) -> float:
        indicators = ['kÃ¼ltÃ¼r', 'gelenek', 'deÄŸer', 'saygÄ±', 'anlayÄ±ÅŸ', 'hassasiyet', 'toplum']
        return min(10.0, sum(1 for i in indicators if i in r.lower()) * 2 + 4)

    def get_grade(self, s: float) -> str:
        if s >= 9.0: return "A+ (OlaÄŸanÃ¼stÃ¼)"
        elif s >= 8.0: return "A (MÃ¼kemmel)"
        elif s >= 7.0: return "B+ (Ã‡ok Ä°yi)"
        elif s >= 6.0: return "B (Ä°yi)"
        elif s >= 5.0: return "C+ (Orta)"
        else: return "F (BaÅŸarÄ±sÄ±z)"

def find_latest_gguf_model(model_dir: Path) -> Optional[Path]:
    if not model_dir.exists(): return None
    gguf_files = list(model_dir.glob("*.gguf"))
    if not gguf_files: return None
    return max(gguf_files, key=lambda p: p.stat().st_mtime)

def load_gguf_model():
    try: from llama_cpp import Llama
    except ImportError: console.print("[bold red]HATA: `llama-cpp-python` kurulu deÄŸil.[/bold red]"); sys.exit(1)
    model_path = find_latest_gguf_model(GGUF_MODEL_DIR)
    if not model_path: console.print("[bold red]HATA: GGUF model bulunamadÄ±![/bold red]"); sys.exit(1)
    console.print(f"[yellow]ğŸš€ Model yÃ¼kleniyor: [cyan]{model_path.name}[/cyan][/yellow]")
    try:
        llm = Llama(
            model_path=str(model_path), n_ctx=CONTEXT_SIZE, n_gpu_layers=GPU_LAYERS,
            n_threads=os.cpu_count() - 1 if os.cpu_count() and os.cpu_count() > 1 else 1,
            verbose=False, chat_format="llama-3"
        )
        console.print("[green]âœ… Model baÅŸarÄ±yla yÃ¼klendi.[/green]")
        return llm
    except Exception as e:
        console.print(f"[bold red]HATA: Model yÃ¼klenemedi - {e}[/bold red]"); sys.exit(1)

def get_test_scenarios() -> List[Dict[str, Any]]:
    return [
        {"name": "Empathetic Reasoning - Vefat Durumu", "description": "Modelin hassas durumlarda empati gÃ¶sterme ve Ã§Ã¶zÃ¼m sunma yeteneÄŸini test eder.", "user_input": "Babam geÃ§en ay vefat etti. Onun telefonunu ve internet aboneliÄŸini kapatmak istiyorum ama Ã§ok zor geliyor. Bu sÃ¼reÃ§te bana nasÄ±l yardÄ±mcÄ± olabilirsiniz?", "emotional_context": "grief", "complexity_level": "high", "expects_tool_usage": True, "expected_topics": ["baÅŸsaÄŸlÄ±ÄŸÄ±", "Ã¼zgÃ¼nÃ¼m", "yardÄ±mcÄ±", "sÃ¼reÃ§", "kolaylaÅŸtÄ±rmak"], "evaluation_criteria": {"cultural_sensitivity": 1.5}},
        {"name": "Strategic Planning - BÃ¼yÃ¼me Stratejisi", "description": "Uzun vadeli stratejik dÃ¼ÅŸÃ¼nme ve planlama yeteneÄŸini deÄŸerlendirir.", "user_input": "Ä°ÅŸimiz bÃ¼yÃ¼yor, 2 yÄ±lda 25 kiÅŸilik bir ekibe ulaÅŸacaÄŸÄ±z ve hibrit Ã§alÄ±ÅŸacaÄŸÄ±z. Bize Ã¶zel, Ã¶lÃ§eklenebilir bir telekomÃ¼nikasyon altyapÄ± planÄ± sunabilir misiniz?", "emotional_context": "neutral", "complexity_level": "high", "expects_tool_usage": True, "expected_topics": ["strateji", "bÃ¼yÃ¼me", "plan", "hibrit", "uzun vadeli", "Ã¶lÃ§eklenebilir"], "evaluation_criteria": {"strategic_thinking": 2.0}},
        {"name": "Social Dynamics - Toplumsal Ã‡Ã¶zÃ¼m", "description": "Sosyal dinamikleri anlayÄ±p toplumsal Ã§Ã¶zÃ¼mler Ã¶nerme yeteneÄŸini test eder.", "user_input": "BÃ¼tÃ¼n mahalle olarak internetimiz Ã§ok yavaÅŸ ve Ã§ocuklarÄ±mÄ±z derslerinden geri kalÄ±yor. KomÅŸularla konuÅŸtuk, toplu bir altyapÄ± iyileÅŸtirmesi iÃ§in ne yapabiliriz?", "emotional_context": "frustrated", "complexity_level": "high", "expects_tool_usage": True, "expected_topics": ["topluluk", "mahalle", "toplu Ã§Ã¶zÃ¼m", "altyapÄ±", "iÅŸ birliÄŸi"], "evaluation_criteria": {"creativity": 1.5, "cultural_sensitivity": 1.2}},
        {"name": "Negotiation Skills - Paket MÃ¼zakeresi", "description": "MÃ¼zakere ve ikna yeteneklerini deÄŸerlendirir.", "user_input": "Rakip firma bana daha iyi bir teklif sundu ama ben sizinle devam etmek istiyorum. Mevcut paketimi hem daha ekonomik hem de daha zengin hale getirecek bir orta yol bulabilir miyiz?", "emotional_context": "neutral", "complexity_level": "medium", "expects_tool_usage": True, "expected_topics": ["mÃ¼zakere", "anlaÅŸma", "teklif", "orta yol", "daha iyi"], "evaluation_criteria": {"negotiation_skills": 2.0}},
        {"name": "Conflicting Information - Bilgi TutarsÄ±zlÄ±ÄŸÄ±", "description": "Ã‡eliÅŸkili bilgileri yÃ¶netme ve doÄŸru bilgiyi sunma yeteneÄŸini test eder.", "user_input": "MÃ¼ÅŸteri hizmetleri 5G'nin bÃ¶lgemde tam kapasite Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± sÃ¶yledi ama mobil uygulamanÄ±zda sinyal zayÄ±f gÃ¶rÃ¼nÃ¼yor. GerÃ§ek durum nedir, net bir bilgi alabilir miyim?", "emotional_context": "skeptical", "complexity_level": "high", "expects_tool_usage": True, "expected_topics": ["tutarsÄ±zlÄ±k", "net bilgi", "doÄŸrulama", "gerÃ§ek durum", "analiz"], "evaluation_criteria": {"strategic_thinking": 1.5}},
        {"name": "Creative Problem Solving - Ä°novatif Ã‡Ã¶zÃ¼m", "description": "YaratÄ±cÄ± ve inovatif problem Ã§Ã¶zme yeteneklerini deÄŸerlendirir.", "user_input": "Ben bir iÃ§erik Ã¼reticisiyim ve yayÄ±n yaptÄ±ÄŸÄ±m saatlerde internet hÄ±zÄ±mÄ±n en Ã¼st seviyede olmasÄ±nÄ± istiyorum. DiÄŸer zamanlarda daha dÃ¼ÅŸÃ¼k olabilir. Bana Ã¶zel, esnek bir hÄ±z ayarlama paketi oluÅŸturabilir misiniz?", "emotional_context": "curious", "complexity_level": "medium", "expects_tool_usage": True, "expected_topics": ["yaratÄ±cÄ±", "Ã¶zel", "esnek", "iÃ§erik Ã¼reticisi", "optimizasyon"], "evaluation_criteria": {"creativity": 2.0}}
    ]

def display_test_results(results: List[Dict[str, Any]]):
    """Test sonuÃ§larÄ±nÄ± gÃ¶rsel olarak sunar"""
    table = Table(title="ğŸ† ULTIMATE MODEL TEST RESULTS (v3.0)")
    table.add_column("Test Senaryosu", style="cyan"); table.add_column("Toplam Skor", justify="center", style="green")
    table.add_column("Harf Notu", justify="center", style="yellow"); table.add_column("AraÃ§ KullanÄ±mÄ±", justify="center", style="blue")
    total_score, valid_results = 0, 0
    for result in results:
        if 'error' not in result:
            score, grade, used_tools = result['evaluation']['total_score'], result['evaluation']['grade'], "âœ…" if result['used_tools'] else "âŒ"
            table.add_row(result['scenario_name'], f"{score:.1f}/10", grade, used_tools)
            total_score += score; valid_results += 1
        else:
            table.add_row(result['scenario_name'], "ERROR", "F", "âŒ")
    console.print(table)
    if valid_results > 0:
        avg_score = total_score / valid_results
        console.print(Panel(f"ğŸ¯ **Genel Performans:** {avg_score:.1f}/10 ({ModelCapabilityTester(None).get_grade(avg_score)})\n\nğŸ’¡ **DeÄŸerlendirme:**\n{get_performance_assessment(avg_score)}", title="ğŸš€ ULTIMATE MODEL ASSESSMENT", expand=False))

def get_performance_assessment(s: float) -> str:
    if s >= 9.0: return "ğŸŒŸ Model olaÄŸanÃ¼stÃ¼ performans sergiliyor! Empati, strateji ve yaratÄ±cÄ±lÄ±kta insan seviyesinde yetenek gÃ¶steriyor."
    elif s >= 8.0: return "ğŸ‰ Model mÃ¼kemmel performans gÃ¶steriyor! KarmaÅŸÄ±k senaryolarÄ± baÅŸarÄ±yla Ã§Ã¶zebiliyor."
    elif s >= 7.0: return "ğŸ‘ Model Ã§ok iyi performans sergiliyor. Ã‡oÄŸu durumda tatmin edici Ã§Ã¶zÃ¼mler sunuyor."
    else: return "âš ï¸ Modelin geliÅŸime aÃ§Ä±k alanlarÄ± var. Ã–zellikle karmaÅŸÄ±k muhakeme gerektiren konularda daha fazla eÄŸitim faydalÄ± olabilir."

def main():
    """Ana program"""
    console.print("\n" + "="*80, style="bold green")
    console.print("ğŸš€ [bold green]ULTIMATE MODEL TEST SUITE v3.0 (Proje Final SÃ¼rÃ¼mÃ¼)[/bold green]", justify="center")
    console.print("="*80, style="bold green")
    llm_model = load_gguf_model()
    tester = ModelCapabilityTester(llm_model)
    test_scenarios = get_test_scenarios()
    console.print(f"\nğŸ“‹ [bold blue]{len(test_scenarios)} adet nihai test senaryosu yÃ¼klendi.[/bold blue]")
    start_test = console.input("ğŸš€ Teste baÅŸlamak iÃ§in ENTER'a basÄ±n (Ã§Ä±kmak iÃ§in 'q'): ")
    if start_test.lower() == 'q': return
    results = [tester.run_test_scenario(s) for s in test_scenarios]
    console.print(f"\n{'='*80}\nğŸ“Š [bold green]TEST TAMAMLANDI - SONUÃ‡LAR[/bold green]\n{'='*80}")
    display_test_results(results)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = PROJECT_ROOT / f"UniqeAi/ai_model/test_results_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    console.print(f"\nğŸ’¾ [green]Test sonuÃ§larÄ± kaydedildi: {results_file}[/green]")
    console.print("\nğŸ‰ [bold green]Ultimate Model Test Suite tamamlandÄ±![/bold green]")

if __name__ == "__main__":
    main()
