# -*- coding: utf-8 -*-
"""
ğŸš€ Uzman Seviye QLoRA EÄŸitim Script'i (Expert-Level QLoRA Trainer)
===================================================================

Bu script, "olaÄŸanÃ¼stÃ¼" bir AI modeli hedefiyle, Llama-3-Instruct
modellerini QLoRA tekniÄŸi ile verimli ve profesyonel bir ÅŸekilde
eÄŸitmek iÃ§in tasarlanmÄ±ÅŸtÄ±r.

Ã–nceki script'in eksikliklerini giderir ve en iyi pratikleri uygular:
- âœ… **DoÄŸru Chat Template:** Llama 3'Ã¼n native sohbet ve araÃ§ Ã§aÄŸÄ±rma
  formatÄ±nÄ± kullanarak modelin tam potansiyelini aÃ§Ä±ÄŸa Ã§Ä±karÄ±r.
- âœ… **Dinamik Veri YÃ¶netimi:** EÄŸitilecek veri dosyalarÄ±nÄ± bir liste
  olarak alarak aÅŸamalÄ± eÄŸitimi (phased training) destekler.
- âœ… **Train/Validation Split:** Veri setini otomatik olarak eÄŸitim ve
  deÄŸerlendirme setlerine ayÄ±rarak overfitting'i izler ve en iyi modeli
  kaydeder.
- âœ… **SFTTrainer:** Hugging Face'in modern ve verimli Supervised
  Fine-tuning Trainer'Ä±nÄ± kullanÄ±r.
- âœ… **W&B Entegrasyonu:** Weights & Biases ile eÄŸitim metriklerinin
  izlenmesini saÄŸlar (opsiyonel).
- âœ… **ModÃ¼ler YapÄ±landÄ±rma:** TÃ¼m parametreler tek bir `TrainingConfig`
  sÄ±nÄ±fÄ±nda toplanarak deney yÃ¶netimini kolaylaÅŸtÄ±rÄ±r.
"""

import os
import gc
import json
import torch
import logging
import uuid  # UUID kÃ¼tÃ¼phanesini ekliyoruz
from datetime import datetime
from typing import List, Dict, Any
from dataclasses import dataclass, field
from pathlib import Path  # pathlib'i ekliyoruz
from dotenv import load_dotenv

# ML KÃ¼tÃ¼phaneleri
from datasets import Dataset, load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments as HfTrainingArguments, # Orijinalini yeniden adlandÄ±rÄ±yoruz
    BitsAndBytesConfig,
    HfArgumentParser, 
    SchedulerType
)
# HATA Ã‡Ã–ZÃœMÃœ: Checkpoint kontrolÃ¼ iÃ§in bu yardÄ±mcÄ± fonksiyonu ekliyoruz
from transformers.trainer_utils import get_last_checkpoint
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# Logging ayarlarÄ±
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- Proje KÃ¶k Dizini TanÄ±mlamasÄ± ---
# Script'in nerede Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±ndan baÄŸÄ±msÄ±z olarak, her zaman doÄŸru yollarÄ± bulmasÄ±nÄ± saÄŸlar.
# expert_trainer.py -> scripts -> ai_model -> UniqeAi -> tddi_proje_planlama (KÃ–K DÄ°ZÄ°N)
PROJECT_ROOT = Path(__file__).resolve().parents[3]

# --- 1. YapÄ±landÄ±rma (Configuration) ---

@dataclass
class ModelAndDataConfig:
    """
    Model, tokenizer ve veri ile ilgili temel yapÄ±landÄ±rmayÄ± tanÄ±mlar.
    Bu parametreler komut satÄ±rÄ±ndan override edilebilir.
    """
    # Model ve Tokenizer AyarlarÄ±
    model_name: str = field(
        default="meta-llama/Meta-Llama-3-8B-Instruct",
        metadata={"help": "KullanÄ±lacak Hugging Face modelinin adÄ±."}
    )
    
    # Veri Seti AyarlarÄ±
    data_paths: List[str] = field(
        default_factory=lambda: [
            "UniqeAi/ai_model/data/expert_tool_chaining_data_v2_validated.json",
            "UniqeAi/ai_model/data/ultra_disambiguation_data_v1_validated.json"
        ],
        metadata={"help": "EÄŸitim iÃ§in kullanÄ±lacak JSON veri dosyalarÄ±nÄ±n proje kÃ¶kÃ¼nden gÃ¶reli yollarÄ±."}
    )
    test_size: float = field(
        default=0.1,
        metadata={"help": "DeÄŸerlendirme seti iÃ§in ayrÄ±lacak veri yÃ¼zdesi."}
    )

    # QLoRA AyarlarÄ± (RTX 4060 iÃ§in optimize)
    lora_r: int = field(default=16, metadata={"help": "LoRA rank."})
    lora_alpha: int = field(default=32, metadata={"help": "LoRA alpha."})
    lora_dropout: float = field(default=0.05, metadata={"help": "LoRA dropout."})
    lora_target_modules: List[str] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        metadata={"help": "LoRA uygulanacak modÃ¼ller."}
    )
    
    # W&B Entegrasyonu
    wandb_project: str = field(default="ChoyrensAI-Telekom-Agent", metadata={"help": "Weights & Biases proje adÄ±."})
    use_wandb: bool = field(default="WANDB_API_KEY" in os.environ, metadata={"help": "W&B entegrasyonunu etkinleÅŸtir."})

@dataclass
class TrainingArguments(HfTrainingArguments):
    """
    EÄŸitim argÃ¼manlarÄ± iÃ§in varsayÄ±lan deÄŸerlerimizi burada tanÄ±mlÄ±yoruz.
    BunlarÄ±n hepsi komut satÄ±rÄ±ndan override edilebilir.
    """
    output_dir: str = "UniqeAi/ai_model/results"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 1
    per_device_eval_batch_size: int = 1
    gradient_accumulation_steps: int = 4
    gradient_checkpointing: bool = True
    learning_rate: float = 2e-5
    logging_strategy: str = "steps"
    logging_steps: int = 10
    save_strategy: str = "steps"
    save_steps: int = 50
    eval_strategy: str = "steps"
    eval_steps: int = 50
    bf16: bool = True
    load_best_model_at_end: bool = True
    metric_for_best_model: str = "eval_loss"
    save_total_limit: int = 2
    # Daha iyi yakÄ±nsama iÃ§in kosinÃ¼s Ã¶ÄŸrenme oranÄ± zamanlayÄ±cÄ±sÄ±
    lr_scheduler_type: SchedulerType = "cosine"
    # Daha bellek verimli bir optimizer
    optim: str = "paged_adamw_8bit"
    report_to: str = "wandb" if "WANDB_API_KEY" in os.environ else "none"

    # UYARI GÄ°DERME: SFTTrainer iÃ§in eskime uyarÄ±sÄ± veren parametreleri buraya taÅŸÄ±yoruz.
    max_seq_length: int = 2048
    dataset_text_field: str = "text"
    packing: bool = True

def setup_huggingface_token():
    """Hugging Face token'Ä±nÄ± .env dosyasÄ±ndan veya ortam deÄŸiÅŸkeninden kurar."""
    # Proje kÃ¶k dizinindeki .env dosyasÄ±nÄ± yÃ¼klemeyi dener
    dotenv_path = PROJECT_ROOT / ".env"
    
    # python-dotenv'i kullanarak .env dosyasÄ±nÄ± yÃ¼kle
    load_dotenv(dotenv_path=dotenv_path)
    
    token = os.getenv('HUGGINGFACE_HUB_TOKEN')
    if not token:
        logger.error("HUGGINGFACE_HUB_TOKEN, .env dosyasÄ±nda veya ortam deÄŸiÅŸkenlerinde bulunamadÄ±.")
        logger.error("LÃ¼tfen proje ana dizininde bir .env dosyasÄ± oluÅŸturup iÃ§ine HUGGINGFACE_HUB_TOKEN='hf_...' satÄ±rÄ±nÄ± ekleyin.")
        return False
        
    os.environ['HF_TOKEN'] = token
    logger.info("âœ… Hugging Face token baÅŸarÄ±yla kuruldu.")
    return True


class ExpertTrainer:
    def __init__(self, model_config: ModelAndDataConfig, training_args: TrainingArguments):
        self.config = model_config
        self.training_args = training_args
        if not setup_huggingface_token():
            raise ValueError("EÄŸitimi baÅŸlatmak iÃ§in Hugging Face token'Ä± gereklidir.")
        
        # Ã‡Ä±ktÄ± dizinini mutlak yola Ã§evirerek "kayÄ±p checkpoint" sorununu Ã§Ã¶zÃ¼yoruz.
        self.training_args.output_dir = str(PROJECT_ROOT / self.training_args.output_dir)
        os.makedirs(self.training_args.output_dir, exist_ok=True)

    def _format_dialogue(self, item: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Bizim JSON formatÄ±mÄ±zÄ± Llama-3'Ã¼n Chat Template formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
        Bu fonksiyon, bu script'in en kritik parÃ§asÄ±dÄ±r.
        """
        dialogue = []
        for turn in item["donguler"]:
            role = turn["rol"]
            content = turn.get("icerik")

            if role == "kullanici":
                dialogue.append({"role": "user", "content": content})
            
            elif role == "asistan":
                if content: # EÄŸer asistanÄ±n sÃ¶zlÃ¼ yanÄ±tÄ± varsa
                    dialogue.append({"role": "assistant", "content": content})
                
                if "arac_cagrilari" in turn and turn["arac_cagrilari"]:
                    # Llama 3 tool call formatÄ±
                    tool_calls = []
                    for call in turn["arac_cagrilari"]:
                        # DAHA SAÄLAM: Her Ã§aÄŸrÄ± iÃ§in Ã§akÄ±ÅŸmasÄ± imkansÄ±z bir UUID kullanÄ±yoruz.
                        tool_call_id = str(uuid.uuid4())
                        tool_calls.append({
                            "id": tool_call_id,
                            "type": "function",
                            "function": {
                                "name": call["fonksiyon"],
                                "arguments": json.dumps(call["parametreler"])
                            }
                        })
                    dialogue.append({"role": "assistant", "content": None, "tool_calls": tool_calls})

            elif role == "arac":
                # Llama 3 tool response formatÄ±
                # MEVCUT VARSAYIM: Veri setimizde her "arac" yanÄ±tÄ±, bir Ã¶nceki "asistan"
                # mesajÄ±ndaki ilk ve tek araÃ§ Ã§aÄŸrÄ±sÄ±na karÅŸÄ±lÄ±k gelir.
                # GELECEK Ä°Ã‡Ä°N NOT: Paralel araÃ§ Ã§aÄŸÄ±rma (multi-tool-call) senaryolarÄ± iÃ§in
                # veri setinde aracÄ±n hangi 'tool_call_id'ye yanÄ±t verdiÄŸini belirten
                # bir anahtar eklenmesi ve buradaki mantÄ±ÄŸÄ±n gÃ¼ncellenmesi gerekecektir.
                try:
                    last_tool_call_id = dialogue[-1]["tool_calls"][0]["id"]
                    dialogue.append({
                        "role": "tool", 
                        "content": content,
                        "tool_call_id": last_tool_call_id
                    })
                except (KeyError, IndexError):
                    logger.warning(f"Bir araÃ§ yanÄ±tÄ± iÃ§in geÃ§erli bir araÃ§ Ã§aÄŸrÄ±sÄ± bulunamadÄ±. Veri ID: {item.get('id', 'N/A')}")

        return dialogue

    def _load_and_prepare_dataset(self, tokenizer: AutoTokenizer) -> Dataset:
        """Verilen yollardan veri setlerini yÃ¼kler, birleÅŸtirir ve formatlar."""
        logger.info(f"ğŸ’¾ Veri setleri yÃ¼kleniyor: {self.config.data_paths}")
        
        all_data = []
        for path in self.config.data_paths:
            # Veri yolu artÄ±k proje kÃ¶kÃ¼ne gÃ¶re, mutlak yola Ã§eviriyoruz.
            full_path = PROJECT_ROOT / path
            with open(full_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                all_data.extend(data)
        
        logger.info(f"ğŸ“„ Toplam {len(all_data)} adet diyalog yÃ¼klendi.")
        
        # Llama-3'Ã¼n ChatML formatÄ±nÄ± kullanarak her bir diyaloÄŸu formatla
        formatted_texts = []
        for item in all_data:
            dialogue = self._format_dialogue(item)
            formatted_text = tokenizer.apply_chat_template(
                dialogue, 
                tokenize=False, 
                add_generation_prompt=False
            )
            formatted_texts.append({"text": formatted_text})

        dataset = Dataset.from_list(formatted_texts)
        logger.info("âœ… Veri seti Llama-3 formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.")
        
        return dataset.train_test_split(test_size=self.config.test_size)

    def run(self):
        """EÄŸitim sÃ¼recini baÅŸlatÄ±r ve yÃ¶netir."""
        logger.info("ğŸš€ Uzman seviye eÄŸitim sÃ¼reci baÅŸlatÄ±lÄ±yor...")
        
        # 1. Tokenizer ve Modelin YÃ¼klenmesi
        tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
        # Llama-3'Ã¼n pad token'Ä± yok, eos_token'Ä± kullanÄ±yoruz
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        model = AutoModelForCausalLM.from_pretrained(
            self.config.model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16, # RTX 40xx serisi iÃ§in float16 daha iyi
        )
        model.config.use_cache = False
        
        logger.info("âœ… Model ve tokenizer baÅŸarÄ±yla yÃ¼klendi.")

        # 2. Veri Setinin YÃ¼klenmesi ve HazÄ±rlanmasÄ±
        split_dataset = self._load_and_prepare_dataset(tokenizer)
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        logger.info(f"Split: {len(train_dataset)} train, {len(eval_dataset)} eval.")

        # 3. QLoRA YapÄ±landÄ±rmasÄ±
        model = prepare_model_for_kbit_training(model)
        lora_config = LoraConfig(
            r=self.config.lora_r,
            lora_alpha=self.config.lora_alpha,
            target_modules=self.config.lora_target_modules,
            lora_dropout=self.config.lora_dropout,
            bias="none",
            task_type="CAUSAL_LM",
        )
        model = get_peft_model(model, lora_config)
        
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in model.parameters())
        logger.info(f"ğŸ¯ EÄŸitilebilir parametreler: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")

        # 4. SFTTrainer'Ä±n OluÅŸturulmasÄ±
        trainer = SFTTrainer(
            model=model,
            args=self.training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            # UYARI GÄ°DERME: Bu parametreler artÄ±k `training_args` iÃ§inden okunuyor.
            # dataset_text_field="text",
            # max_seq_length=self.config.max_seq_length,
            # packing=True, 
        )

        # 5. EÄŸitimin BaÅŸlatÄ±lmasÄ±
        logger.info("ğŸ”¥ EÄŸitim baÅŸlÄ±yor...")
        
        # HATA Ã‡Ã–ZÃœMÃœ: output_dir iÃ§inde bir checkpoint olup olmadÄ±ÄŸÄ±nÄ± akÄ±llÄ±ca kontrol et.
        # Varsa, oradan devam et. Yoksa, sÄ±fÄ±rdan baÅŸla.
        last_checkpoint = get_last_checkpoint(self.training_args.output_dir)
        if last_checkpoint:
            logger.info(f"âœ… GeÃ§erli bir checkpoint bulundu, eÄŸitim '{last_checkpoint}' adresinden devam edecek.")
        else:
            logger.info("â„¹ï¸ GeÃ§erli bir checkpoint bulunamadÄ±, eÄŸitim sÄ±fÄ±rdan baÅŸlÄ±yor.")

        trainer.train(resume_from_checkpoint=last_checkpoint)


        # 6. Modelin Kaydedilmesi
        logger.info("ğŸ’¾ En iyi model kaydediliyor...")
        final_model_path = os.path.join(self.training_args.output_dir, "final_model")
        trainer.save_model(final_model_path)
        tokenizer.save_pretrained(final_model_path)
        
        logger.info(f"ğŸ‰ EÄŸitim tamamlandÄ±! Model '{final_model_path}' dizinine kaydedildi.")

        # Temizlik
        del model
        del trainer
        gc.collect()
        torch.cuda.empty_cache()


def main():
    """
    Ana fonksiyonu Ã§alÄ±ÅŸtÄ±rÄ±r. ArgÃ¼manlarÄ± komut satÄ±rÄ±ndan alÄ±r,
    eÄŸitimi yapÄ±landÄ±rÄ±r ve baÅŸlatÄ±r.
    """
    logger.info("--- Strateji 1 & 2 iÃ§in 'Core-Engine' Modeli EÄŸitimi ---")

    # UZMAN SEVÄ°YESÄ° GÃœNCELLEMESÄ°:
    # ArgÃ¼manlarÄ± iki ayrÄ±, mantÄ±ksal gruba ayÄ±rÄ±yoruz ve HfArgumentParser
    # ile komut satÄ±rÄ±ndan okuyoruz. Bu en temiz ve doÄŸru yÃ¶ntemdir.
    parser = HfArgumentParser((ModelAndDataConfig, TrainingArguments))
    model_config, training_args = parser.parse_args_into_dataclasses()

    # W&B iÃ§in dinamik olarak bir Ã§alÄ±ÅŸtÄ±rma adÄ± oluÅŸturuyoruz
    if model_config.use_wandb:
        training_args.run_name = f"core-engine-{datetime.now().strftime('%Y-%m-%d-%H-%M')}"

    try:
        trainer = ExpertTrainer(model_config, training_args)
        trainer.run()
    except Exception as e:
        logger.error(f"EÄŸitim sÄ±rasÄ±nda beklenmedik bir hata oluÅŸtu: {e}", exc_info=True)

if __name__ == "__main__":
    main()