# -*- coding: utf-8 -*-
"""
ğŸš€ Uzman Seviye Ä°nteraktif Test AlanÄ± (Advanced Interactive Playground) - v4 (Diagnostik Modlu)
================================================================================================

Bu script, eÄŸitilmiÅŸ ve birleÅŸtirilmiÅŸ (merged) QLoRA modelinizin
hem temel akÄ±l saÄŸlÄ±ÄŸÄ±nÄ± hem de araÃ§ kullanma yeteneklerini test etmek
iÃ§in bir "basit mod" iÃ§erir.

Ã–nceki hatalarÄ±n tÃ¼mÃ¼ giderilmiÅŸtir:
- âœ… **KESÄ°N Ã‡Ã–ZÃœM:** Tensor oluÅŸturma sÃ¼reci manueldir.
- âœ… **YENÄ° - Diagnostik Modu:** '!simple' komutu ile modelin temel konuÅŸma
  yetenekleri, araÃ§-Ã§aÄŸÄ±rma mantÄ±ÄŸÄ± olmadan test edilebilir. Bu, sorunun
  modelin kendisinde mi yoksa tool-following yeteneÄŸinde mi olduÄŸunu anlamamÄ±zÄ± saÄŸlar.
"""

import os
import torch
import json
import re
import sys
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from rich.console import Console
from rich.markdown import Markdown

try:
    PROJECT_ROOT = Path(__file__).resolve().parents[3]
except NameError:
    PROJECT_ROOT = Path.cwd()

sys.path.append(str(PROJECT_ROOT))

from UniqeAi.ai_model.scripts.tool_definitions import get_tool_definitions, get_tool_response

console = Console()

MODEL_PATH = PROJECT_ROOT / "UniqeAi/ai_model/merged_model_bf16"

def load_model_and_tokenizer(model_path: Path):
    console.print(f"[yellow]ğŸš€ Model yÃ¼kleniyor: [cyan]{model_path}[/cyan][/yellow]")
    if not model_path.exists():
        console.print(f"[bold red]HATA: Model yolu bulunamadÄ±![/bold red]")
        sys.exit(1)

    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=True,
    )

    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
        )
        tokenizer = AutoTokenizer.from_pretrained(model_path)
    except Exception as e:
        console.print(f"[bold red]Model veya Tokenizer yÃ¼klenirken hata oluÅŸtu: {e}[/bold red]")
        sys.exit(1)

    console.print("[green]âœ… Model ve Tokenizer baÅŸarÄ±yla yÃ¼klendi.[/green]")
    return model, tokenizer

def parse_tool_calls(text: str):
    """
    Modelin Ã§Ä±ktÄ±sÄ±ndaki DÃœZELTÄ°LMÄ°Å tool-call formatÄ±nÄ± yakalar.
    <|begin_of_tool_code|> ... <|end_of_tool_code|>
    """
    pattern = r"<\|begin_of_tool_code\|>([\s\S]*?)<\|end_of_tool_code\|>"
    match = re.search(pattern, text)
    if not match:
        return None
    
    tool_code_str = match.group(1).strip()
    
    # "print(fonksiyon(arg=val))" formatÄ±nÄ± yakalamak iÃ§in daha esnek bir regex
    call_pattern = re.compile(r"print\((\w+)\((.*)\)\)")
    call_match = call_pattern.search(tool_code_str)
    
    if not call_match:
        return None
        
    function_name = call_match.group(1)
    args_str = call_match.group(2)
    
    try:
        params = {}
        # String deÄŸerleri doÄŸru ÅŸekilde yakalamak iÃ§in daha saÄŸlam bir regex
        arg_pattern = re.compile(r"(\w+)=((?:\"(?:\\\"|[^\"])*\")|(?:'(?:\\'|[^'])*')|[^,)]+)")
        for p_match in arg_pattern.finditer(args_str):
            key = p_match.group(1)
            raw_value = p_match.group(2)

            # DeÄŸerin tÄ±rnak iÃ§inde olup olmadÄ±ÄŸÄ±nÄ± kontrol et ve temizle
            if (raw_value.startswith('"') and raw_value.endswith('"')) or \
               (raw_value.startswith("'") and raw_value.endswith("'")):
                # JSON'un string ayrÄ±ÅŸtÄ±rma yeteneÄŸini kullanarak kaÃ§Ä±ÅŸ karakterlerini doÄŸru yÃ¶net
                value = json.loads(raw_value)
            else:
                # SayÄ±sal veya boolean olabilir
                try:
                    # json.loads() sayÄ±larÄ± ve boolean'larÄ± da doÄŸru ÅŸekilde iÅŸler
                    value = json.loads(raw_value.lower()) # 'True' -> true
                except (json.JSONDecodeError, AttributeError):
                    value = raw_value # Ham string olarak bÄ±rak

            params[key] = value

        return [{
            "id": f"tool_call_{os.urandom(4).hex()}",
            "type": "function",
            "function": {
                "name": function_name,
                "arguments": json.dumps(params, ensure_ascii=False)
            }
        }]
    except Exception as e:
        console.print(f"[red]AraÃ§ parametreleri ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {args_str}, Hata: {e}[/red]")
        return None

def main():
    model, tokenizer = load_model_and_tokenizer(MODEL_PATH)
    tools = get_tool_definitions() # Bu script OpenAI formatÄ±nÄ± kullandÄ±ÄŸÄ± iÃ§in tool tanÄ±mlarÄ±na ihtiyaÃ§ duyar.
    terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids("<|eot_id|>")]
    dialogue = []

    console.print("\n" + "="*50, style="bold green")
    console.print("ğŸ¤– [bold green]Diagnostik Test AlanÄ±na HoÅŸ Geldiniz (v4 - Geri YÃ¼klendi)[/bold green]")
    console.print("   - Normal komut girerek [bold]araÃ§ kullanma[/bold] yeteneÄŸini test edin.")
    console.print("   - `!simple <sorunuz>` yazarak modelin [bold]temel konuÅŸma[/bold] yeteneÄŸini test edin.")
    console.print("   - Ã‡Ä±kmak iÃ§in 'quit' veya 'exit' yazÄ±n.")
    console.print("="*50, style="bold green")

    while True:
        try:
            user_input = console.input("[bold blue]ğŸ‘¤ Siz: [/bold blue]")
            if user_input.lower() in ["quit", "exit"]: break
        except (KeyboardInterrupt, EOFError): break

        # --- Diagnostik MantÄ±ÄŸÄ± ---
        if user_input.startswith("!simple "):
            # --- BASÄ°T TEST MODU ---
            simple_question = user_input[len("!simple "):]
            simple_dialogue = [{"role": "user", "content": simple_question}]
            
            console.print("[yellow]... model basit modda dÃ¼ÅŸÃ¼nÃ¼yor ...[/yellow]")
            
            # AraÃ§lar olmadan, basit bir ÅŸablon uygula
            token_ids = tokenizer.apply_chat_template(simple_dialogue, add_generation_prompt=True, tokenize=True, return_tensors="pt").to(model.device)
            attention_mask = torch.ones_like(token_ids)

            outputs = model.generate(
                input_ids=token_ids,
                attention_mask=attention_mask,
                max_new_tokens=256,
                eos_token_id=terminators,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id
            )
            response_text = tokenizer.decode(outputs[0][token_ids.shape[-1]:], skip_special_tokens=True)
            console.print(f"ğŸ¤– [bold green]Asistan (Basit YanÄ±t):[/bold green]", end="")
            console.print(Markdown(response_text))
            # Basit testler ana konuÅŸma geÃ§miÅŸine eklenmez, her seferinde temiz baÅŸlar.
        
        else:
            # --- NORMAL ARAÃ‡ KULLANIM MODU (OpenAI Stili) ---
            # Bu script'in kullandÄ±ÄŸÄ± diyalog formatÄ±, `terminal_app.py`den farklÄ±dÄ±r.
            # Bu format OpenAI API'sine daha yakÄ±ndÄ±r.
            dialogue.append({"role": "user", "content": user_input})
            console.print("[yellow]... model dÃ¼ÅŸÃ¼nÃ¼yor ...[/yellow]")

            # Not: Bu script'in kullandÄ±ÄŸÄ± `apply_chat_template` formatÄ± (tools parametresi ile)
            # aslÄ±nda Llama-3'Ã¼n standart tool kullanÄ±mÄ± iÃ§in Ã¶nerilmez, ama test iÃ§in bir yÃ¶ntemdir.
            token_ids = tokenizer.apply_chat_template(dialogue, add_generation_prompt=True, tokenize=True, return_tensors="pt").to(model.device)
            attention_mask = torch.ones_like(token_ids)

            outputs = model.generate(
                input_ids=token_ids,
                attention_mask=attention_mask,
                max_new_tokens=1024,
                eos_token_id=terminators,
                do_sample=True, temperature=0.6, top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
            response_text = tokenizer.decode(outputs[0][token_ids.shape[-1]:], skip_special_tokens=False)
            
            # Bu script'in `parse_tool_calls` fonksiyonu, `terminal_app.py`'daki ile aynÄ±.
            tool_calls = parse_tool_calls(response_text)
            
            if tool_calls:
                # `tool_calls` formatÄ± OpenAI API'sinin beklediÄŸi formattÄ±r.
                # EÄŸitim verimiz bu formatÄ± doÄŸrudan iÃ§ermese de, Llama-3 bazen bu formatÄ± Ã¼retebilir.
                # `terminal_app.py`deki metin bazlÄ± yaklaÅŸÄ±m daha gÃ¼venilirdir.
                dialogue.append({"role": "assistant", "content": None, "tool_calls": tool_calls})
                
                for call in tool_calls:
                    func_name = call["function"]["name"]
                    func_args = json.loads(call["function"]["arguments"])
                    tool_call_id = call["id"]
                    console.print(f"ğŸ› ï¸  [bold yellow]AraÃ§ Ã‡aÄŸrÄ±sÄ±:[/bold yellow] [green]{func_name}[/green]({json.dumps(func_args, ensure_ascii=False)})")
                    
                    tool_response_content = get_tool_response(func_name, func_args)
                    console.print(f"âš™ï¸  [bold magenta]AraÃ§ YanÄ±tÄ±:[/bold magenta] {tool_response_content}")
                    
                    dialogue.append({"role": "tool", "name": func_name, "tool_call_id": tool_call_id, "content": tool_response_content})

                console.print("[yellow]... model araÃ§ sonuÃ§larÄ±nÄ± Ã¶zetliyor ...[/yellow]")

                final_token_ids = tokenizer.apply_chat_template(dialogue, add_generation_prompt=True, tokenize=True, return_tensors="pt").to(model.device)
                final_attention_mask = torch.ones_like(final_token_ids)

                final_outputs = model.generate(
                    input_ids=final_token_ids,
                    attention_mask=final_attention_mask,
                    max_new_tokens=1024,
                    eos_token_id=terminators,
                    do_sample=True, temperature=0.6, top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id
                )
                final_response_text = tokenizer.decode(final_outputs[0][final_token_ids.shape[-1]:], skip_special_tokens=True)
                console.print(f"ğŸ¤– [bold green]Asistan:[/bold green] ", end="")
                console.print(Markdown(final_response_text))
                dialogue.append({"role": "assistant", "content": final_response_text})
            else:
                cleaned_response = re.sub(r'<\|.*?\|>', '', response_text).strip()
                console.print(f"ğŸ¤– [bold green]Asistan:[/bold green] ", end="")
                console.print(Markdown(cleaned_response))
                dialogue.append({"role": "assistant", "content": cleaned_response})
    
    console.print("\n[bold red]GÃ¶rÃ¼ÅŸmek Ã¼zere![/bold red]")

if __name__ == "__main__":
    main() 