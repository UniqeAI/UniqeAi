# -*- coding: utf-8 -*-
"""
ğŸš€ Telekom AI Agent Terminal UygulamasÄ± - ArkadaÅŸ PaylaÅŸÄ±m Versiyonu
================================================================

Bu uygulama, eÄŸitilmiÅŸ Telekom mÃ¼ÅŸteri hizmetleri AI modelini
terminal Ã¼zerinden kolayca test etmenizi saÄŸlar.

Ã–zellikler:
- ğŸ”„ Otomatik model indirme (Hugging Face Hub'dan)
- ğŸ¯ GerÃ§ek zamanlÄ± yanÄ±t akÄ±ÅŸÄ±
- ğŸ› ï¸ Tool-calling desteÄŸi
- ğŸ’¡ Dinamik baÄŸlam yÃ¶netimi
- ğŸ” Hata ayÄ±klama modu

Gereksinimler:
- NVIDIA GPU (CUDA desteÄŸi)
- Python 3.8+
- Yeterli RAM (8GB+ Ã¶nerilir)
"""

import os
import sys
import json
import re
import uuid
import torch
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import argparse

# UI ve etkileÅŸim iÃ§in
try:
    from rich.console import Console
    from rich.markdown import Markdown
    from rich.panel import Panel
    from rich.progress import Progress, SpinnerColumn, TextColumn
    from rich.prompt import Prompt, Confirm
except ImportError:
    print("âŒ Rich kÃ¼tÃ¼phanesi bulunamadÄ±. LÃ¼tfen 'pip install rich' Ã§alÄ±ÅŸtÄ±rÄ±n.")
    sys.exit(1)

# AI model kÃ¼tÃ¼phaneleri
try:
    from transformers import (
        AutoModelForCausalLM, 
        AutoTokenizer,
        BitsAndBytesConfig,
        TextStreamer
    )
    from huggingface_hub import snapshot_download, HfApi
except ImportError:
    print("âŒ Transformers veya huggingface_hub bulunamadÄ±. LÃ¼tfen requirements.txt'i kurun.")
    sys.exit(1)

# Tool tanÄ±mlarÄ± iÃ§in yerel import
try:
    from tool_definitions import get_tool_definitions, get_tool_response
except ImportError:
    print("âš ï¸ tool_definitions.py bulunamadÄ±. AraÃ§ iÅŸlevselliÄŸi devre dÄ±ÅŸÄ± olacak.")
    get_tool_definitions = lambda: []
    get_tool_response = lambda name, args: f"AraÃ§ '{name}' bulunamadÄ±."

console = Console()

@dataclass
class AppConfig:
    """Uygulama yapÄ±landÄ±rmasÄ±"""
    model_name: str = "Choyrens/ChoyrensAI-Telekom-Agent-v1-merged"
    local_model_dir: str = "./models/telekom-agent"
    max_conversation_length: int = 6  # Son N diyalog Ã§iftini tutar
    max_new_tokens: int = 1024
    temperature: float = 0.7
    top_p: float = 0.9
    debug_mode: bool = False
    streaming: bool = True

class ModelManager:
    """Model indirme ve yÃ¶netim sÄ±nÄ±fÄ±"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.model_path = Path(config.local_model_dir)
        
    def ensure_model_available(self) -> bool:
        """Model varsa True, yoksa indir ve True dÃ¶ndÃ¼r"""
        if self.model_path.exists() and any(self.model_path.iterdir()):
            console.print(f"âœ… Model zaten mevcut: {self.model_path}")
            return True
            
        return self._download_model()
    
    def _download_model(self) -> bool:
        """Modeli Hugging Face Hub'dan indir"""
        try:
            console.print(f"ğŸ“¥ Model indiriliyor: {self.config.model_name}")
            console.print("   Bu iÅŸlem internet hÄ±zÄ±nÄ±za baÄŸlÄ± olarak birkaÃ§ dakika sÃ¼rebilir...")
            
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                console=console
            ) as progress:
                task = progress.add_task("Model indiriliyor...", total=None)
                
                # Model dosyalarÄ±nÄ± indir
                snapshot_download(
                    repo_id=self.config.model_name,
                    local_dir=self.model_path,
                    local_dir_use_symlinks=False  # Windows uyumluluÄŸu iÃ§in
                )
                
                progress.update(task, description="âœ… Ä°ndirme tamamlandÄ±!")
            
            console.print(f"âœ… Model baÅŸarÄ±yla indirildi: {self.model_path}")
            return True
            
        except Exception as e:
            console.print(f"âŒ Model indirme hatasÄ±: {e}")
            if "authentication" in str(e).lower():
                console.print("ğŸ’¡ Ä°pucu: Hugging Face token'Ä±nÄ±zÄ± kontrol edin.")
            return False

class ChatSession:
    """Sohbet oturumu yÃ¶netimi"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.conversation_history: List[Dict[str, str]] = []
        self.tools = get_tool_definitions()
        
    def add_message(self, role: str, content: str):
        """KonuÅŸma geÃ§miÅŸine mesaj ekle"""
        self.conversation_history.append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })
        
        # Uzun konuÅŸmalarÄ± kÄ±salt
        if len(self.conversation_history) > self.config.max_conversation_length * 2:
            # Ä°lk sistem mesajÄ±nÄ± koru, ortadakileri sil
            self.conversation_history = (
                self.conversation_history[:1] + 
                self.conversation_history[-(self.config.max_conversation_length * 2 - 1):]
            )
    
    def get_chat_context(self) -> List[Dict[str, str]]:
        """Mevcut sohbet baÄŸlamÄ±nÄ± dÃ¶ndÃ¼r"""
        # Sistem mesajÄ±yla baÅŸla
        context = [{
            "role": "system",
            "content": (
                "Sen TÃ¼rk Telekom mÃ¼ÅŸteri hizmetleri asistanÄ±sÄ±n. "
                "MÃ¼ÅŸterilere yardÄ±m etmek iÃ§in Ã§eÅŸitli araÃ§larÄ± kullanabilirsin. "
                "Her zaman kibar, yardÄ±msever ve Ã§Ã¶zÃ¼m odaklÄ± ol. "
                "EÄŸer bir araÃ§ kullanman gerekiyorsa, uygun aracÄ± Ã§aÄŸÄ±r ve sonucunu mÃ¼ÅŸteriye aÃ§Ä±kla."
            )
        }]
        
        # KonuÅŸma geÃ§miÅŸini ekle
        for msg in self.conversation_history:
            if msg["role"] in ["user", "assistant"]:
                context.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        return context

class ToolCallParser:
    """AraÃ§ Ã§aÄŸrÄ±larÄ±nÄ± ayrÄ±ÅŸtÄ±rma sÄ±nÄ±fÄ±"""
    
    @staticmethod
    def extract_tool_calls(text: str) -> Optional[List[Dict[str, Any]]]:
        """
        Model Ã§Ä±ktÄ±sÄ±ndan araÃ§ Ã§aÄŸrÄ±larÄ±nÄ± Ã§Ä±kart
        Format: <|begin_of_tool_code|>print(function_name(param="value"))<|end_of_tool_code|>
        """
        pattern = r"<\|begin_of_tool_code\|>(.*?)<\|end_of_tool_code\|>"
        matches = re.findall(pattern, text, re.DOTALL)
        
        if not matches:
            return None
        
        tool_calls = []
        for match in matches:
            tool_code = match.strip()
            
            # print(function_name(args)) formatÄ±nÄ± yakala
            func_pattern = r"print\((\w+)\((.*)\)\)"
            func_match = re.search(func_pattern, tool_code)
            
            if func_match:
                function_name = func_match.group(1)
                args_str = func_match.group(2)
                
                # Parametreleri ayrÄ±ÅŸtÄ±r
                params = ToolCallParser._parse_arguments(args_str)
                
                tool_calls.append({
                    "id": f"call_{uuid.uuid4().hex[:8]}",
                    "function": function_name,
                    "arguments": params
                })
        
        return tool_calls if tool_calls else None
    
    @staticmethod
    def _parse_arguments(args_str: str) -> Dict[str, Any]:
        """Fonksiyon argÃ¼manlarÄ±nÄ± ayrÄ±ÅŸtÄ±r"""
        if not args_str.strip():
            return {}
        
        params = {}
        
        # key="value" veya key=value formatÄ±nÄ± yakala
        arg_pattern = r'(\w+)=(?:"([^"]*)"|\'([^\']*)\"|([^,\)]+))'
        matches = re.findall(arg_pattern, args_str)
        
        for match in matches:
            key = match[0]
            value = match[1] or match[2] or match[3]
            
            # Tip dÃ¶nÃ¼ÅŸÃ¼mÃ¼
            if value.lower() in ['true', 'false']:
                params[key] = value.lower() == 'true'
            elif value.isdigit():
                params[key] = int(value)
            elif value.replace('.', '').isdigit():
                params[key] = float(value)
            else:
                params[key] = value
        
        return params

class TelekomAITerminal:
    """Ana terminal uygulamasÄ±"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.model_manager = ModelManager(config)
        self.chat_session = ChatSession(config)
        self.model = None
        self.tokenizer = None
        
    def initialize(self) -> bool:
        """UygulamayÄ± baÅŸlat"""
        console.print("ğŸ¤– Telekom AI Agent Terminal'e HoÅŸ Geldiniz!")
        console.print("=" * 60)
        
        # Model varlÄ±ÄŸÄ±nÄ± kontrol et
        if not self.model_manager.ensure_model_available():
            return False
        
        # Modeli yÃ¼kle
        return self._load_model()
    
    def _load_model(self) -> bool:
        """AI modelini belleÄŸe yÃ¼kle"""
        try:
            console.print("ğŸ”„ Model yÃ¼kleniyor...")
            
            # GPU kontrolÃ¼
            if not torch.cuda.is_available():
                console.print("âš ï¸ CUDA bulunamadÄ±. CPU modu kullanÄ±lacak (yavaÅŸ olabilir).")
                device_map = "cpu"
                quantization_config = None
            else:
                console.print(f"âœ… GPU tespit edildi: {torch.cuda.get_device_name()}")
                device_map = "auto"
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                    bnb_4bit_use_double_quant=True,
                )
            
            # Tokenizer yÃ¼kle
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_manager.model_path,
                trust_remote_code=True
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Model yÃ¼kle
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_manager.model_path,
                quantization_config=quantization_config,
                device_map=device_map,
                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True
            )
            
            console.print("âœ… Model baÅŸarÄ±yla yÃ¼klendi!")
            return True
            
        except Exception as e:
            console.print(f"âŒ Model yÃ¼kleme hatasÄ±: {e}")
            return False
    
    def _generate_response(self, user_input: str) -> str:
        """KullanÄ±cÄ± girdisi iÃ§in yanÄ±t Ã¼ret"""
        # BaÄŸlamÄ± hazÄ±rla
        self.chat_session.add_message("user", user_input)
        context = self.chat_session.get_chat_context()
        
        if self.config.debug_mode:
            console.print("ğŸ” Debug - GÃ¶nderilen baÄŸlam:")
            for msg in context:
                console.print(f"  {msg['role']}: {msg['content'][:100]}...")
        
        # Tokenize et
        chat_template = self.tokenizer.apply_chat_template(
            context,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            chat_template,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # Terminasyon tokenlarÄ±
        terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]
        
        # YanÄ±t Ã¼ret
        if self.config.streaming:
            streamer = TextStreamer(
                self.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
        else:
            streamer = None
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=self.config.max_new_tokens,
                eos_token_id=terminators,
                do_sample=True,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                pad_token_id=self.tokenizer.eos_token_id,
                streamer=streamer
            )
        
        # YanÄ±tÄ± decode et
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[-1]:],
            skip_special_tokens=True
        )
        
        return response.strip()
    
    def _handle_tool_calls(self, response: str) -> str:
        """AraÃ§ Ã§aÄŸrÄ±larÄ±nÄ± iÅŸle ve nihai yanÄ±tÄ± dÃ¶ndÃ¼r"""
        tool_calls = ToolCallParser.extract_tool_calls(response)
        
        if not tool_calls:
            return response
        
        console.print("\nğŸ› ï¸ AraÃ§ Ã§aÄŸrÄ±larÄ± tespit edildi:")
        
        # Her araÃ§ Ã§aÄŸrÄ±sÄ±nÄ± iÅŸle
        tool_results = []
        for call in tool_calls:
            func_name = call["function"]
            args = call["arguments"]
            
            console.print(f"  ğŸ“ {func_name}({', '.join(f'{k}={v}' for k, v in args.items())})")
            
            # AracÄ± Ã§alÄ±ÅŸtÄ±r
            result = get_tool_response(func_name, args)
            tool_results.append(f"ğŸ”§ {func_name}: {result}")
            
            console.print(f"    âœ… SonuÃ§: {result}")
        
        # AraÃ§ sonuÃ§larÄ±nÄ± baÄŸlama ekle ve Ã¶zet iste
        tool_summary = "\n".join(tool_results)
        summary_prompt = f"""
YukarÄ±daki araÃ§ Ã§aÄŸrÄ±larÄ±nÄ±n sonuÃ§larÄ±na dayanarak, mÃ¼ÅŸteriye net ve anlaÅŸÄ±lÄ±r bir yanÄ±t ver:

AraÃ§ SonuÃ§larÄ±:
{tool_summary}

LÃ¼tfen bu bilgileri kullanarak mÃ¼ÅŸterinin sorusunu yanÄ±tla:
"""
        
        return self._generate_response(summary_prompt)
    
    def run(self):
        """Ana uygulama dÃ¶ngÃ¼sÃ¼"""
        if not self.initialize():
            console.print("âŒ Uygulama baÅŸlatÄ±lamadÄ±.")
            return
        
        # KullanÄ±m talimatlarÄ±
        console.print("\nğŸ“‹ KullanÄ±m TalimatlarÄ±:")
        console.print("  â€¢ Herhangi bir soru sorun")
        console.print("  â€¢ 'debug' yazarak hata ayÄ±klama modunu aÃ§Ä±n/kapatÄ±n")
        console.print("  â€¢ 'clear' yazarak konuÅŸma geÃ§miÅŸini temizleyin")
        console.print("  â€¢ 'quit' veya Ctrl+C ile Ã§Ä±kÄ±n")
        console.print("=" * 60)
        
        try:
            while True:
                # KullanÄ±cÄ± girdisi al
                user_input = Prompt.ask("\n[bold blue]Siz")
                
                if not user_input.strip():
                    continue
                
                # Ã–zel komutlar
                if user_input.lower() == 'quit':
                    break
                elif user_input.lower() == 'debug':
                    self.config.debug_mode = not self.config.debug_mode
                    console.print(f"ğŸ” Debug modu: {'AÃ‡IK' if self.config.debug_mode else 'KAPALI'}")
                    continue
                elif user_input.lower() == 'clear':
                    self.chat_session.conversation_history.clear()
                    console.print("ğŸ—‘ï¸ KonuÅŸma geÃ§miÅŸi temizlendi.")
                    continue
                
                # AI yanÄ±tÄ± Ã¼ret
                console.print("\n[bold green]ğŸ¤– Asistan:[/bold green]", end=" ")
                
                if not self.config.streaming:
                    console.print("DÃ¼ÅŸÃ¼nÃ¼yor...")
                
                try:
                    response = self._generate_response(user_input)
                    
                    # AraÃ§ Ã§aÄŸrÄ±larÄ± varsa iÅŸle
                    if "<|begin_of_tool_code|>" in response:
                        final_response = self._handle_tool_calls(response)
                        
                        if not self.config.streaming:
                            console.print(final_response)
                        
                        self.chat_session.add_message("assistant", final_response)
                    else:
                        if not self.config.streaming:
                            console.print(response)
                        
                        self.chat_session.add_message("assistant", response)
                
                except KeyboardInterrupt:
                    console.print("\nâ¹ï¸ YanÄ±t oluÅŸturma durduruldu.")
                except Exception as e:
                    console.print(f"\nâŒ Hata: {e}")
                    if self.config.debug_mode:
                        import traceback
                        console.print(traceback.format_exc())
        
        except KeyboardInterrupt:
            pass
        
        console.print("\nğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")

def main():
    """Ana fonksiyon"""
    parser = argparse.ArgumentParser(description="Telekom AI Agent Terminal")
    parser.add_argument("--model", default="Choyrens/ChoyrensAI-Telekom-Agent-v1-merged", 
                       help="Model repository adÄ±")
    parser.add_argument("--debug", action="store_true", help="Debug modunda baÅŸlat")
    parser.add_argument("--no-streaming", action="store_true", help="Streaming'i devre dÄ±ÅŸÄ± bÄ±rak")
    parser.add_argument("--max-tokens", type=int, default=1024, help="Maksimum token sayÄ±sÄ±")
    
    args = parser.parse_args()
    
    # KonfigÃ¼rasyon oluÅŸtur
    config = AppConfig(
        model_name=args.model,
        debug_mode=args.debug,
        streaming=not args.no_streaming,
        max_new_tokens=args.max_tokens
    )
    
    # Terminal uygulamasÄ±nÄ± baÅŸlat
    app = TelekomAITerminal(config)
    app.run()

if __name__ == "__main__":
    main() 